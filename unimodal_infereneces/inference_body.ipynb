{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea8d5e05-042a-47ea-ac9d-731db6bcb373",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "from transformers import CLIPProcessor\n",
    "from models.models import EmotionMamba, PersonalityMamba, FusionTransformer\n",
    "from data_loading.feature_extractor import PretrainedImageEmbeddingExtractor\n",
    "from utils.config_loader import ConfigLoader\n",
    "\n",
    "def draw_box(image, box, color=(255, 0, 255)):\n",
    "    \"\"\"Draw a rectangle on the image.\"\"\"\n",
    "    line_width = 2\n",
    "    lw = line_width or max(round(sum(image.shape) / 2 * 0.003), 2)\n",
    "    p1, p2 = (int(box[0]), int(box[1])), (int(box[2]), int(box[3]))\n",
    "    cv2.rectangle(image, p1, p2, color, thickness=lw, lineType=cv2.LINE_AA)\n",
    "\n",
    "def image_processing(image, image_processor):\n",
    "    image = image_processor(images=image, return_tensors=\"pt\").to(\"cuda\")\n",
    "    image = image['pixel_values']\n",
    "    return image\n",
    "\n",
    "def preprocess_face(face_roi: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ–±–ª–∞—Å—Ç–∏ –ª–∏—Ü–∞ (–ø—Ä–∏–º–µ—Ä: –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è + resize).\"\"\"\n",
    "    # –ü—Ä–∏–º–µ—Ä: –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ 112x112 –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º [0, 1]\n",
    "    face_roi = cv2.resize(face_roi, (112, 112))\n",
    "    face_roi = face_roi.astype('float32') / 255.0\n",
    "    return face_roi\n",
    "\n",
    "def preprocess_body(body_roi: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ–±–ª–∞—Å—Ç–∏ —Ç–µ–ª–∞ (–ø—Ä–∏–º–µ—Ä: –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è + resize).\"\"\"\n",
    "    # –ü—Ä–∏–º–µ—Ä: –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ 224x224 –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º [0, 1]\n",
    "    body_roi = cv2.resize(body_roi, (224, 224))\n",
    "    body_roi = body_roi.astype('float32') / 255.0\n",
    "    return body_roi\n",
    "\n",
    "def select_uniform_frames(frames, N):\n",
    "    if len(frames) <= N:\n",
    "        return frames\n",
    "    else:\n",
    "        indices = np.linspace(0, len(frames) - 1, num=N, dtype=int)\n",
    "        return [frames[i] for i in indices]\n",
    "\n",
    "def get_fusion_model(config, device):\n",
    "    emo_model = EmotionMamba(\n",
    "    input_dim_emotion     = config.image_embedding_dim,\n",
    "    input_dim_personality = config.image_embedding_dim,\n",
    "    len_seq               = config.counter_need_frames, \n",
    "    hidden_dim            = config.hidden_dim_emo,\n",
    "    out_features          = config.out_features_emo,\n",
    "    tr_layer_number       = config.tr_layer_number_emo,\n",
    "    num_transformer_heads = config.num_transformer_heads_emo,\n",
    "    positional_encoding   = config.positional_encoding_emo,\n",
    "    mamba_d_model         = config.mamba_d_state_emo,\n",
    "    mamba_layer_number    = config.mamba_layer_number_emo,\n",
    "    dropout               = config.dropout,\n",
    "    num_emotions          = 7,\n",
    "    num_traits            = 5,\n",
    "    device                = device\n",
    "    ).to(device).eval()\n",
    "    # –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–¥–∞–µ–º –¥–ª—è –ª—É—á—à–µ–π –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏\n",
    "    per_model = PersonalityMamba(\n",
    "    input_dim_emotion     = config.image_embedding_dim,\n",
    "    input_dim_personality = config.image_embedding_dim,\n",
    "    len_seq               = config.counter_need_frames, \n",
    "    hidden_dim            = config.hidden_dim_per,\n",
    "    out_features          = config.out_features_per,\n",
    "    per_activation        = config.best_per_activation,\n",
    "    tr_layer_number       = config.tr_layer_number_per,\n",
    "    num_transformer_heads = config.num_transformer_heads_per,\n",
    "    positional_encoding   = config.positional_encoding_per,\n",
    "    mamba_d_model         = config.mamba_d_state_per,\n",
    "    mamba_layer_number    = config.mamba_layer_number_per,\n",
    "    dropout               = config.dropout,\n",
    "    num_emotions          = 7,\n",
    "    num_traits            = 5,\n",
    "    device                = device\n",
    "    ).to(device).eval()\n",
    "\n",
    "    # emo_state = torch.load(config.path_to_saved_emotion_model, map_location=device)\n",
    "    # emo_model.load_state_dict(emo_state)\n",
    "\n",
    "    # emo_state = torch.load(config.path_to_saved_personality_model, map_location=device)\n",
    "    # per_model.load_state_dict(emo_state)\n",
    "    model = FusionTransformer(\n",
    "        emo_model             = emo_model,\n",
    "        per_model             = per_model,\n",
    "        input_dim_emotion     = config.image_embedding_dim,\n",
    "        input_dim_personality = config.image_embedding_dim,\n",
    "        hidden_dim            = config.hidden_dim,\n",
    "        out_features          = config.out_features,\n",
    "        per_activation        = config.per_activation,\n",
    "        tr_layer_number       = config.tr_layer_number,\n",
    "        num_transformer_heads = config.num_transformer_heads,\n",
    "        positional_encoding   = config.positional_encoding,\n",
    "        mamba_d_model         = config.mamba_d_state,\n",
    "        mamba_layer_number    = config.mamba_layer_number,\n",
    "        dropout               = config.dropout,\n",
    "        num_emotions          = 7,\n",
    "        num_traits            = 5,\n",
    "        device                = device\n",
    "        ).to(device).eval()\n",
    "\n",
    "    return model\n",
    "\n",
    "def transform_matrix(matrix):\n",
    "    threshold1 = 1 - 1/7 \n",
    "    threshold2 = 1/7\n",
    "    mask1 = matrix[:, 0] >= threshold1\n",
    "    result = np.zeros_like(matrix[:, 1:])\n",
    "    transformed = (matrix[:, 1:] >= threshold2).astype(int)\n",
    "    result[~mask1] = transformed[~mask1]\n",
    "    return result\n",
    "\n",
    "def process_predictions(pred_emo):\n",
    "    pred_emo = torch.nn.functional.softmax(pred_emo, dim=1).cpu().detach().numpy()\n",
    "    pred_emo = transform_matrix(pred_emo).tolist()\n",
    "    return pred_emo\n",
    "\n",
    "def get_metadata(video_path: str, segment_length: int, image_processor: None, image_feature_extractor: None, device: None) -> pd.DataFrame:\n",
    "    \"\"\"–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è: –ø–æ–ª—É—á–∞–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤–∏–¥–µ–æ.\"\"\"\n",
    "    if hasattr(body_detector.predictor, 'trackers'):\n",
    "        body_detector.predictor.trackers[0].reset()\n",
    "    \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    video_name = os.path.basename(video_path)\n",
    "    w, h, fps, total_frames = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS, cv2.CAP_PROP_FRAME_COUNT))\n",
    "    need_frames = select_uniform_frames(list(range(total_frames)), segment_length)\n",
    "    \n",
    "    counter = 0\n",
    "    embeds = []\n",
    "\n",
    "    body_list = []\n",
    "    face_list = []\n",
    "    \n",
    "    while True:\n",
    "        ret, im0 = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        if counter in need_frames:\n",
    "            # –î–µ—Ç–µ–∫—Ü–∏—è –≤—Å–µ—Ö –ª–∏—Ü\n",
    "            preprocessed_body = []\n",
    "            preprocessed_face = []\n",
    "            face_results = face_detector.process(cv2.cvtColor(im0, cv2.COLOR_BGR2RGB))\n",
    "            # –î–µ—Ç–µ–∫—Ü–∏—è –≤—Å–µ—Ö —Ç–µ–ª\n",
    "            body_results = body_detector.track(im0, persist=True, imgsz=640, conf=0.01, iou=0.5, \n",
    "                                             augment=False, device=0, verbose=False)\n",
    "\n",
    "            # –°–ª—É—á–∞–π 1: –ï—Å—Ç—å –ª–∏—Ü–∞ ‚Äî –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥–æ–µ\n",
    "            if face_results.detections:\n",
    "                for face_idx, detection in enumerate(face_results.detections):\n",
    "                    # –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –ª–∏—Ü–∞\n",
    "                    bbox = detection.location_data.relative_bounding_box\n",
    "                    x1, y1 = max(int(bbox.xmin * w), 0), max(int(bbox.ymin * h), 0)\n",
    "                    x2, y2 = min(int((bbox.xmin + bbox.width) * w), w), min(int((bbox.ymin + bbox.height) * h), h)\n",
    "                    face_bbox = (x1, y1, x2, y2)\n",
    "                    face_center = ((x1 + x2) // 2, (y1 + y2) // 2)\n",
    "\n",
    "                    # –ò—â–µ–º —Ç–µ–ª–æ, —Å–æ–¥–µ—Ä–∂–∞—â–µ–µ —Ü–µ–Ω—Ç—Ä –ª–∏—Ü–∞\n",
    "                    body_bbox = None\n",
    "                    body_id = -1\n",
    "                    if body_results and len(body_results[0].boxes) > 0:\n",
    "                        for box in body_results[0].boxes:\n",
    "                            box_coords = box.xyxy.int().cpu().numpy()[0]\n",
    "                            if (box_coords[0] <= face_center[0] <= box_coords[2] and \n",
    "                                box_coords[1] <= face_center[1] <= box_coords[3]):\n",
    "                                body_bbox = box_coords\n",
    "                                body_id = box.id.int().cpu().item() if box.id else -1\n",
    "                                break\n",
    "\n",
    "                    # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞\n",
    "                    face_roi = im0[y1:y2, x1:x2]\n",
    "                    draw_box(im0, [x1, y1, x2, y2])\n",
    "                    draw_box(im0, [body_bbox[0], body_bbox[1], body_bbox[2], body_bbox[3]])\n",
    "                    preprocessed_face = image_processing(face_roi, image_processor) if face_roi.size > 0 else None\n",
    "                    \n",
    "                    if body_bbox is not None:\n",
    "                        body_roi = im0[body_bbox[1]:body_bbox[3], body_bbox[0]:body_bbox[2]]\n",
    "                        preprocessed_body = image_processing(body_roi, image_processor) if body_roi.size > 0 else None\n",
    "                    else:\n",
    "                        preprocessed_body = []\n",
    "\n",
    "                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç\n",
    "                    embeds.append([\n",
    "                        video_name, counter, body_id,\n",
    "                        x1, y1, x2, y2,\n",
    "                        body_bbox[0] if body_bbox is not None else None,\n",
    "                        body_bbox[1] if body_bbox is not None else None,\n",
    "                        body_bbox[2] if body_bbox is not None else None,\n",
    "                        body_bbox[3] if body_bbox is not None else None,\n",
    "                        # preprocessed_face,\n",
    "                        # preprocessed_body\n",
    "                    ])\n",
    "                    # print(preprocessed_body.shape)\n",
    "                    # print(preprocessed_face.shape)\n",
    "                    if preprocessed_body.shape[0] > 0:\n",
    "                        body_list.append(preprocessed_body)\n",
    "                    if preprocessed_face.shape[0] > 0:\n",
    "                        face_list.append(preprocessed_face)\n",
    "                    \n",
    "\n",
    "            # –°–ª—É—á–∞–π 2: –õ–∏—Ü –Ω–µ—Ç ‚Äî –±–µ—Ä—ë–º —Å–∞–º–æ–µ –±–æ–ª—å—à–æ–µ —Ç–µ–ª–æ\n",
    "            elif body_results and len(body_results[0].boxes) > 0:\n",
    "                largest_body = max(\n",
    "                    body_results[0].boxes,\n",
    "                    key=lambda box: (box.xyxy[0,2] - box.xyxy[0,0]) * (box.xyxy[0,3] - box.xyxy[0,1])\n",
    "                )\n",
    "                body_coords = largest_body.xyxy.int().cpu().numpy()[0]\n",
    "                body_id = largest_body.id.int().cpu().item() if largest_body.id else -1\n",
    "\n",
    "                # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–ª–∞\n",
    "                body_roi = im0[body_coords[1]:body_coords[3], body_coords[0]:body_coords[2]]\n",
    "                preprocessed_body = preprocess_body(body_roi) if body_roi.size > 0 else []\n",
    "\n",
    "                embeds.append([\n",
    "                    video_name, counter, body_id,\n",
    "                    None, None, None, None,  # –ù–µ—Ç –ª–∏—Ü–∞\n",
    "                    body_coords[0], body_coords[1], body_coords[2], body_coords[3],\n",
    "                    # None,  # –ù–µ—Ç –ª–∏—Ü–∞\n",
    "                    # preprocessed_body\n",
    "                ])\n",
    "\n",
    "                if preprocessed_body.shape[0] > 0:\n",
    "                    body_list.append(preprocessed_body)\n",
    "                if preprocessed_face.shape[0] > 0:\n",
    "                    face_list.append(preprocessed_face)\n",
    "\n",
    "            plt.imshow(cv2.cvtColor(im0, cv2.COLOR_BGR2RGB))\n",
    "            plt.show()\n",
    "\n",
    "        counter += 1\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    body_list = torch.cat(body_list, dim=0)\n",
    "    body_feature = image_feature_extractor.extract(body_list).to(device)\n",
    "\n",
    "    face_list = torch.cat(face_list, dim=0)\n",
    "    face_feature = image_feature_extractor.extract(face_list).to(device)\n",
    "    \n",
    "    df = pd.DataFrame(embeds, columns=[\n",
    "        \"video_name\", \"frame\", \"person_id\",\n",
    "        \"face_x1\", \"face_y1\", \"face_x2\", \"face_y2\",\n",
    "        \"body_x1\", \"body_y1\", \"body_x2\", \"body_y2\",\n",
    "        # \"preprocessed_face\", \"preprocessed_body\"\n",
    "    ])\n",
    "    return df, body_feature, face_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca879103-6ed9-4998-9d67-2261ff0c4e80",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for FusionTransformer:\n\tsize mismatch for emo_proj.0.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for emo_proj.0.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for emo_proj.1.weight: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for emo_proj.1.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for per_proj.0.weight: copying a param with shape torch.Size([1024, 1024]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n\tsize mismatch for per_proj.0.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for per_proj.1.weight: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for per_proj.1.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for emotion_to_personality_attn.0.self_attention.in_proj_weight: copying a param with shape torch.Size([3072, 1024]) from checkpoint, the shape in current model is torch.Size([768, 256]).\n\tsize mismatch for emotion_to_personality_attn.0.self_attention.in_proj_bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for emotion_to_personality_attn.0.self_attention.out_proj.weight: copying a param with shape torch.Size([1024, 1024]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for emotion_to_personality_attn.0.self_attention.out_proj.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for emotion_to_personality_attn.0.feed_forward.layer_1.weight: copying a param with shape torch.Size([1024, 1024]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for emotion_to_personality_attn.0.feed_forward.layer_1.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for emotion_to_personality_attn.0.feed_forward.layer_2.weight: copying a param with shape torch.Size([1024, 1024]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for emotion_to_personality_attn.0.feed_forward.layer_2.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for emotion_to_personality_attn.0.add_norm_after_attention.norm.weight: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for emotion_to_personality_attn.0.add_norm_after_attention.norm.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for emotion_to_personality_attn.0.add_norm_after_ff.norm.weight: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for emotion_to_personality_attn.0.add_norm_after_ff.norm.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for emotion_to_personality_attn.0.positional_encoding.pe: copying a param with shape torch.Size([5000, 1024]) from checkpoint, the shape in current model is torch.Size([5000, 256]).\n\tsize mismatch for personality_to_emotion_attn.0.self_attention.in_proj_weight: copying a param with shape torch.Size([3072, 1024]) from checkpoint, the shape in current model is torch.Size([768, 256]).\n\tsize mismatch for personality_to_emotion_attn.0.self_attention.in_proj_bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for personality_to_emotion_attn.0.self_attention.out_proj.weight: copying a param with shape torch.Size([1024, 1024]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for personality_to_emotion_attn.0.self_attention.out_proj.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for personality_to_emotion_attn.0.feed_forward.layer_1.weight: copying a param with shape torch.Size([1024, 1024]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for personality_to_emotion_attn.0.feed_forward.layer_1.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for personality_to_emotion_attn.0.feed_forward.layer_2.weight: copying a param with shape torch.Size([1024, 1024]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for personality_to_emotion_attn.0.feed_forward.layer_2.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for personality_to_emotion_attn.0.add_norm_after_attention.norm.weight: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for personality_to_emotion_attn.0.add_norm_after_attention.norm.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for personality_to_emotion_attn.0.add_norm_after_ff.norm.weight: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for personality_to_emotion_attn.0.add_norm_after_ff.norm.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for personality_to_emotion_attn.0.positional_encoding.pe: copying a param with shape torch.Size([5000, 1024]) from checkpoint, the shape in current model is torch.Size([5000, 256]).\n\tsize mismatch for emotion_personality_fc_out.0.weight: copying a param with shape torch.Size([128, 2048]) from checkpoint, the shape in current model is torch.Size([128, 512]).\n\tsize mismatch for personality_emotion_fc_out.0.weight: copying a param with shape torch.Size([128, 2048]) from checkpoint, the shape in current model is torch.Size([128, 512]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m config_face \u001b[38;5;241m=\u001b[39m ConfigLoader(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minference_config_face.toml\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# image_feature_extractor = PretrainedImageEmbeddingExtractor(config_body)\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m image_feature_extractor \u001b[38;5;241m=\u001b[39m \u001b[43mPretrainedImageEmbeddingExtractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Models can download from https://drive.google.com/drive/folders/1APMtC4LXjuW9behd2TxVXz0DsjQKAgRR?usp=sharing\u001b[39;00m\n\u001b[0;32m     11\u001b[0m body_model \u001b[38;5;241m=\u001b[39m get_fusion_model(config_body, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mC:\\Prgrm\\EAAI_2025\\data_loading\\feature_extractor.py:16\u001b[0m, in \u001b[0;36mPretrainedImageEmbeddingExtractor.__init__\u001b[1;34m(self, device, clip_name)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m device\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessor \u001b[38;5;241m=\u001b[39m CLIPProcessor\u001b[38;5;241m.\u001b[39mfrom_pretrained(clip_name)\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbody_model \u001b[38;5;241m=\u001b[39m \u001b[43mget_fusion_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbody\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mface_model \u001b[38;5;241m=\u001b[39m get_fusion_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mface\u001b[39m\u001b[38;5;124m\"\u001b[39m, device)\n",
      "File \u001b[1;32mC:\\Prgrm\\EAAI_2025\\utils\\body\\model_loader.py:103\u001b[0m, in \u001b[0;36mget_fusion_model\u001b[1;34m(modality, device)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;66;03m# ‚Üê‚Äì‚Äì‚Äì –∑–∞–≥—Ä—É–∂–∞–µ–º –∂—ë—Å—Ç–∫–æ –∑–∞–¥–∞–Ω–Ω—ã–π checkpoint ‚Äì‚Äì‚Äì‚Üí\u001b[39;00m\n\u001b[0;32m    102\u001b[0m state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(meta[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mckpt\u001b[39m\u001b[38;5;124m\"\u001b[39m], map_location\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m--> 103\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m        \u001b[38;5;66;03m# strict=True\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mc:\\Prgrm\\EAAI_2025\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2581\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2573\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2574\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   2575\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2576\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[0;32m   2577\u001b[0m             ),\n\u001b[0;32m   2578\u001b[0m         )\n\u001b[0;32m   2580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2581\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   2582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2583\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[0;32m   2584\u001b[0m         )\n\u001b[0;32m   2585\u001b[0m     )\n\u001b[0;32m   2586\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for FusionTransformer:\n\tsize mismatch for emo_proj.0.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for emo_proj.0.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for emo_proj.1.weight: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for emo_proj.1.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for per_proj.0.weight: copying a param with shape torch.Size([1024, 1024]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n\tsize mismatch for per_proj.0.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for per_proj.1.weight: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for per_proj.1.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for emotion_to_personality_attn.0.self_attention.in_proj_weight: copying a param with shape torch.Size([3072, 1024]) from checkpoint, the shape in current model is torch.Size([768, 256]).\n\tsize mismatch for emotion_to_personality_attn.0.self_attention.in_proj_bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for emotion_to_personality_attn.0.self_attention.out_proj.weight: copying a param with shape torch.Size([1024, 1024]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for emotion_to_personality_attn.0.self_attention.out_proj.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for emotion_to_personality_attn.0.feed_forward.layer_1.weight: copying a param with shape torch.Size([1024, 1024]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for emotion_to_personality_attn.0.feed_forward.layer_1.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for emotion_to_personality_attn.0.feed_forward.layer_2.weight: copying a param with shape torch.Size([1024, 1024]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for emotion_to_personality_attn.0.feed_forward.layer_2.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for emotion_to_personality_attn.0.add_norm_after_attention.norm.weight: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for emotion_to_personality_attn.0.add_norm_after_attention.norm.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for emotion_to_personality_attn.0.add_norm_after_ff.norm.weight: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for emotion_to_personality_attn.0.add_norm_after_ff.norm.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for emotion_to_personality_attn.0.positional_encoding.pe: copying a param with shape torch.Size([5000, 1024]) from checkpoint, the shape in current model is torch.Size([5000, 256]).\n\tsize mismatch for personality_to_emotion_attn.0.self_attention.in_proj_weight: copying a param with shape torch.Size([3072, 1024]) from checkpoint, the shape in current model is torch.Size([768, 256]).\n\tsize mismatch for personality_to_emotion_attn.0.self_attention.in_proj_bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for personality_to_emotion_attn.0.self_attention.out_proj.weight: copying a param with shape torch.Size([1024, 1024]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for personality_to_emotion_attn.0.self_attention.out_proj.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for personality_to_emotion_attn.0.feed_forward.layer_1.weight: copying a param with shape torch.Size([1024, 1024]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for personality_to_emotion_attn.0.feed_forward.layer_1.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for personality_to_emotion_attn.0.feed_forward.layer_2.weight: copying a param with shape torch.Size([1024, 1024]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for personality_to_emotion_attn.0.feed_forward.layer_2.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for personality_to_emotion_attn.0.add_norm_after_attention.norm.weight: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for personality_to_emotion_attn.0.add_norm_after_attention.norm.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for personality_to_emotion_attn.0.add_norm_after_ff.norm.weight: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for personality_to_emotion_attn.0.add_norm_after_ff.norm.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for personality_to_emotion_attn.0.positional_encoding.pe: copying a param with shape torch.Size([5000, 1024]) from checkpoint, the shape in current model is torch.Size([5000, 256]).\n\tsize mismatch for emotion_personality_fc_out.0.weight: copying a param with shape torch.Size([128, 2048]) from checkpoint, the shape in current model is torch.Size([128, 512]).\n\tsize mismatch for personality_emotion_fc_out.0.weight: copying a param with shape torch.Size([128, 2048]) from checkpoint, the shape in current model is torch.Size([128, 512])."
     ]
    }
   ],
   "source": [
    "mp_face_detection = mp.solutions.face_detection\n",
    "face_detector = mp_face_detection.FaceDetection(model_selection=1, min_detection_confidence=0.6)\n",
    "body_detector = YOLO('extractors/body/best.pt')\n",
    "image_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "config_body = ConfigLoader(\"inference_config_body.toml\")\n",
    "config_face = ConfigLoader(\"inference_config_face.toml\")\n",
    "# image_feature_extractor = PretrainedImageEmbeddingExtractor(config_body)\n",
    "image_feature_extractor = PretrainedImageEmbeddingExtractor(device=\"cuda\")\n",
    "# Models can download from https://drive.google.com/drive/folders/1APMtC4LXjuW9behd2TxVXz0DsjQKAgRR?usp=sharing\n",
    "\n",
    "body_model = get_fusion_model(config_body, 'cuda')\n",
    "face_model = get_fusion_model(config_face, 'cuda')\n",
    "# results_clip_body_true_mamba_fusiontransformer_2025-06-27_16-10-57/metrics_by_epoch/metrics_epochlog_FusionTransformer_num_transformer_heads_16_20250627_183039_timestamp/best_model_dev.pt\n",
    "body_fusion_model_path = 'extractors/body/clip_body_mamba_transformer_fusion_model.pt'\n",
    "# results_fusiontransformer_2025-07-03_09-41-13/metrics_by_epoch/metrics_epochlog_FusionTransformer_tr_layer_number_3_20250703_124848_timestamp/best_model_dev.pt\n",
    "face_fusion_model_path = 'extractors/face/clip_face_mamba_transformer_fusion_model.pt'\n",
    "\n",
    "body_state = torch.load(body_fusion_model_path, map_location='cuda')\n",
    "body_model.load_state_dict(body_state)\n",
    "\n",
    "face_state = torch.load(face_fusion_model_path, map_location='cuda')\n",
    "face_model.load_state_dict(face_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68ac5f2-8c4a-4f4e-b95c-209089183698",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bfabde07-afb4-4f68-9c28-4ba802d5921f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç –ü–µ—Ä–≤—ã–π —ç–ª–µ–º–µ–Ω—Ç:\n",
      "{'sample_name': '-6rXp3zJ3kc_14.4680_22.8820', 'video_path': 'E:/CMU-MOSEI//video/test/-6rXp3zJ3kc_14.4680_22.8820.mp4', 'audio_path': 'E:/CMU-MOSEI//audio/test/-6rXp3zJ3kc_14.4680_22.8820.wav', 'features': {'body': {'emotion_logits': tensor([ 0.4595,  1.5333, -0.9546,  0.3222, -0.7833,  1.2438, -0.9155]), 'personality_scores': tensor([0.3668, 0.6629, 0.3182, 0.5780, 0.5149]), 'last_emo_encoder_features': tensor([ 1.6797,  1.1995,  0.0361,  ...,  1.3864,  5.1510, -0.0212]), 'last_per_encoder_features': tensor([-0.1187,  3.0620, -1.4489,  ..., -0.2055, -0.5260,  0.7611])}, 'face': {'emotion_logits': tensor([ 0.6361,  1.7097, -0.3265, -0.9575, -1.2868,  0.9896, -0.9342]), 'personality_scores': tensor([0.4642, 0.6192, 0.3096, 0.5205, 0.4690]), 'last_emo_encoder_features': tensor([-8.2230e+00, -6.9223e+00, -5.7896e+00, -3.3821e+00, -3.7589e+00,\n",
      "         2.9955e+00,  3.7664e+00,  1.5413e+00, -1.1885e-01,  6.8728e-01,\n",
      "        -2.7760e+00, -4.4210e+00,  2.4823e+00,  3.9500e+00,  2.7767e+00,\n",
      "         2.2599e+00,  6.4001e+00,  4.4730e+00, -1.3443e+00, -2.7031e+00,\n",
      "         2.0057e+00, -4.8556e+00, -2.8505e+00, -1.9169e+00, -3.1854e+00,\n",
      "        -5.5063e+00, -4.2218e+00,  3.9150e+00, -1.7913e+00, -4.4975e+00,\n",
      "         8.2613e+00, -3.4638e+00,  1.6140e+00, -6.0594e+00, -8.1935e+00,\n",
      "        -1.0164e-01,  9.1869e-01,  1.6131e+00,  8.4634e-01,  2.2539e+00,\n",
      "         3.3320e+00, -8.5332e-01,  8.1516e-01, -3.0834e-02, -3.8450e+00,\n",
      "        -1.2430e+00, -2.4587e+00,  2.1414e+00, -3.6509e-01, -1.2770e+00,\n",
      "        -2.8965e+00, -7.0327e+00,  4.1330e-01, -2.6275e+00,  9.6468e-01,\n",
      "         6.3285e-01, -2.2495e+00,  3.2848e+00,  4.6593e+00,  1.6630e+00,\n",
      "         2.8398e+00,  3.1506e+00,  3.3379e+00,  4.5112e+00, -2.4032e+00,\n",
      "        -6.9692e-01,  7.6390e+00,  3.9145e+00,  1.7551e+00,  5.0412e-01,\n",
      "         3.8861e+00,  7.0959e-01, -6.1046e+00,  8.3063e-02, -2.3711e+00,\n",
      "        -8.8101e+00,  4.8471e+00, -7.1430e+00, -5.4627e+00,  5.2093e+00,\n",
      "        -4.6960e+00, -4.4088e+00, -7.7346e-01, -5.7917e+00, -3.2479e+00,\n",
      "         1.9602e+00,  6.1130e+00,  3.5822e+00, -1.8639e+00,  3.0398e-01,\n",
      "         5.0134e+00, -7.7534e-01, -3.2203e-01, -4.7398e+00, -5.3993e+00,\n",
      "        -3.3205e+00,  2.3089e+00,  2.6681e+00, -1.3856e+00,  1.6674e+00,\n",
      "        -9.4366e-01,  1.0224e+00, -2.5366e-01, -1.0956e+00, -2.5524e-03,\n",
      "        -2.3988e+00, -1.4950e+00, -3.7093e+00, -1.0811e+00, -2.3931e+00,\n",
      "        -2.0864e+00, -2.4135e+00, -5.5368e-01, -8.0471e-02,  4.6487e+00,\n",
      "         1.3678e+00,  7.7976e-01,  9.8927e-01, -1.0328e+00, -1.6091e+00,\n",
      "        -3.9836e+00, -9.4524e-01,  2.9006e+00, -2.1330e+00, -1.2851e+00,\n",
      "        -2.6698e+00, -6.2792e+00,  4.1980e+00,  2.4782e+00,  3.8517e+00,\n",
      "        -1.5659e+00,  3.0305e+00,  1.6429e+00,  2.8950e+00,  5.4329e+00,\n",
      "        -5.4138e-01,  5.6006e-01, -3.7967e+00, -4.9832e-02,  3.6089e+00,\n",
      "        -2.5060e+00, -3.0748e-01,  2.9736e+00, -4.2760e-01, -5.5670e+00,\n",
      "        -1.4583e+00,  3.9547e+00,  1.8745e+00, -2.4937e+00, -4.5061e+00,\n",
      "         7.1186e-01,  1.2927e+00,  1.2588e+00, -3.3954e+00,  2.0785e-01,\n",
      "        -5.9976e+00,  2.5679e+00, -2.1499e+00, -9.0661e+00, -6.7806e-01,\n",
      "         2.8380e+00, -1.9499e+00, -1.8708e+00,  3.8926e-01, -6.8401e+00,\n",
      "         2.5549e+00, -1.3256e+00, -1.1856e+00, -5.8949e+00,  6.7677e+00,\n",
      "        -2.3680e+00,  5.5643e-02,  4.7482e+00, -3.3046e+00,  3.3528e+00,\n",
      "         5.5654e+00, -2.7584e+00,  1.1931e+00, -2.2151e+00,  2.5569e+00,\n",
      "         3.1064e+00, -5.0153e-01, -6.3089e+00, -9.4190e-02, -1.2811e+00,\n",
      "         1.0611e+00,  2.9882e+00,  7.8081e+00,  4.4540e+00,  2.0978e+00,\n",
      "        -3.1566e+00,  4.7358e-01,  2.5205e+00, -2.4982e+00, -1.2211e+00,\n",
      "         1.0048e+00,  1.8879e+00, -7.6357e+00,  7.4511e+00,  9.3876e-01,\n",
      "        -7.5215e-01, -1.6824e+00,  2.7095e+00, -1.3934e+00, -5.8884e+00,\n",
      "        -7.5591e-01,  7.3867e-01, -6.6348e+00, -5.1034e+00, -5.5617e-01,\n",
      "         1.4414e+00, -2.9867e-01, -1.5052e+00, -5.5971e+00, -9.7235e+00,\n",
      "         2.0925e+00, -3.4975e+00,  2.0486e+00,  2.8045e-01, -1.2995e+00,\n",
      "        -4.3494e-01,  4.4868e-01, -4.7383e+00, -4.5860e+00, -6.6773e+00,\n",
      "         9.3402e-01, -3.3955e+00, -4.1155e+00, -1.1057e+00, -3.5150e+00,\n",
      "        -1.4785e+00,  7.1319e+00, -4.2114e+00,  2.2300e+00,  2.0169e+00,\n",
      "         6.4295e+00,  6.1487e-01, -3.5167e+00,  1.0974e+00,  3.8238e+00,\n",
      "         1.7027e-01,  3.6279e-02,  4.6143e+00, -2.5862e+00,  6.0949e-01,\n",
      "         5.9581e+00, -3.6620e+00, -3.2413e+00, -6.0385e+00,  3.5595e+00,\n",
      "        -1.3551e-01,  5.6899e-01,  3.4073e+00,  1.1798e+00,  2.2692e+00,\n",
      "         2.6881e+00,  2.6486e+00,  1.7158e+00, -2.5973e+00, -1.6380e+00,\n",
      "        -3.0164e+00,  4.9023e-02, -2.0941e+00,  7.0048e+00, -4.2084e+00,\n",
      "        -5.9765e+00, -5.3780e-01, -6.2532e+00, -6.2134e-02, -3.1948e+00,\n",
      "        -5.3641e+00, -1.2445e-01,  9.5685e-01,  3.4994e-01,  1.7162e+00,\n",
      "         3.9847e+00, -6.2725e-01,  2.3030e+00, -9.6583e-02, -9.6818e-01,\n",
      "        -5.3083e+00,  1.2905e+00, -2.6743e+00, -2.4229e+00,  2.0376e+00,\n",
      "        -3.1894e+00, -6.2834e+00,  2.1044e-03,  1.1365e+00, -6.1925e+00,\n",
      "         5.5213e-01,  6.3934e-01,  4.5598e+00,  8.0125e+00,  1.9740e+00,\n",
      "        -3.0094e+00,  2.0265e+00,  5.0624e+00, -1.6511e+00,  1.3980e+00,\n",
      "        -1.5678e+00,  3.4822e+00, -3.7313e+00,  6.3424e+00,  2.6572e+00,\n",
      "        -5.6988e+00,  1.6478e-01,  5.8050e-01,  4.5561e+00,  3.4308e+00,\n",
      "        -1.9348e+00,  6.4955e+00,  1.9993e+00,  3.4664e+00,  1.1618e+00,\n",
      "         2.0522e+00,  2.2624e+00,  4.1647e+00, -3.8520e+00,  6.3257e+00,\n",
      "        -3.3091e-01,  2.3852e+00,  7.0001e-02, -4.2246e+00, -9.2681e+00,\n",
      "        -1.7196e-01, -8.3950e-01, -1.0774e+00,  1.7218e+00, -1.1135e+00,\n",
      "        -2.5927e+00,  4.4034e+00,  7.4703e-01, -1.0076e-01, -1.8658e+00,\n",
      "         1.5608e+00,  7.4284e+00,  8.0560e+00, -6.7068e-01,  4.6330e+00,\n",
      "        -2.8679e+00,  9.7087e-01, -1.2751e+00, -2.6933e-02, -7.6210e+00,\n",
      "        -3.1978e+00, -5.7582e+00, -8.8414e+00, -2.3240e+00, -8.3657e-01,\n",
      "        -3.7112e+00,  2.3510e+00, -3.4441e+00, -1.4957e+00,  1.8680e+00,\n",
      "        -6.4914e-01,  5.2631e+00,  1.4055e+00,  1.4343e+00,  6.2031e+00,\n",
      "         3.2263e-01,  7.1986e-01, -3.3497e+00,  5.2781e+00, -2.0148e+00,\n",
      "         9.2892e-01,  1.8375e+00,  4.1965e+00,  2.4361e+00, -3.8927e+00,\n",
      "        -1.6976e+00, -4.0349e+00, -1.1983e+00, -1.6589e+00, -5.9584e+00,\n",
      "         2.2770e-02, -6.2872e-02,  1.0817e+00, -3.5063e+00,  3.0854e-01,\n",
      "         1.0663e-01, -8.5329e-02, -2.8056e+00,  1.9624e+00,  9.1626e+00,\n",
      "         5.3828e+00,  2.4524e+00,  6.2560e+00, -6.0847e+00,  2.6875e+00,\n",
      "        -1.8888e+00,  2.6180e+00, -1.0192e+00,  1.8928e+00,  6.1438e+00,\n",
      "         1.1989e+00, -1.4508e+00,  2.8505e+00,  3.9434e+00,  8.1090e-01,\n",
      "         3.0524e+00,  4.1107e+00,  1.9565e+00,  6.6892e-01, -5.8196e+00,\n",
      "        -5.4994e-01, -3.0158e+00,  4.3137e-02, -1.9308e+00,  2.5785e+00,\n",
      "        -5.5248e-01, -1.5829e-01,  6.0883e-01, -3.6279e+00,  2.9361e+00,\n",
      "         2.8819e+00,  1.8848e+00, -1.8269e+00,  4.4456e+00,  1.4964e+00,\n",
      "         2.3559e+00,  2.8386e+00, -1.1184e+00,  6.9954e-01, -5.3147e+00,\n",
      "        -2.0223e-01, -6.5192e+00, -2.9826e-03, -6.1818e+00, -1.0043e+00,\n",
      "        -1.8644e+00,  1.1995e+00, -1.5130e+00,  2.9251e+00,  6.8909e-01,\n",
      "        -3.3229e-01,  2.0579e+00, -1.9918e+00, -6.3983e+00,  1.5820e+00,\n",
      "        -1.8549e+00,  5.1048e+00,  6.5702e-01,  3.8572e-01,  1.0853e+01,\n",
      "         4.2606e+00,  7.0870e+00,  7.2006e-01,  5.0863e+00, -3.6083e+00,\n",
      "        -4.3109e+00, -2.9254e+00,  7.9206e-02, -6.5374e+00,  8.2698e+00,\n",
      "         8.3765e+00,  3.1523e+00,  4.3179e+00, -6.9291e+00, -5.7146e-01,\n",
      "         4.1675e-01,  8.5232e-01, -7.1066e+00,  6.6067e+00,  5.4377e+00,\n",
      "         9.4504e-01,  2.5154e+00, -4.8530e+00, -6.5161e+00,  8.0458e+00,\n",
      "         1.3343e+00,  7.0417e+00, -5.2461e-01, -7.5717e-01, -3.0376e+00,\n",
      "        -6.4579e+00,  4.6384e+00, -3.2064e-01,  6.8399e+00,  5.4367e+00,\n",
      "         4.1278e+00,  7.6766e-01, -2.1928e-01,  8.0883e+00, -3.2628e+00,\n",
      "         1.4437e-01,  4.0816e+00,  3.9095e+00,  1.7602e+00, -1.4343e+00,\n",
      "        -1.6665e+00, -8.4307e-03, -2.7171e+00, -9.3143e-01, -1.2625e+00,\n",
      "         4.6817e+00, -6.5978e-01,  6.3072e+00,  5.1893e+00,  9.2511e-01,\n",
      "         5.7323e-01, -2.9279e+00, -2.0089e+00,  2.4654e+00, -1.7888e+00,\n",
      "         2.5497e+00,  2.8937e+00,  4.3658e+00, -2.9963e+00, -1.8934e+00,\n",
      "        -4.1100e-01,  6.4432e-01]), 'last_per_encoder_features': tensor([-1.1956e+00, -1.4257e-01, -1.4800e+00, -1.0203e+00,  6.1449e-01,\n",
      "         1.4512e+00,  2.5570e+00, -1.9612e+00, -8.3595e-01,  1.2635e+00,\n",
      "        -6.7601e-03, -1.8668e+00,  1.0171e+00,  8.9388e-01,  3.3275e+00,\n",
      "         1.4547e+00,  6.3957e-02, -2.2506e+00,  5.1887e+00,  7.2922e+00,\n",
      "         8.2894e-01,  2.9701e+00,  2.3753e+00, -2.0052e+00, -3.9808e+00,\n",
      "        -3.4508e+00,  4.1967e+00, -2.4241e+00,  1.6925e+00, -1.7567e+00,\n",
      "         5.1017e+00, -3.2309e+00, -2.0472e+00,  5.3050e+00,  2.2768e+00,\n",
      "         2.0056e-01,  2.7105e+00, -8.6809e-01,  4.8544e-01, -1.0089e+00,\n",
      "        -8.0589e-02, -2.2238e+00, -2.2309e+00,  1.5864e+00, -5.0825e-01,\n",
      "        -1.2272e+00,  3.4127e-02,  1.0247e-01, -4.6980e+00,  2.4505e+00,\n",
      "         1.4319e+00, -3.1828e+00,  2.2441e+00, -1.8705e+00, -6.9859e-01,\n",
      "         7.7636e-01,  3.3317e+00,  8.1688e-01,  2.2520e+00, -2.7532e+00,\n",
      "        -4.3729e+00,  7.7408e+00, -2.4143e+00, -1.7663e+00,  4.6938e+00,\n",
      "         1.2273e+00,  8.8919e-01,  1.1716e+00, -1.7520e+00, -3.1161e+00,\n",
      "        -4.1274e+00,  4.2425e+00,  1.3422e+00,  7.5631e+00, -2.3085e+00,\n",
      "         3.2478e+00, -5.6842e+00, -4.8356e-01, -7.7743e-01,  2.1669e+00,\n",
      "         1.1606e+00, -1.3926e+00, -3.3021e-01, -6.5281e-01,  2.4478e+00,\n",
      "         3.1388e+00,  1.2902e+00,  1.2050e+00,  2.6362e+00,  2.7109e+00,\n",
      "         1.5126e+00, -1.6687e-01, -4.7609e-01,  5.1137e+00, -4.6336e+00,\n",
      "        -6.1363e-01, -1.1604e-01, -1.6072e+00, -4.3464e+00,  6.4762e-01,\n",
      "         1.1588e-01,  2.2450e-01, -8.6734e-01, -9.1110e-01, -2.2721e+00,\n",
      "        -1.1587e+00, -6.2232e-01,  7.5925e-01,  2.1445e+00, -2.9712e+00,\n",
      "         4.1312e+00, -7.3622e-01, -3.4427e+00, -1.6633e+00,  3.1826e+00,\n",
      "        -6.9155e-01,  1.3750e+00, -3.2018e+00,  1.6784e+00,  9.4154e-01,\n",
      "        -8.0728e-01, -4.5005e+00, -4.1519e+00,  1.2112e+00, -1.1257e+00,\n",
      "        -2.9794e+00,  2.9857e+00,  8.5248e-01,  2.2703e+00, -2.9965e+00,\n",
      "        -1.0371e+00,  4.5198e+00,  8.4541e-01,  2.9599e+00,  1.2485e+00,\n",
      "        -2.5343e+00,  7.1509e+00, -3.8614e-01, -3.1841e+00,  2.5762e+00,\n",
      "         5.0922e-01,  5.3787e-01, -4.7055e-01,  1.4044e+00,  5.0434e-01,\n",
      "         3.1450e+00,  2.7750e+00,  1.5062e-01,  2.9778e-01, -2.1670e-01,\n",
      "         3.1531e+00,  3.1851e+00,  4.5128e-01, -3.7008e+00, -1.4875e+00,\n",
      "        -3.0960e+00,  3.4612e+00,  2.9452e+00, -1.1490e+00, -2.7636e+00,\n",
      "        -2.2861e+00,  1.6817e+00, -3.7541e+00,  5.3285e+00,  4.3442e+00,\n",
      "         1.3789e+00, -5.4005e-01,  1.5185e+00, -1.0920e+00,  7.2181e-01,\n",
      "         2.2241e+00, -2.0053e+00, -9.0704e-01, -2.3622e+00, -5.9643e+00,\n",
      "         4.2818e+00, -1.8112e+00,  1.1493e+00, -3.9741e-01, -6.4411e-01,\n",
      "        -1.8526e+00, -1.2493e+00, -5.5603e+00, -1.5978e+00,  1.4695e-01,\n",
      "        -4.9023e+00, -6.1502e+00,  2.2660e+00,  4.8033e-01, -1.4105e+00,\n",
      "         1.0854e+00,  1.7496e-01,  1.9329e+00,  7.8413e-01, -3.6434e+00,\n",
      "         2.4942e+00,  5.0837e+00, -1.0060e+00, -4.1455e-01, -1.3801e+00,\n",
      "         1.3972e+00,  1.3569e-01, -2.1337e-01,  4.3847e-01, -5.4291e-01,\n",
      "         2.3687e+00, -4.8644e+00,  1.1332e+00, -9.5954e-01,  1.6698e-01,\n",
      "         3.3734e+00,  3.5599e+00, -3.0967e+00,  2.4895e+00,  1.8307e-01,\n",
      "         2.3437e+00, -1.6860e+00, -7.4562e-01,  4.1024e+00,  1.3354e+00,\n",
      "        -4.3255e+00, -4.0565e-02,  1.3429e+00, -3.6988e+00,  4.1463e+00,\n",
      "        -3.8066e+00, -5.5523e+00,  1.1830e+00, -3.4526e-01, -4.3035e+00,\n",
      "        -3.1081e+00, -1.9549e+00,  5.2116e-02, -2.4909e+00,  3.6313e+00,\n",
      "         3.7308e+00, -1.1656e+00,  2.7124e+00,  2.8690e+00, -3.7376e+00,\n",
      "         1.6889e+00,  1.5041e+00,  1.1491e+00,  5.2397e+00,  4.2034e-01,\n",
      "         5.3707e-01,  3.2612e+00, -3.0864e+00, -3.5407e+00,  2.2537e+00,\n",
      "         1.8376e+00,  5.2764e+00,  3.7649e-01,  1.4560e+00,  9.8268e-01,\n",
      "        -5.1011e+00,  5.9703e-01,  4.1511e+00,  1.6714e+00, -6.3416e+00,\n",
      "        -4.4849e+00, -1.9616e+00, -8.7672e+00, -2.0786e+00, -4.9608e+00,\n",
      "         4.0571e+00,  1.8596e+00,  3.4207e-02, -5.0172e+00,  1.4763e+00,\n",
      "         3.2753e+00,  2.0708e+00, -2.2833e+00,  6.5065e-01, -8.9259e-01,\n",
      "         6.1852e-01, -7.1792e+00,  9.2044e-01,  5.6267e-01, -4.6747e-01,\n",
      "         2.0115e+00, -2.1248e+00, -3.7582e-01,  8.7119e-01, -4.9023e+00,\n",
      "        -8.1783e-01, -2.2953e+00, -8.9224e-01, -3.7232e+00,  5.6078e+00,\n",
      "         2.1440e+00, -2.3636e+00, -1.0670e+00, -6.5007e+00, -3.1750e+00,\n",
      "        -2.7770e+00,  1.2156e+00, -2.8212e+00,  3.0820e+00,  2.1906e+00,\n",
      "         5.3644e-01,  4.0787e+00, -1.5636e+00,  1.4104e+00, -5.8426e+00,\n",
      "         4.5281e+00,  2.5768e-01, -5.3646e-01, -2.9252e-01,  1.0354e+01,\n",
      "         3.0106e+00, -7.3969e-01, -4.3983e+00, -2.3761e-01, -4.1746e+00,\n",
      "        -2.1513e+00,  2.7986e+00,  4.4146e+00, -3.1968e+00,  5.6137e+00,\n",
      "        -2.5926e+00, -1.4483e+00,  2.9632e+00, -9.5199e-01, -3.5649e+00,\n",
      "        -9.1007e-01,  5.8761e-01,  2.9677e+00,  2.1091e+00, -9.6730e-01,\n",
      "        -1.1950e+00,  4.2964e+00,  8.8464e-01, -5.6999e-01, -8.4325e-01,\n",
      "        -2.4963e-01, -3.9704e+00,  4.7755e+00,  6.7932e-01, -1.9336e-01,\n",
      "         8.6468e-01,  4.5822e+00, -4.6049e+00, -2.9996e+00,  3.7518e+00,\n",
      "        -4.9539e+00,  4.5371e+00, -1.8402e+00,  1.2579e-01, -7.8131e-01,\n",
      "         9.9778e-01,  2.1694e+00,  3.3439e-01, -4.8620e+00,  8.2149e-01,\n",
      "        -1.1994e+00, -2.2537e+00,  3.5201e+00,  2.2295e+00, -2.5026e+00,\n",
      "        -1.7625e+00, -2.2471e+00, -2.9352e+00,  2.9395e+00,  1.4159e+00,\n",
      "         1.7875e+00, -3.8404e+00, -2.4462e+00, -3.1560e+00, -2.6114e+00,\n",
      "         2.9903e+00,  1.0480e+00,  6.9566e-01,  3.4284e-01, -1.6339e-01,\n",
      "         5.3163e+00,  5.0338e-03, -2.9113e+00,  1.0665e+00, -1.5730e+00,\n",
      "         2.5091e+00, -2.2708e+00,  3.2762e+00, -4.0386e+00, -6.6649e-01,\n",
      "         1.1057e+00,  1.3895e+00,  3.7091e+00, -6.7106e+00, -2.0928e-02,\n",
      "         4.1830e+00, -8.3880e-01, -1.2823e+00,  9.2604e-01, -1.0632e+00,\n",
      "         1.5000e+00,  3.3975e+00,  1.4446e+00,  2.8187e+00, -1.4946e+00,\n",
      "        -4.7649e+00, -8.6203e-02, -1.9486e+00, -6.2698e-02,  1.8385e+00,\n",
      "        -6.4872e-01,  2.5066e+00, -3.6508e+00,  1.9902e-01, -1.0010e+00,\n",
      "         9.9560e-01, -2.5517e+00,  1.4461e+00, -9.7284e-01, -3.8140e+00,\n",
      "        -1.5772e+00, -3.4730e+00, -7.8874e+00, -1.0917e+00, -1.1314e-01,\n",
      "         3.2832e+00,  2.1131e-01, -3.9033e+00, -2.1121e-01,  2.6342e+00,\n",
      "        -9.2859e-01,  8.8836e-01,  4.2129e-01,  2.2333e+00,  1.2740e+00,\n",
      "         2.6908e+00,  2.7322e+00, -1.2759e+00,  7.4289e-01,  4.1224e+00,\n",
      "         1.7729e+00, -8.6192e-01, -3.6214e+00,  9.1707e-01, -1.0902e+00,\n",
      "         9.9702e-01,  1.7284e+00, -2.2635e+00, -1.3035e+00, -7.8553e-01,\n",
      "        -2.0401e+00, -2.0716e+00, -2.5511e+00,  2.0938e+00,  8.9188e-01,\n",
      "        -3.0981e+00,  1.3676e-01, -2.9521e+00,  4.0491e+00, -3.5398e-01,\n",
      "        -4.2148e-01,  2.9475e+00,  3.6427e+00, -2.1459e+00, -4.1912e+00,\n",
      "         2.4158e+00,  7.1285e+00,  1.1886e+00,  1.3678e+00,  1.7753e+00,\n",
      "         4.1111e-01, -1.9923e+00,  7.5460e-01, -2.0834e-01,  6.4048e-01,\n",
      "         3.0398e-02,  3.5322e+00, -2.1514e-02,  4.7102e+00,  2.8624e+00,\n",
      "        -1.9120e+00,  8.3368e-01,  3.9288e+00, -6.9795e+00,  2.5870e+00,\n",
      "        -5.1068e+00, -3.6768e+00, -2.8805e-02, -1.4011e+00, -7.8055e-01,\n",
      "         2.3593e+00,  1.0439e-01, -4.6426e+00,  3.4683e-01,  2.9670e-01,\n",
      "        -6.1627e-01,  1.2926e+00, -1.6850e+00, -8.4600e-01,  1.4434e+00,\n",
      "        -7.8742e+00,  2.2936e+00, -1.4492e+00,  4.1491e+00, -2.3288e+00,\n",
      "         3.2073e+00,  1.3170e+00, -7.7178e-04, -1.5989e+00,  1.3186e+00,\n",
      "        -1.8878e+00, -4.9344e+00,  3.5554e+00,  1.0948e+00, -5.0737e+00,\n",
      "         2.8131e+00,  1.8211e+00])}, 'scene': {'emotion_logits': tensor([ 0.4340,  0.8846, -1.4251,  0.3228, -0.6891,  1.5473, -1.7124]), 'personality_scores': tensor([0.3037, 0.6032, 0.2477, 0.4853, 0.3627]), 'last_emo_encoder_features': tensor([ 1.2217e+00, -8.9602e+00, -1.7551e+00, -4.0319e+00,  2.2815e+00,\n",
      "         2.9453e+00,  2.7794e+00, -3.7807e-01, -2.7433e+00, -6.4504e+00,\n",
      "        -3.9900e+00, -8.2831e+00,  2.6902e-01, -2.3909e+00,  9.1235e+00,\n",
      "         2.5213e+00, -1.8649e-01,  4.5236e+00,  1.5040e-01, -1.3834e+00,\n",
      "        -2.3296e+00,  3.2758e+00, -6.9711e+00, -1.4475e+00,  1.7681e+00,\n",
      "        -6.1205e+00, -4.5116e-01, -9.6991e+00, -5.7082e+00,  1.7945e+00,\n",
      "         3.4375e+00,  8.1405e-01,  8.8073e+00,  7.9224e-01, -5.8375e-01,\n",
      "        -1.8409e+00, -6.5707e+00, -7.6375e+00, -4.8186e+00, -4.9546e+00,\n",
      "        -6.9039e+00,  9.7882e-01, -1.5009e+00,  7.6022e+00, -1.1113e+01,\n",
      "         5.8002e+00, -1.1282e+01,  4.6391e-01,  3.1907e+00, -3.7432e+00,\n",
      "        -3.9211e+00,  1.4173e+00, -4.4895e+00, -1.8551e+00, -2.8924e+00,\n",
      "        -6.8016e+00,  4.7094e+00, -1.7210e-01,  7.5409e-01, -2.7515e+00,\n",
      "        -1.9702e+00, -8.5547e-01,  1.1245e+01,  1.1329e+00, -3.6584e+00,\n",
      "        -4.3793e+00,  6.5021e-01, -1.0507e+01, -5.2010e+00, -1.5302e+01,\n",
      "         2.4888e+00, -9.5641e+00, -1.0418e+01, -1.6824e+00,  2.1268e+00,\n",
      "        -1.3275e+00, -2.9629e+00,  1.1652e+01, -4.5521e+00,  5.1430e+00,\n",
      "        -6.3036e-02, -3.3122e+00,  4.4246e+00,  5.1170e+00, -4.6569e-01,\n",
      "        -5.7971e-01, -3.7886e-02, -3.0951e+00, -9.3335e-01, -4.3930e+00,\n",
      "         1.8401e+00, -6.3377e+00, -6.8257e+00, -7.0588e+00, -9.0184e-01,\n",
      "        -4.7097e+00, -2.0480e+00,  9.6633e+00, -2.7481e+00, -6.2915e+00,\n",
      "        -3.0376e+00, -1.0228e+01, -2.4418e+00,  4.8489e+00, -6.0519e+00,\n",
      "        -2.4313e+00,  6.0970e+00,  8.5645e+00, -6.3870e+00,  8.4453e+00,\n",
      "         8.7474e-01, -1.0864e+01, -3.0926e+00,  5.3285e+00, -7.6918e+00,\n",
      "        -1.4152e+00,  1.2937e+01,  6.3798e+00, -1.7780e+00, -9.4001e+00,\n",
      "        -5.7429e+00, -1.5135e+00,  5.5344e+00, -4.0834e+00, -5.9259e+00,\n",
      "        -3.2774e+00,  1.4065e-01, -1.2872e+00,  5.6771e+00, -1.1437e+01,\n",
      "        -5.4436e+00,  4.2994e+00,  1.9486e+00, -4.8808e+00,  8.1240e-02,\n",
      "        -5.0294e-01,  2.8486e+00, -6.3830e+00,  3.4106e+00,  1.9890e-03,\n",
      "        -3.3673e-01,  7.7208e+00,  2.4456e+00, -1.0594e+01,  2.4043e+00,\n",
      "        -3.9086e+00, -4.6095e+00,  1.9967e+00, -5.8392e+00, -1.0830e+01,\n",
      "         9.4867e+00,  5.7355e-01, -9.0110e+00,  6.7070e-01,  5.3583e+00,\n",
      "        -1.1271e+00, -3.3653e+00, -5.1677e+00, -9.3209e+00,  8.6048e+00,\n",
      "         1.0556e+01, -3.5906e+00,  1.8631e+00, -4.6639e+00,  1.9642e+00,\n",
      "         8.8270e+00,  4.6348e+00,  1.2815e+00, -8.7714e+00, -1.0297e+01,\n",
      "        -4.2200e-01, -4.8159e+00,  8.5046e+00,  4.2125e-01, -9.0848e+00,\n",
      "         8.9708e+00, -5.0332e+00, -5.6349e-02, -5.8743e+00, -4.7551e+00,\n",
      "         1.0420e+01, -2.5725e+00,  9.0946e+00,  1.7001e+00, -2.7871e+00,\n",
      "         6.2071e+00, -1.1256e+00,  3.7818e+00,  1.7232e+00, -8.2791e+00,\n",
      "        -7.1448e+00,  1.0707e+00,  3.2924e+00, -7.3003e+00,  1.0440e+01,\n",
      "        -6.1808e-01, -9.2407e+00,  3.6726e+00,  9.1212e+00,  1.2026e+01,\n",
      "         7.5204e+00, -8.2279e-01,  1.2117e+00, -6.2814e+00,  5.7574e+00,\n",
      "        -2.3413e+00,  1.0452e+01,  8.5774e+00,  2.6902e+00,  3.7137e+00,\n",
      "        -3.1037e+00,  2.7199e+00,  1.2827e+00,  2.9258e+00,  3.4275e+00,\n",
      "         3.3025e+00, -1.6017e+00,  6.8913e+00, -3.1167e+00, -2.2015e+00,\n",
      "        -2.7570e+00,  6.5389e+00,  1.0044e+00, -1.4165e+00, -1.7763e+00,\n",
      "        -2.6450e+00,  4.6008e+00,  3.6367e+00,  2.1782e-01,  7.2253e+00,\n",
      "         3.7917e+00, -5.8991e+00, -7.2645e+00, -5.5821e+00,  2.6666e+00,\n",
      "        -2.4436e+00,  1.8438e+00,  7.5079e+00, -1.3923e+00, -6.2474e+00,\n",
      "         1.1648e+01,  7.0364e+00, -6.9127e-01,  2.8601e+00,  4.4659e+00,\n",
      "         1.1924e+00,  1.1235e+00, -3.7256e-01, -6.5801e+00, -6.4200e+00,\n",
      "        -2.6790e+00,  6.4258e+00, -1.4457e+00, -1.6862e+00, -2.4093e+00,\n",
      "         4.8780e+00,  4.4683e+00,  2.2484e+00,  8.8936e+00,  5.6933e+00,\n",
      "         3.5838e-01, -4.2888e-01,  5.8465e+00, -2.1255e-02, -7.8355e+00,\n",
      "        -4.4258e+00, -7.3730e+00,  1.9342e+00,  1.2140e+00, -5.2888e+00,\n",
      "        -7.3742e+00,  6.7059e+00,  1.0309e+01,  1.2673e+01, -7.9654e+00,\n",
      "         7.7794e+00, -4.1205e+00, -1.7468e+00, -9.3065e-01,  8.8445e+00,\n",
      "        -6.2238e+00, -1.2452e+00,  1.4563e+00,  8.1812e-01,  2.5104e+00,\n",
      "        -1.7283e+00,  2.8316e+00,  4.5145e+00,  1.1206e+01,  2.5550e+00,\n",
      "         3.6423e+00,  3.8213e+00,  6.3788e+00,  3.4287e-01, -8.8371e+00,\n",
      "         2.3005e+00, -7.2260e+00, -5.6899e+00, -8.8060e+00, -2.1667e+00,\n",
      "         4.7845e+00, -2.7188e+00,  1.4144e-01,  6.9757e+00,  4.1029e-01,\n",
      "        -5.6272e+00,  1.1212e+00,  7.5376e+00, -4.0073e+00,  2.8744e+00,\n",
      "        -7.6239e+00,  2.9869e+00,  1.2012e+01, -3.0944e+00,  2.3374e+00,\n",
      "         2.4542e+00, -5.3905e+00,  1.3843e+00, -6.7923e+00,  6.7458e+00,\n",
      "         1.7607e+00, -3.7648e+00,  8.3726e+00, -5.2702e+00, -1.2475e+01,\n",
      "        -1.4132e+00, -5.9330e-01,  7.5488e+00, -2.7223e-01,  2.6254e+00,\n",
      "         3.0209e+00,  1.8855e+00,  5.3436e+00,  7.5094e+00, -4.8222e+00,\n",
      "        -3.1615e+00, -5.5330e+00,  3.9027e+00, -1.3401e+01,  2.4370e+00,\n",
      "         5.5299e-01,  1.2234e+00,  1.2659e+01, -1.3771e-01, -2.4566e+00,\n",
      "         6.2265e+00,  4.6051e+00,  7.7095e-01,  2.0200e+00, -3.0239e-01,\n",
      "        -1.2289e+00,  8.8080e+00, -1.9205e+00,  7.6920e+00,  6.8637e+00,\n",
      "        -1.0710e+00,  3.9175e+00,  4.2594e+00,  2.2631e-01,  3.1262e+00,\n",
      "        -1.2051e+01,  5.5550e+00,  3.2718e+00, -6.8487e+00, -3.7159e+00,\n",
      "        -7.6867e+00, -6.9791e+00,  4.7470e+00,  3.2389e+00,  3.4066e+00,\n",
      "        -2.4775e-01,  2.1495e+00,  1.6185e+00,  2.0929e-01, -1.7861e-02,\n",
      "        -6.3839e+00,  5.9498e-01,  8.5504e+00,  5.5596e+00, -1.0089e+00,\n",
      "        -6.4706e+00,  1.5331e+01,  4.2172e+00,  1.3346e+01, -3.5894e+00,\n",
      "         2.0100e+00, -1.4589e+01, -8.2423e+00,  5.5055e+00,  1.2651e+01,\n",
      "        -1.7630e+00,  6.4116e+00,  7.8224e-01,  6.2216e+00, -4.6620e+00,\n",
      "         2.1835e+00,  1.5931e+00,  2.6112e+00,  3.0046e+00,  4.6356e+00,\n",
      "         4.4772e+00, -4.9113e+00, -3.0065e+00,  6.8660e-01, -4.5388e+00,\n",
      "        -3.0267e-01,  1.1093e+01,  9.7616e+00,  5.3064e+00, -3.3494e+00,\n",
      "        -4.1876e+00, -4.7406e+00,  1.5060e+00,  4.3726e+00, -1.5205e+00,\n",
      "        -1.5068e+01, -3.6817e+00, -3.5107e+00,  2.4940e+00, -1.1031e+01,\n",
      "        -7.9549e+00,  5.2045e-01, -4.6977e+00, -5.1406e+00,  9.3801e+00,\n",
      "        -3.2354e+00,  3.7903e+00,  3.1685e+00, -1.7297e+01,  1.7925e+00,\n",
      "         7.6932e+00,  6.0081e+00,  2.1452e+00,  2.6669e+00, -5.9290e+00,\n",
      "         1.3378e+01, -2.2700e+00,  1.6147e+00,  9.9213e+00,  9.8308e+00,\n",
      "         2.8789e+00, -4.5569e-01, -3.3837e+00,  1.9135e+00, -5.4969e+00,\n",
      "        -3.8451e-01, -2.1745e+00, -1.2010e-01, -5.0566e+00, -2.6929e+00,\n",
      "         3.7594e+00, -1.0827e+01,  5.9425e+00,  1.2104e+01,  4.3683e+00,\n",
      "         6.4608e+00,  5.4284e+00, -1.2010e+00, -2.4383e+00,  2.7006e+00,\n",
      "        -2.4887e+00, -3.1846e+00, -3.4277e+00, -2.7606e+00, -1.8824e+00,\n",
      "         5.2940e-01, -1.8982e+00, -1.1574e+01, -1.0739e+01,  6.7891e+00,\n",
      "         6.5938e+00, -2.4923e-01, -9.0324e-02,  7.3374e+00, -7.3537e+00,\n",
      "        -6.5008e+00,  4.9910e+00,  8.1479e+00, -3.7358e-01,  8.2054e+00,\n",
      "        -3.1079e+00,  7.8963e+00,  9.1582e+00, -4.9025e+00,  1.1761e+00,\n",
      "         4.0979e-01, -2.8927e+00,  5.5592e+00, -3.1613e+00,  7.2585e+00,\n",
      "         1.4196e+00, -7.1262e-01, -3.0269e+00,  1.2238e+00, -4.2195e+00,\n",
      "         1.2351e+00, -9.8858e-01, -1.1431e+00, -6.8081e+00,  8.9447e+00,\n",
      "        -2.8946e+00,  8.4672e-01,  6.3097e-01,  8.5769e+00,  6.0293e-01,\n",
      "        -3.2654e+00,  4.7155e+00, -2.5350e+00, -8.4108e+00,  6.6807e-01,\n",
      "         5.0546e+00,  3.8142e+00, -4.6141e+00, -3.5147e+00, -1.0086e+01,\n",
      "        -5.4860e+00,  4.8984e+00, -2.3355e+00,  2.3893e+00,  7.4647e-01,\n",
      "         1.9526e+00, -5.4756e-01, -1.3231e+00,  9.3781e-01,  1.4491e+00,\n",
      "        -3.3552e+00,  6.6459e+00, -3.4361e+00,  1.6148e+00, -1.8817e+00,\n",
      "        -4.7850e+00, -6.4652e-01, -9.6941e+00, -2.1499e+00,  5.6858e+00,\n",
      "        -3.7792e-01,  6.3642e+00, -2.5013e+00, -5.6793e+00, -6.8593e+00,\n",
      "        -9.8514e+00,  6.8960e-01, -2.0991e+00, -2.0142e+00, -1.0609e+01,\n",
      "         6.9789e+00, -2.8329e-01, -8.2473e+00,  6.8587e+00, -2.0356e+00,\n",
      "         5.4948e+00, -2.8449e-01,  1.2301e+00, -6.9751e-01,  9.9070e-01,\n",
      "         8.9922e+00, -4.7860e+00, -1.4071e+00, -2.9924e+00,  7.7300e+00,\n",
      "        -5.1595e-02, -3.5635e+00, -1.3328e+00, -5.9459e-01, -3.1563e+00,\n",
      "         1.4040e+00, -3.0574e+00, -2.1791e+00, -1.3479e+00, -1.1660e+00,\n",
      "         1.1131e+01,  3.6063e+00, -2.1863e+00,  2.2527e+00, -3.5165e+00,\n",
      "         1.0305e+00, -6.2277e+00,  5.5108e+00,  3.9454e+00,  4.5599e+00,\n",
      "        -6.6983e-01,  1.2683e+01, -2.0384e+00,  4.0517e-01,  4.1845e+00,\n",
      "         8.4457e+00, -1.0848e+00,  1.9968e+00, -2.6238e+00, -1.1490e+01,\n",
      "         6.8016e+00,  3.4315e+00,  3.5205e-01, -3.4590e-01, -3.4651e+00,\n",
      "         1.7326e+00,  1.0789e+00,  6.7501e+00, -5.0178e+00,  2.4352e+00,\n",
      "        -4.3006e+00,  4.5153e-01,  2.0113e+00, -2.0870e+00, -3.0422e-01,\n",
      "         7.5339e+00, -1.0622e+01, -3.5743e+00,  4.8818e+00,  1.9492e+00,\n",
      "         1.4579e-01, -8.1872e+00,  4.6323e+00,  1.5995e+01, -2.2408e+00,\n",
      "         5.9435e+00,  5.3168e+00, -2.9048e+00, -2.2277e-01,  3.2686e+00,\n",
      "        -3.8679e-01, -3.3821e+00, -1.8525e-01,  2.9899e+00, -1.0259e+01,\n",
      "        -3.0591e+00,  3.2534e+00,  5.0183e+00,  2.1025e+00,  2.1684e+00,\n",
      "        -2.3563e+00,  1.8194e+00,  8.5576e+00, -1.3071e+00, -1.2467e+01,\n",
      "        -6.3604e+00, -1.0367e+00,  3.7884e+00, -2.3507e+00, -1.6568e+00,\n",
      "         6.6598e+00,  6.8683e+00,  1.4847e+00,  2.6437e+00,  6.8087e+00,\n",
      "        -2.7819e+00, -4.2079e+00,  2.7172e+00, -2.5094e+00,  1.1810e+01,\n",
      "         2.7841e+00,  2.6252e+00,  1.8891e+00,  3.5404e+00, -1.2818e+01,\n",
      "         3.8977e+00, -2.3633e+00, -5.2806e+00, -6.9943e+00,  1.0293e+01,\n",
      "         1.0911e+01,  8.4429e+00, -1.7468e-01, -6.9942e+00, -1.4680e+00,\n",
      "         2.8720e+00,  1.2538e+00, -2.8429e+00,  5.9695e+00,  1.1043e+01,\n",
      "         1.0275e+01, -4.3886e+00,  4.6166e+00, -2.5425e+00,  6.7124e+00,\n",
      "        -6.3402e-01, -9.8085e+00, -7.9306e+00,  4.9513e+00, -3.5772e+00,\n",
      "         2.6235e+00,  4.4676e+00,  5.8254e+00, -2.3423e+00, -1.0887e+01,\n",
      "        -1.8227e+00,  2.0273e+00, -1.1722e+01,  9.7483e+00,  1.8299e+00,\n",
      "        -1.1624e+00, -6.4352e+00, -2.9325e+00,  1.0306e+01,  4.0449e+00,\n",
      "        -2.5185e+00, -3.3362e+00,  1.6176e+00, -7.0300e+00,  2.2775e+00,\n",
      "        -4.0481e+00, -2.6464e+00,  2.2618e+00,  2.5917e+00, -1.3160e+00,\n",
      "        -1.5104e+00, -1.0487e+01,  4.8516e+00, -8.2209e+00, -1.5418e+01,\n",
      "        -1.6062e+00, -1.1603e+01,  8.2033e+00, -8.8305e+00, -6.1379e-01,\n",
      "         2.1584e+00,  3.1527e+00,  1.4028e+00,  3.3513e+00, -5.7012e+00,\n",
      "        -1.7114e+00,  6.9216e+00,  1.3378e+00,  1.7218e+00,  2.5845e-02,\n",
      "         5.6218e+00,  5.0038e+00,  2.8219e+00, -1.8279e-01,  1.4583e+00,\n",
      "         1.8924e+00,  3.7013e+00,  5.6275e+00, -3.3774e+00, -1.2231e+01,\n",
      "        -3.0961e+00,  3.7815e+00, -1.7560e+00, -1.4537e+01, -2.1641e+00,\n",
      "         1.9368e+00, -1.9701e+00,  4.6438e-01, -5.7024e+00, -1.7770e+00,\n",
      "         4.8117e+00, -5.7986e+00,  5.8175e+00, -1.8358e+00,  1.9073e+00,\n",
      "        -6.3955e+00,  5.4007e-01, -7.1028e+00, -1.3466e+00,  9.2929e+00,\n",
      "        -6.9603e+00, -1.3135e+01, -1.3302e+00,  6.8117e+00,  1.2055e+00,\n",
      "        -1.3527e+00,  3.1760e-01,  1.8500e+00,  4.5776e+00,  3.0958e+00,\n",
      "        -8.1687e+00,  3.8411e+00, -5.1250e+00]), 'last_per_encoder_features': tensor([ 4.5201e+00,  5.1942e+00, -4.5707e+00,  7.3472e-01, -1.8096e+00,\n",
      "        -9.0183e+00, -7.7191e+00,  2.1242e+00, -5.6698e+00,  3.5174e+00,\n",
      "        -1.9038e+00,  4.3116e+00, -5.2693e+00, -1.5951e+00, -5.5408e+00,\n",
      "         4.2053e+00, -1.3448e+01,  1.1094e+01, -1.7712e+00, -3.7969e+00,\n",
      "         6.8961e-01, -1.1966e+00, -2.2038e+00,  2.1897e+00,  3.6671e+00,\n",
      "         2.2563e+00, -2.6904e+00,  1.1397e+00,  3.3585e+00,  7.9125e+00,\n",
      "        -9.2788e+00, -2.3659e+00, -6.6228e+00,  1.3333e+00,  6.6109e+00,\n",
      "        -9.5044e+00, -5.2969e-02,  5.4487e+00, -8.6163e+00, -8.9236e+00,\n",
      "        -6.1324e+00, -6.0055e+00, -9.3112e+00, -4.5179e+00, -2.1421e+00,\n",
      "        -4.4059e+00,  8.0847e-01,  7.9763e+00,  1.6104e+00, -5.0198e+00,\n",
      "         2.7316e+00, -7.0768e-01,  1.4445e+00, -1.0394e+00, -3.8230e-01,\n",
      "        -2.8979e+00,  1.5083e+00, -4.5411e-01,  7.2885e-01,  3.0477e+00,\n",
      "        -2.5524e+00, -8.3784e+00, -2.5075e+00,  2.4037e+00, -7.5999e+00,\n",
      "        -6.0773e+00,  7.3587e+00,  8.7723e-01,  3.9107e-01,  3.8267e+00,\n",
      "         6.8607e+00,  7.7275e+00, -4.7494e-01,  3.0111e+00, -1.1856e+00,\n",
      "         1.6877e+00,  2.3376e-01, -8.5098e-01,  1.8752e+00,  9.1950e-01,\n",
      "         1.3830e+00, -2.9078e+00, -5.3387e-01, -4.5310e+00,  2.5046e+00,\n",
      "         2.0158e+00, -7.7899e+00, -3.6840e+00,  1.1566e+01,  8.0151e-01,\n",
      "         5.8989e+00, -2.5297e+00,  1.7230e+00,  1.5812e+00, -4.1509e+00,\n",
      "        -5.5869e+00, -4.9673e+00,  3.6599e+00, -5.4359e+00, -8.8548e+00,\n",
      "         8.7994e+00, -5.2780e+00,  4.5500e+00, -4.0100e+00,  3.9064e-01,\n",
      "         2.6066e+00,  5.7258e+00, -5.6849e-01, -4.7480e+00,  2.8668e+00,\n",
      "        -3.3706e+00, -2.9285e+00,  9.7911e-01, -3.3062e+00,  5.3045e+00,\n",
      "         3.7942e-01,  4.6763e-01, -1.8668e+00,  8.2925e-01,  2.8480e+00,\n",
      "        -9.3774e-01,  5.2750e+00,  3.9696e+00,  5.2423e+00,  4.3611e+00,\n",
      "         4.2615e+00, -7.5280e-01, -3.5221e-01, -1.4926e-01,  7.2913e+00,\n",
      "         4.0564e-01, -1.2494e+00, -1.5096e+00, -6.9552e-01, -2.5281e+00,\n",
      "         5.3814e+00,  5.2195e+00, -4.8434e+00,  9.4976e-01, -3.0518e+00,\n",
      "        -6.5737e-01, -3.1558e+00,  6.3368e+00,  3.4367e+00, -5.4742e+00,\n",
      "        -2.4756e+00, -8.9766e+00, -2.0141e+00,  1.5673e+00,  6.0534e-01,\n",
      "        -5.0722e+00,  8.8375e-01, -4.1930e+00,  3.6838e-01, -3.4493e+00,\n",
      "         2.7877e+00, -7.5403e+00, -3.6960e+00, -5.2667e+00, -1.3885e+00,\n",
      "         4.1415e+00, -4.0861e+00,  3.2208e+00,  5.1656e+00,  9.3932e-02,\n",
      "         9.4379e+00,  1.3977e+01,  1.3983e+00, -2.3752e+00,  1.1207e+01,\n",
      "         2.6887e+00,  2.5905e+00, -3.6320e+00,  3.8610e+00,  5.8090e+00,\n",
      "         3.3347e+00, -3.0097e+00, -1.7908e+00,  1.2954e+00, -3.0814e+00,\n",
      "         7.3054e+00,  2.1738e+00, -1.6038e+00,  3.2450e+00, -4.9978e-01,\n",
      "        -3.0677e+00, -7.0729e+00, -1.4534e+00,  1.4923e+00,  6.0551e+00,\n",
      "         1.5771e+00,  2.4934e+00,  2.5713e+00,  3.2873e+00, -6.3567e+00,\n",
      "        -3.6071e+00, -5.6840e+00,  2.9053e+00,  4.3611e+00,  6.6235e+00,\n",
      "        -3.9984e+00,  9.2616e+00, -2.9759e+00,  7.4410e-01,  1.3060e+00,\n",
      "        -8.1639e+00, -3.3917e+00,  4.1268e+00, -1.6937e-01, -3.2361e-01,\n",
      "        -2.9280e+00,  4.3292e+00,  1.1432e+00, -1.9240e+00,  3.9335e+00,\n",
      "        -4.4853e-02,  3.2807e+00,  7.6804e-01,  1.2747e+01,  5.4010e-01,\n",
      "        -4.2382e-01, -5.0781e+00,  2.7268e+00, -4.7278e+00, -2.0230e+00,\n",
      "         1.1943e+00, -8.9545e+00,  5.5735e+00,  7.6956e+00, -1.1420e+00,\n",
      "        -1.0812e+00,  1.6610e+00,  6.5339e+00, -1.3163e+00,  3.7538e+00,\n",
      "         6.7883e+00,  4.1196e+00, -1.5216e-01, -1.2318e+00,  6.3709e+00,\n",
      "         5.1743e+00,  3.4850e-01, -3.5331e+00, -8.8532e+00, -9.5591e+00,\n",
      "        -2.7327e+00, -3.8167e+00,  7.9779e-01,  1.7630e+00, -8.8417e-02,\n",
      "        -4.2238e-01,  1.2917e+00, -1.2233e+00,  2.1290e+00, -6.9629e+00,\n",
      "        -4.5751e+00,  7.2036e+00, -4.8681e-01,  4.3724e+00, -5.1149e+00,\n",
      "        -5.2599e-01,  9.9783e-01,  7.5808e-01, -2.8651e+00,  3.6175e+00,\n",
      "         3.7714e+00, -1.0672e+00, -9.9646e+00,  1.8230e+00, -2.3727e+00,\n",
      "        -3.2894e+00, -1.5748e+00,  3.5672e-01, -1.0931e+00, -2.9637e+00,\n",
      "        -3.8143e+00,  3.4939e+00,  7.5502e+00,  3.9010e+00,  2.2294e+00,\n",
      "        -7.9996e+00,  2.9727e-01,  4.9412e+00,  5.0856e-01,  4.1064e+00,\n",
      "        -1.1064e+00, -4.1637e-01, -7.7077e+00,  9.7876e+00,  5.0591e+00,\n",
      "        -3.2524e+00, -1.0206e+01, -6.7940e+00,  1.1699e+00, -2.0022e+00,\n",
      "        -2.1715e+00,  8.4682e+00,  2.9849e+00, -1.2049e+00, -6.2802e+00,\n",
      "        -7.1563e-01,  1.9898e+00, -7.1180e+00,  8.6564e+00, -9.7644e-01,\n",
      "        -3.5700e+00, -1.2707e-01, -2.5993e+00, -9.2174e-01,  1.5704e+00,\n",
      "         2.9958e+00,  2.2914e+00,  8.0242e+00, -2.7513e+00,  4.5822e+00,\n",
      "        -8.1259e+00,  5.6880e+00,  2.4849e+00,  6.7958e-01, -2.3381e+00,\n",
      "        -1.1927e+00,  2.8535e+00,  8.6307e+00, -3.1599e+00, -1.1895e+00,\n",
      "        -4.9879e+00, -5.1416e+00,  1.1168e-01, -5.9050e+00, -2.3007e-01,\n",
      "         1.3126e+00, -1.6802e+00, -1.2102e+00,  3.0952e-01, -5.6566e+00,\n",
      "         2.4336e+00, -4.4125e+00, -6.6612e+00, -1.4992e+00, -1.8419e+00,\n",
      "        -9.3471e+00, -1.2356e+01,  3.3628e+00,  9.1651e+00,  5.6033e+00,\n",
      "         1.4792e+00, -3.4611e+00,  4.9550e+00, -1.6370e+00, -6.3226e+00,\n",
      "         1.2799e+00, -7.4378e+00, -3.2884e+00,  7.9174e+00, -7.7304e+00,\n",
      "         1.1117e+01,  1.9776e+00,  9.3973e-01, -1.6657e+00,  6.3677e+00,\n",
      "        -9.1364e+00, -4.8015e+00, -2.4661e+00, -3.3141e+00, -4.0770e+00,\n",
      "        -5.7710e+00, -2.7518e-01,  9.6619e+00,  7.7996e-01, -5.7445e+00,\n",
      "        -3.6233e+00, -3.8185e+00,  4.3506e+00, -1.5899e+00,  7.0205e+00,\n",
      "        -3.4252e+00, -5.8510e+00,  7.6807e-01,  6.0706e+00, -4.1745e+00,\n",
      "         4.4973e-01, -3.8074e+00,  5.2203e+00, -3.6680e+00, -9.2555e+00,\n",
      "         4.0483e-01, -9.7755e-01, -1.7897e-01, -1.9905e+00, -7.7934e+00,\n",
      "        -8.2677e-01, -8.9102e+00, -1.5592e+00, -1.0689e+01, -3.8611e+00,\n",
      "         1.1594e+01, -3.1963e+00,  2.1829e+00,  3.0591e+00,  4.3354e-01,\n",
      "         2.5297e+00,  5.7394e+00, -1.8363e+00, -6.8760e-01,  2.5932e+00,\n",
      "        -2.7962e-01, -6.4488e+00,  3.8876e+00, -1.5073e+00,  2.8422e+00,\n",
      "        -1.9206e+00, -1.0985e+00, -2.9138e+00,  3.8040e+00,  3.6504e+00,\n",
      "         5.3468e+00,  2.1958e+00,  8.0706e-01, -9.1702e-01, -7.6649e-01,\n",
      "        -2.6626e+00,  4.6867e+00, -2.7567e+00,  5.1231e+00, -4.2895e+00,\n",
      "         6.0808e+00, -7.2320e+00,  4.5281e+00,  2.3553e+00,  3.9814e+00,\n",
      "        -2.4757e-01,  1.3456e-01,  3.0894e+00,  3.3261e+00,  6.1707e+00,\n",
      "        -9.2060e-01, -1.6756e+00,  4.9550e+00,  3.9249e+00, -8.0805e-01,\n",
      "         2.1026e+00,  4.6922e-01,  6.5214e+00,  2.7796e+00,  1.1153e-02,\n",
      "         4.5814e+00, -3.2083e-01, -5.3974e+00, -2.4021e+00,  7.1075e+00,\n",
      "         2.3739e+00, -6.1923e+00, -1.1991e+00,  7.0069e+00,  2.7199e+00,\n",
      "        -8.3536e-01,  3.5538e-01,  2.4033e+00, -6.0884e-01, -1.6064e+00,\n",
      "        -6.0604e+00,  4.2687e+00,  1.6897e+00,  1.2786e+00, -2.7446e-01,\n",
      "         9.6538e-01,  2.2747e+00,  6.8645e+00, -4.1295e+00,  4.9770e+00,\n",
      "        -5.4339e+00, -1.4455e+00, -8.0863e-01, -5.7469e+00, -3.6156e+00,\n",
      "         2.7752e+00,  2.4012e+00, -6.3051e+00, -3.2512e+00,  3.7048e+00,\n",
      "        -1.2770e+01, -2.3781e+00,  4.1077e+00,  8.3986e+00, -4.2713e+00,\n",
      "         2.1306e+00, -1.5241e+00, -3.3699e+00,  3.8277e-01, -7.6846e+00,\n",
      "         2.5484e+00, -1.5534e+00,  1.5844e+00, -3.9874e+00, -6.6262e+00,\n",
      "         1.9443e+00,  2.1397e+00,  1.0379e+00, -5.9546e-01, -2.7210e+00,\n",
      "         1.1403e+01,  1.6578e+00, -6.3015e+00,  1.7579e+00,  9.1024e-01,\n",
      "        -4.2082e-02,  1.3013e+00,  1.3893e+00, -5.7330e-01,  7.0467e+00,\n",
      "        -4.3614e+00,  5.2747e+00, -5.3925e+00,  5.4410e+00,  5.0858e+00,\n",
      "        -1.9435e+00,  1.6880e+00, -4.6583e-01, -8.3689e+00,  7.7806e+00,\n",
      "         4.1115e-01,  6.5468e+00, -1.9374e+00, -6.2317e+00,  5.0596e+00,\n",
      "         7.7046e+00,  1.3867e+01,  3.3990e+00, -6.8243e+00, -1.0725e+01,\n",
      "         4.8443e+00,  1.9118e+00,  2.6142e+00, -1.0910e+01, -2.9280e+00,\n",
      "        -5.1764e+00, -6.5276e+00, -1.0840e+00, -4.8290e+00, -4.1217e+00,\n",
      "         3.9288e+00,  3.6881e+00,  3.9910e+00,  3.8343e-01, -1.4978e+00,\n",
      "        -3.1547e+00,  4.4486e+00, -3.4393e-01, -3.9515e+00,  6.6660e+00,\n",
      "         4.2730e+00,  2.8136e+00, -6.6209e+00,  8.8890e+00,  4.6384e+00,\n",
      "        -2.0799e+00,  4.0577e+00, -2.2544e+00, -1.1583e+00, -7.8418e+00,\n",
      "        -2.4349e+00,  3.6551e+00,  1.2553e+01, -1.5744e+00, -1.3202e+00,\n",
      "         4.3491e-01, -6.6286e-01,  2.7133e-01, -5.6736e+00,  4.2417e+00,\n",
      "        -4.2776e+00, -1.2709e-01,  1.2063e+00, -5.4993e+00, -4.7965e+00,\n",
      "         5.5949e+00,  9.0100e+00, -3.7840e-01,  7.3247e+00, -2.5321e+00,\n",
      "         1.7036e+00, -1.8222e+00,  6.4886e+00, -1.0731e+01,  7.4828e+00,\n",
      "        -2.5214e+00, -4.2918e-01, -7.9947e+00, -1.5935e+00,  5.0965e+00,\n",
      "        -2.1908e+00,  1.9765e+00,  5.0882e+00, -2.8450e+00, -8.3093e+00,\n",
      "         1.6115e+00,  1.4317e-01, -1.0273e+00,  1.4044e+00,  7.3945e+00,\n",
      "        -3.9151e+00,  3.5837e+00,  9.5746e+00,  4.6916e+00, -3.5766e+00,\n",
      "         4.2397e+00, -6.6243e+00,  4.4796e+00,  6.0166e+00, -3.7366e+00,\n",
      "         8.3851e+00,  2.9503e+00, -3.1615e+00,  1.9325e+00, -5.0427e+00,\n",
      "         2.7740e+00, -3.0512e-01,  5.0627e+00,  5.5331e-01,  2.4304e+00,\n",
      "         1.2823e+00, -5.5423e+00,  2.6544e+00,  1.3280e+00, -1.9807e+00,\n",
      "        -1.9999e-01,  6.4924e+00,  2.7474e+00,  3.0397e+00,  8.3817e-01,\n",
      "         1.2409e+00, -5.1070e+00,  3.6058e+00,  3.3052e+00, -5.9145e+00,\n",
      "        -6.4118e+00,  4.0364e+00,  1.4887e+00,  1.0614e+01, -7.2195e+00,\n",
      "         9.6257e-01, -1.4639e+00,  3.5712e+00,  8.8447e+00, -2.3957e+00,\n",
      "         7.3026e+00,  3.5543e+00, -5.9194e+00,  2.8582e+00, -4.0391e+00,\n",
      "        -2.4769e+00, -2.2458e+00,  7.5438e-01,  3.4859e+00, -4.4907e+00,\n",
      "         2.3614e+00,  2.5337e+00, -3.9899e+00,  1.1547e+01,  1.0945e+00,\n",
      "         9.2221e+00,  3.0211e+00,  4.1178e+00, -5.1979e+00, -3.6109e+00,\n",
      "        -2.9265e+00, -8.0740e+00,  6.0669e+00, -5.2615e+00, -1.3142e+00,\n",
      "        -1.6913e+00,  1.0182e+01, -4.5387e+00, -2.6199e+00, -1.1914e+00,\n",
      "        -1.6310e+00, -3.4836e+00, -3.4204e+00,  2.3740e+00, -3.3422e+00,\n",
      "         1.1245e+01,  2.0558e+00,  1.4935e-01,  3.3152e+00, -2.8464e+00,\n",
      "        -5.4142e+00, -8.6610e+00,  3.8864e+00, -2.2124e+00, -3.0208e+00,\n",
      "         1.3128e+00,  4.6146e-01, -1.3209e+00,  6.4085e+00,  3.6046e+00,\n",
      "         3.2530e+00, -8.2287e+00, -7.5628e+00,  1.2070e+00,  2.9131e+00,\n",
      "         2.7282e+00,  3.7820e-01, -4.6437e+00,  1.6885e+00, -4.5005e+00,\n",
      "        -9.5642e-01,  2.3368e+00, -3.6254e+00, -5.4336e-01, -4.5842e-01,\n",
      "         5.9258e+00, -5.9875e+00,  4.3842e+00, -7.6273e+00,  2.1271e+00,\n",
      "        -5.0902e+00,  5.8521e+00,  3.6038e+00, -7.9091e-01, -7.4048e-01,\n",
      "        -5.6484e+00, -6.0443e+00,  3.3885e+00, -8.1508e+00,  5.7909e+00,\n",
      "        -2.8848e+00,  9.5175e-01, -5.2242e-01,  2.7234e+00, -8.0945e+00,\n",
      "        -4.4856e+00, -1.5924e+00, -2.4281e+00,  1.1244e+01,  5.6689e-01,\n",
      "        -2.7598e+00, -1.2509e+00, -9.9751e-01, -2.3154e-01,  6.4807e+00,\n",
      "         1.0363e+01,  5.6466e+00, -2.0261e+00, -4.3923e+00,  9.4839e+00,\n",
      "        -3.8021e+00,  1.2987e+00, -1.1115e+00, -2.4903e-01, -6.5760e+00,\n",
      "        -5.2979e+00, -3.9959e+00, -1.8522e+00, -5.4322e+00, -8.6189e+00,\n",
      "         8.9756e+00,  3.2931e+00, -6.4819e+00, -8.5078e+00,  4.8990e+00,\n",
      "        -5.0139e+00, -2.2624e+00, -1.8012e-01,  2.9558e+00,  3.0410e-01,\n",
      "         1.1983e-01,  7.0533e+00,  1.1664e+00])}, 'audio': {'emotion_logits': tensor([ 1.2522, -0.9251, -1.6752,  1.2129,  0.9181,  1.9537, -2.1170]), 'personality_scores': tensor([0.3824, 0.5868, 0.2976, 0.4869, 0.4345]), 'last_emo_encoder_features': tensor([-1.1459e+00,  3.4363e-01, -2.7228e-01,  4.5581e-01,  3.2446e-01,\n",
      "         1.7883e-01, -3.4156e-01, -8.5532e-01, -7.1697e-01,  3.4404e-01,\n",
      "         2.7300e-01,  6.9761e-01, -6.2860e-01, -1.0057e+00,  5.9837e-01,\n",
      "         3.1771e-02,  6.3699e-02, -4.9480e-01, -4.9797e-01, -7.4518e-01,\n",
      "         2.7359e-01,  4.8410e-01,  6.7021e-01,  2.4414e-01,  1.2029e+00,\n",
      "         1.4264e-03,  5.4377e-01, -1.9698e-01, -2.3366e-01, -1.8078e-01,\n",
      "         3.4672e-01, -3.6639e-01,  1.0629e+00, -1.8531e-01,  4.0008e-01,\n",
      "         1.8928e-02, -2.7896e-03, -5.5281e-05, -3.3159e-01, -6.8064e-01,\n",
      "        -2.8615e-02,  4.7856e-03, -1.5395e-01,  7.7461e-02,  4.0387e-01,\n",
      "        -3.2867e-01,  1.7661e-01,  6.2669e-02,  1.1362e-01, -1.1992e-01,\n",
      "         1.7907e-01,  3.6917e-01, -4.8777e-01,  3.1120e-01, -1.4931e-01,\n",
      "        -6.2994e-01,  3.2467e-01, -3.2077e-02,  2.3959e-01,  4.6667e-01,\n",
      "         4.4556e-01, -3.4032e-02, -6.4684e-01, -3.0087e-01, -1.1330e-01,\n",
      "         6.0749e-01,  4.1461e-01,  3.2515e-02,  5.2565e-01, -1.9399e-01,\n",
      "         8.2437e-01,  3.1426e-01,  2.5181e-01, -4.9917e-01, -1.3820e+00,\n",
      "        -7.8447e-01,  1.3226e-01, -6.8764e-01, -2.0756e-01,  1.2060e-02,\n",
      "        -1.8413e-01,  7.1184e-01, -5.9132e-03,  1.6167e-01, -7.9189e-01,\n",
      "         1.3240e-01,  7.4786e-01, -4.9731e-03,  2.1953e-01, -3.8488e-01,\n",
      "        -5.3016e-01, -2.5486e-01, -3.1207e-01,  3.2090e-01,  4.1965e-01,\n",
      "        -4.6514e-02, -1.0724e-01, -4.7942e-01, -1.4028e+00, -2.9167e-01,\n",
      "        -5.8794e-02,  5.4748e-01,  2.6169e-01, -4.9637e-01,  3.3700e-01,\n",
      "         1.1443e-01, -1.8780e-01,  2.0946e-01, -4.4084e-01, -7.1341e-01,\n",
      "         4.0987e-01, -2.0472e-01, -7.5692e-01, -2.2181e-01, -6.5261e-01,\n",
      "        -4.5150e-01,  4.1663e-01,  4.6721e-01,  4.9534e-01,  5.3008e-01,\n",
      "         1.1200e+00, -3.0185e-01, -7.6319e-01,  1.7444e-01, -3.5818e-01,\n",
      "        -1.4607e-02,  1.7513e-01, -3.9243e-01,  1.0763e-01, -3.6562e-01,\n",
      "         2.0571e-01,  1.5236e-01,  4.7361e-01,  4.2905e-01,  1.1097e-01,\n",
      "         3.0949e-01, -4.4075e-01, -4.4815e-01,  2.0614e-01,  5.7864e-01,\n",
      "         2.1525e-01,  6.9125e-01, -4.1427e-02,  7.7593e-02,  3.9551e-02,\n",
      "        -7.0129e-01,  5.0375e-01,  8.9657e-02, -8.1872e-01, -2.3437e-01,\n",
      "         3.6144e-01,  2.2433e-01, -8.2914e-02,  5.5653e-01,  1.1518e-01,\n",
      "        -6.3257e-01, -5.8566e-01, -5.5548e-01, -5.4307e-02,  1.2404e+00,\n",
      "         8.9894e-01,  8.9713e-01, -3.1588e-01, -6.7805e-01,  6.4938e-01,\n",
      "        -1.1798e-01,  7.3371e-01, -3.7331e-01,  6.8778e-02,  3.9513e-01,\n",
      "         2.3621e-01, -5.3472e-01,  1.0624e+00,  6.2746e-01,  1.4450e-01,\n",
      "        -5.6089e-01,  1.0251e-01, -4.3961e-01,  4.6816e-01,  5.2880e-01,\n",
      "         6.2057e-01, -2.3974e-02, -1.6427e-01,  5.6944e-01, -1.0187e-01,\n",
      "        -1.4595e-01,  3.2818e-01,  4.9442e-01,  6.8914e-01, -1.1065e+00,\n",
      "        -3.0655e-01, -2.4689e-01,  2.3536e-01, -5.8100e-02, -9.5478e-02,\n",
      "         2.5082e-01,  2.4861e-01,  9.4055e-01,  2.2479e-01,  1.3839e+00,\n",
      "         5.6216e-01, -2.7781e-02,  6.6554e-01, -7.3303e-01, -7.5980e-02,\n",
      "        -1.4066e-01,  1.8955e-01, -1.8036e-01,  5.0391e-01, -9.1001e-01,\n",
      "         2.2362e-01, -5.9209e-01,  3.3144e-01,  5.9314e-01, -1.1557e+00,\n",
      "        -8.8634e-01,  6.9497e-01,  7.7547e-01,  5.1209e-01,  2.3463e-01,\n",
      "        -2.1282e-01, -5.3779e-01, -2.0461e-01, -6.9068e-01, -1.3202e+00,\n",
      "         2.8074e-02,  3.7384e-01, -4.3972e-01,  8.1870e-01,  3.4269e-01,\n",
      "        -2.7619e-01, -6.9583e-01,  3.8031e-01, -6.9739e-02,  2.9916e-01,\n",
      "        -1.6847e-02,  7.1552e-02, -1.2524e-02,  5.5468e-01,  4.9632e-01,\n",
      "        -1.0352e-01,  3.8819e-01,  2.6041e-01,  3.9754e-01, -8.5152e-01,\n",
      "        -6.6239e-01, -3.6751e-01,  8.4718e-02, -5.4614e-01,  2.3846e-01,\n",
      "        -3.7195e-01, -1.4872e-01, -9.4981e-02, -2.2678e-01,  5.2938e-01,\n",
      "        -1.3690e-01]), 'last_per_encoder_features': tensor([-8.2018e-02,  6.0864e-03, -3.3018e-01,  1.4379e-01,  7.1081e-01,\n",
      "         2.3200e-01, -9.0385e-01,  8.2969e-01, -2.4979e-01, -3.6336e-01,\n",
      "        -1.6210e+00, -9.4982e-01, -2.9554e-01,  1.7937e+00, -1.6106e+00,\n",
      "         4.4128e-01,  3.6703e-01,  1.1394e-01,  7.8856e-01,  1.0690e+00,\n",
      "        -6.0316e-01,  4.5971e-01, -1.9958e-01, -8.4004e-01,  7.3590e-01,\n",
      "         2.7164e-01, -1.7686e+00, -4.1535e-01,  1.4585e-01, -1.9355e-01,\n",
      "         1.5619e-01,  4.6923e-01,  5.1726e-01,  2.2411e-01,  7.0359e-02,\n",
      "        -5.7569e-01,  3.4239e-01,  1.4611e+00,  1.5824e+00, -2.1914e-01,\n",
      "         1.7410e-01,  4.9457e-01,  8.2393e-01,  1.7217e-01, -2.4642e-01,\n",
      "         5.2939e-01, -3.3539e-01, -3.3356e-01, -1.8194e-01,  4.5160e-01,\n",
      "        -6.3912e-01, -9.8358e-01,  2.6914e-01, -8.7890e-03, -6.5239e-01,\n",
      "        -1.4058e-01,  4.6011e-01,  3.1600e-01,  1.0420e+00,  4.1395e-01,\n",
      "         5.7192e-01, -5.6138e-01,  1.3224e+00,  1.1357e-01,  4.5948e-01,\n",
      "        -1.0109e+00,  4.1002e-01, -1.8881e+00, -8.7086e-01, -4.8240e-01,\n",
      "        -6.5730e-01, -1.0201e-01,  4.8395e-01,  2.5050e-01, -8.7604e-01,\n",
      "         4.7585e-01,  7.3997e-01,  5.1677e-03,  6.4665e-01, -1.8910e+00,\n",
      "         1.1004e+00,  1.2555e+00,  1.5384e+00, -6.6462e-01,  6.5387e-02,\n",
      "        -8.2393e-01,  1.5020e-01, -2.3800e-01,  5.4048e-01, -5.3662e-01,\n",
      "         2.8020e-01,  4.4546e-01,  4.1529e-01,  5.5195e-01, -2.9595e-01,\n",
      "        -6.3132e-02, -9.8433e-01,  5.0995e-01, -1.4345e+00,  5.4130e-01,\n",
      "         1.5711e-01,  7.6619e-01, -2.9473e-01,  4.4122e-01, -5.2884e-01,\n",
      "        -2.9528e-01, -7.8789e-02,  4.5728e-01,  5.5157e-01,  1.5448e+00,\n",
      "         1.4800e+00, -8.1552e-01, -3.3570e-01,  2.5655e-01,  9.5877e-01,\n",
      "        -8.6151e-01,  5.1902e-01, -2.9346e-01, -3.7297e-01, -2.8025e-01,\n",
      "        -4.1102e-01,  7.0596e-01,  7.3030e-01, -1.4640e+00, -1.7231e-01,\n",
      "         1.0954e+00,  1.2387e+00, -1.2414e-01,  1.2119e+00, -6.3808e-01,\n",
      "         1.6535e+00,  7.9420e-01,  2.8802e-01,  5.7710e-01,  5.5147e-01,\n",
      "        -1.3570e-01,  5.1547e-01, -1.0723e+00, -1.3723e+00,  1.8144e-01,\n",
      "        -5.1702e-01, -7.5431e-01,  4.1834e-01, -1.3239e+00, -4.8288e-01,\n",
      "        -1.6535e-01,  1.2852e+00, -3.1215e-01,  3.3830e-01,  8.3369e-01,\n",
      "         8.7595e-01,  1.2064e+00,  6.5493e-01, -2.3779e-01,  2.7715e-01,\n",
      "         1.0856e+00,  8.0893e-01, -1.1745e-01, -3.3627e-01,  9.4683e-01,\n",
      "         1.2105e+00,  6.3412e-02,  5.3545e-01, -5.6813e-02, -5.3816e-01,\n",
      "         1.4897e+00, -1.2738e-01, -6.8748e-02, -2.9898e-01, -5.1954e-01,\n",
      "        -4.9504e-01,  9.5096e-02,  1.7668e-02, -8.3154e-01, -5.5405e-01,\n",
      "         1.0553e+00,  3.2725e-01, -1.3749e+00,  5.2283e-01,  1.2577e+00,\n",
      "         3.1933e-02, -9.3541e-01,  5.6254e-01, -2.2808e-01, -3.9635e-01,\n",
      "        -1.8639e-01, -6.8930e-01, -3.1386e-01,  7.9359e-01,  4.2217e-01,\n",
      "        -7.3590e-01,  2.3915e-01, -8.8523e-02, -8.4026e-01, -1.9652e-03,\n",
      "         7.9048e-01, -4.2702e-01,  2.8162e-01, -1.1776e+00,  1.3174e+00,\n",
      "         1.0787e+00, -1.1391e-01, -1.0467e-01, -2.3466e-01, -1.5083e+00,\n",
      "         9.8502e-01,  2.0987e-01,  2.1323e-01,  4.8594e-01, -5.3487e-01,\n",
      "        -1.5963e+00, -3.8359e-01,  3.8008e-01, -6.3301e-01, -1.3258e+00,\n",
      "         1.8443e-01, -2.2040e-01, -2.2649e-02, -5.9202e-01,  4.7324e-01,\n",
      "        -4.7545e-01, -1.8483e-01, -8.3967e-01, -1.1601e+00,  2.1467e-02,\n",
      "        -5.3281e-01,  1.4140e-01,  9.1906e-01,  7.5175e-01,  1.5971e-01,\n",
      "        -1.6699e-01,  2.4664e-01, -2.3653e-01,  4.6706e-01, -5.0924e-01,\n",
      "         2.8798e+00,  5.2385e-01, -2.0921e+00,  2.0259e-01,  4.8159e-01,\n",
      "        -3.6804e-01,  6.2902e-01,  4.3946e-01, -1.6756e+00,  2.5742e-01,\n",
      "        -1.8339e-01, -1.3727e+00,  2.1349e-01,  4.7086e-01, -5.6110e-02,\n",
      "        -3.6365e-01,  2.1786e-01,  1.4871e-01, -2.0622e-02,  2.8997e-02,\n",
      "        -1.4948e-01])}, 'text': {'emotion_logits': tensor([ 0.1648, -0.1253, -2.0227,  0.9871,  0.1848,  0.5950, -0.6119]), 'personality_scores': tensor([0.5357, 0.6516, 0.4252, 0.5848, 0.5745]), 'last_emo_encoder_features': tensor([-1.3358e+00, -8.4758e-01, -2.3044e-01, -4.7164e-01, -4.7235e-01,\n",
      "         3.6936e-01,  6.6671e-01, -2.3499e+00, -2.4166e-02,  8.8484e-01,\n",
      "        -9.6384e-01,  1.2834e-01,  1.2693e+00, -3.2887e-01, -2.5729e-01,\n",
      "         8.8311e-01,  2.3343e+00,  1.2263e+00,  8.1256e-02,  5.0738e-01,\n",
      "        -1.4421e+00,  1.1396e+00,  8.8917e-01, -7.1836e-01,  3.4119e-01,\n",
      "        -9.0240e-01,  2.6701e-01, -8.2720e-01,  4.3867e-01, -4.5529e+00,\n",
      "         7.8713e-02, -2.2155e-01, -6.6450e-01,  1.0895e+00,  8.2795e-01,\n",
      "        -9.7151e-01, -8.5668e-01, -1.5223e-01,  4.0128e-01,  3.9293e-03,\n",
      "        -7.4627e-01,  2.2587e-01,  1.4062e-01,  2.5501e-01,  8.7616e-01,\n",
      "         4.2099e-01,  1.4730e+00, -9.7164e-01,  1.7201e+00, -8.6028e-01,\n",
      "        -5.8336e-01,  6.2175e-01,  5.4672e-04,  4.1562e-01,  1.0455e+00,\n",
      "         1.1301e+00,  8.5382e-01,  1.2429e+00, -5.8854e-01,  2.5407e-01,\n",
      "        -2.6758e-01, -1.9859e+00,  1.5960e-01, -1.6895e-03,  3.2072e-01,\n",
      "         1.5387e+00, -5.3779e-01,  2.2951e-01, -6.3712e-02,  5.5153e-01,\n",
      "        -2.1833e-01, -1.1838e+00, -1.1049e+00, -1.3221e+00,  4.4275e-01,\n",
      "        -4.2986e-01, -8.3918e-02,  1.4309e+00, -1.1395e+00,  5.3268e-01,\n",
      "        -2.7168e-01,  2.5212e-01,  1.1273e+00,  3.1878e-01, -1.6921e+00,\n",
      "        -6.8164e-01, -1.5162e-01, -1.9468e+00,  3.3035e-01,  9.6203e-01,\n",
      "        -3.5072e-01, -5.9996e-01, -4.9253e-01, -2.7389e+00,  5.4882e-01,\n",
      "        -9.0683e-01,  3.8994e-01, -8.0958e-01, -6.4628e-01, -7.9893e-01,\n",
      "         1.6379e+00, -7.7442e-01, -3.0949e-01, -1.4134e+00,  7.8849e-01,\n",
      "         2.3537e-01,  1.7712e-01, -1.6739e-01, -1.9857e-01, -5.3808e-02,\n",
      "        -9.8509e-02, -1.9747e+00,  7.7699e-01,  4.3137e-01, -4.7894e-01,\n",
      "        -5.2931e-01, -6.3877e-01, -4.6840e-01, -8.2920e-01, -3.9678e-01,\n",
      "         3.6880e-01, -1.4190e+00,  2.7874e-01, -1.3437e+00,  1.1089e-01,\n",
      "         5.1367e-01,  5.0949e-01, -4.4769e-01,  1.3273e-01, -2.6371e+00,\n",
      "        -4.5426e-02, -8.1573e-01,  9.5208e-02, -4.6106e-01,  4.0965e-01,\n",
      "         9.0766e-02,  7.4265e-01, -4.3592e-01, -3.8997e-01, -4.2355e-02,\n",
      "         3.9041e-01, -5.0474e-01,  8.1442e-01,  1.0279e+00,  3.7824e-01,\n",
      "         1.0921e+00, -3.7227e-01, -1.1158e+00, -8.7036e-01, -1.8758e+00,\n",
      "         4.8802e-01, -5.9608e-01, -7.4713e-01,  3.4617e-01, -6.5450e-01,\n",
      "        -1.0589e+00, -4.4185e-01,  6.1498e-01,  4.1213e-01, -7.8912e-01,\n",
      "         8.9569e-01, -2.3864e+00,  1.2559e-01,  3.2472e-01,  1.0245e+00,\n",
      "         3.2428e-01, -1.8818e+00,  3.6022e-02,  4.9296e-01, -1.0467e+00,\n",
      "         7.6520e-01, -2.2724e+00, -9.9586e-01, -1.4003e+00,  9.0628e-01,\n",
      "        -1.0744e+00,  9.7487e-01, -2.5344e-01, -4.9345e-01, -1.3290e+00,\n",
      "        -5.6224e-01,  1.2525e+00, -8.5775e-01, -1.1377e-01,  1.6923e-01,\n",
      "         6.8671e-01,  8.3530e-01, -1.4700e+00, -1.6231e-02,  7.9436e-02,\n",
      "         1.3398e-01,  3.2815e-01,  2.1025e-01, -3.7068e-01, -4.2592e-01,\n",
      "        -7.7271e-01,  5.0315e-01, -7.0661e-01, -1.5529e+00, -1.0931e+00,\n",
      "        -5.0692e-01, -1.1385e+00, -3.4177e-01,  7.6960e-02, -9.2017e-01,\n",
      "        -1.8970e+00,  8.6781e-01, -2.5839e-01, -6.6447e-01, -7.2077e-01,\n",
      "         1.2968e+00,  2.0852e+00, -1.4753e-01, -7.8208e-01,  8.8558e-02,\n",
      "        -4.6750e-01,  4.5017e-01, -3.9350e-01,  1.0663e+00, -8.7411e-01,\n",
      "        -2.2423e-01, -1.5293e+00, -5.0875e-01, -2.1820e+00,  2.8155e-01,\n",
      "        -3.5808e-01, -6.5394e-01, -7.3360e-01,  4.0936e-01,  2.4147e+00,\n",
      "         9.4128e-01,  2.3534e-02,  3.8097e-01,  2.2371e-01,  8.3138e-01,\n",
      "        -6.5672e-01,  2.6908e+00, -1.3753e+00,  1.6127e+00,  3.7658e-01,\n",
      "         1.1774e+00, -1.5133e+00,  7.2319e-01, -2.2244e+00,  2.6715e-01,\n",
      "         2.5115e-01,  1.7715e+00, -8.7660e-01,  1.5534e+00,  1.2835e+00,\n",
      "         1.9960e+00,  2.8364e+00, -5.1621e-01, -9.0650e-01, -1.8167e+00,\n",
      "        -2.4892e-01]), 'last_per_encoder_features': tensor([-0.5617, -0.4754, -0.5106, -1.9139, -2.2368, -1.5442,  0.3889,  1.3059,\n",
      "        -2.5080, -2.7024,  1.6704, -3.3659, -2.1284,  0.4708, -2.3888, -1.8362,\n",
      "        -1.7174, -3.1922, -1.6847, -4.8887,  1.5824,  1.1730, -1.1249, -1.2272,\n",
      "        -0.6495,  2.5964, -0.3116,  2.4261,  0.5855,  3.2978,  1.2819, -3.1739,\n",
      "         1.2655,  0.3990,  1.7054, -5.4910, -1.1800,  4.1596, -0.3188,  0.5426,\n",
      "         0.3531,  2.3963,  1.6766,  4.4662, -2.4233, -0.4173,  2.6558, -1.4216,\n",
      "        -1.9991, -1.4899, -1.8076,  0.8971, -0.9136,  3.9833, -0.8959, -5.3687,\n",
      "        -1.0280,  0.6228, -0.3523, -2.1558,  0.4374, -1.7920, -2.2489,  1.7766,\n",
      "         3.6240, -3.6330, -0.8952, -0.5685,  2.9917, -1.7360, -1.2813,  2.9777,\n",
      "        -1.1733,  0.2116,  0.3904,  0.3609, -1.8032,  0.7615, -3.0894, -4.0969,\n",
      "         1.0973,  0.9327,  1.3468, -2.7670,  0.8470, -2.5896, -0.4575, -2.4000,\n",
      "        -1.3299, -3.0254,  3.8014, -3.5726,  1.8988, -0.2193, -1.6550,  0.8846,\n",
      "        -0.0711, -0.9288, -0.7491,  4.5822, -1.1895, -0.2547, -1.0454, -0.8254,\n",
      "         2.0710, -3.0766,  2.6767,  1.4832, -4.5981, -0.7972, -0.4137,  1.9193,\n",
      "        -0.4573, -0.1614, -0.6088, -1.7981,  1.7275,  4.9123, -4.2653,  1.2501,\n",
      "        -1.2568, -1.6046, -0.8565,  1.9740, -1.2555,  0.6438, -0.9028,  0.3833,\n",
      "         2.4911, -0.3401, -0.4489,  1.7245,  2.7385,  3.7248, -1.5682,  0.6908,\n",
      "        -0.9083,  2.3505,  0.7613,  1.0179,  0.6986,  1.7139,  4.0763,  2.6789,\n",
      "         1.5764,  0.8972, -1.8942, -0.5954, -0.6580,  1.9810,  2.4286,  2.0008,\n",
      "         1.4488,  3.2282, -3.5589,  0.1318, -1.1317,  0.9613, -0.6777, -0.4712,\n",
      "         2.0476,  1.6964, -2.6010,  1.9819,  2.7621,  2.5636,  2.2354, -1.6254,\n",
      "         3.8726,  2.0713, -2.0942, -2.3288, -1.0607, -0.3918, -1.6984,  1.1620,\n",
      "         1.4016,  2.7543,  0.7612, -0.0228, -0.8079,  0.3211,  1.1013,  0.2292,\n",
      "        -1.6727,  1.7841,  0.7838,  0.1734, -0.9457, -0.3311,  2.0503, -2.3349,\n",
      "        -1.4906, -1.8379,  1.1762, -2.9025,  1.8075,  1.4080, -3.1575,  1.8343,\n",
      "        -2.6904, -1.5437,  0.5474, -0.8632,  2.9140, -0.0882,  0.2358, -0.6875,\n",
      "        -0.8142,  1.3002, -0.2281,  2.4046, -0.6001, -0.5803, -0.1952,  0.3090,\n",
      "        -4.0194,  1.9237,  1.9944, -1.9277,  2.9720, -0.3715,  0.0689,  2.8654,\n",
      "        -1.3955,  5.6283, -1.0063,  0.3758, -0.6852, -3.7382, -1.6456, -0.3087,\n",
      "        -0.9658,  1.3646,  0.6237, -0.2511,  0.5698,  2.2141,  2.6259,  1.7165,\n",
      "        -3.0457,  0.9200, -1.4867,  0.6875, -0.0336, -0.1556, -4.6411, -0.2918,\n",
      "        -2.3902,  2.4793,  2.3391,  1.8227, -0.1899, -2.1618,  3.1631,  2.0859])}}, 'labels': {'emotion': tensor([0.0000, 0.5000, 0.0000, 0.0000, 0.0000, 0.5000, 0.0000])}}\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# –ø—É—Ç—å –∫ —Ç–≤–æ–µ–º—É .pickle —Ñ–∞–π–ª—É\n",
    "pickle_path = \"../features/cmu_mosei_test_seed_42_subset_size_2_average_features_True_feature_norm_False.pickle\"\n",
    "\n",
    "with open(pickle_path, \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# –ü–æ—Å–º–æ—Ç—Ä–µ—Ç—å –ø–µ—Ä–≤—ã–π —ç–ª–µ–º–µ–Ω—Ç\n",
    "print(\"üîç –ü–µ—Ä–≤—ã–π —ç–ª–µ–º–µ–Ω—Ç:\")\n",
    "item = data[0]\n",
    "print(item)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdb5996-61e3-471b-b5e5-b7a21b8adad6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8d4d3641-6bec-401f-9595-fd3ea934b237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîé –®–µ–π–ø—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:\n",
      "\n",
      "[BODY]\n",
      "  emotion_logits: torch.Size([7])\n",
      "  personality_scores: torch.Size([5])\n",
      "  last_emo_encoder_features: torch.Size([1024])\n",
      "  last_per_encoder_features: torch.Size([1024])\n",
      "\n",
      "[FACE]\n",
      "  emotion_logits: torch.Size([7])\n",
      "  personality_scores: torch.Size([5])\n",
      "  last_emo_encoder_features: torch.Size([512])\n",
      "  last_per_encoder_features: torch.Size([512])\n",
      "\n",
      "[SCENE]\n",
      "  emotion_logits: torch.Size([7])\n",
      "  personality_scores: torch.Size([5])\n",
      "  last_emo_encoder_features: torch.Size([768])\n",
      "  last_per_encoder_features: torch.Size([768])\n",
      "\n",
      "[AUDIO]\n",
      "  emotion_logits: torch.Size([7])\n",
      "  personality_scores: torch.Size([5])\n",
      "  last_emo_encoder_features: torch.Size([256])\n",
      "  last_per_encoder_features: torch.Size([256])\n",
      "\n",
      "[TEXT]\n",
      "  emotion_logits: torch.Size([7])\n",
      "  personality_scores: torch.Size([5])\n",
      "  last_emo_encoder_features: torch.Size([256])\n",
      "  last_per_encoder_features: torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ: —Ä–∞—Å–ø–µ—á–∞—Ç–∞—Ç—å —à–µ–π–ø—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—è–º\n",
    "print(\"\\nüîé –®–µ–π–ø—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:\")\n",
    "modalities = item.get(\"features\", {})\n",
    "for mod_name, features in modalities.items():\n",
    "    print(f\"\\n[{mod_name.upper()}]\")\n",
    "    for feat_name, feat_val in features.items():\n",
    "        if isinstance(feat_val, torch.Tensor):\n",
    "            print(f\"  {feat_name}: {feat_val.shape}\")\n",
    "        elif isinstance(feat_val, np.ndarray):\n",
    "            print(f\"  {feat_name}: {feat_val.shape}\")\n",
    "        else:\n",
    "            print(f\"  {feat_name}: not a tensor ({type(feat_val)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b3717095-7b61-4f31-bd8a-50d091ef1f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç –ü–µ—Ä–≤—ã–π —ç–ª–µ–º–µ–Ω—Ç:\n",
      "{'sample_name': '-6rXp3zJ3kc_14.4680_22.8820', 'video_path': 'E:/CMU-MOSEI//video/test/-6rXp3zJ3kc_14.4680_22.8820.mp4', 'audio_path': 'E:/CMU-MOSEI//audio/test/-6rXp3zJ3kc_14.4680_22.8820.wav', 'features': {'body': {'emotion_logits': tensor([ 0.4595,  1.5333, -0.9546,  0.3222, -0.7833,  1.2438, -0.9155]), 'personality_scores': tensor([0.3668, 0.6629, 0.3182, 0.5780, 0.5149]), 'last_emo_encoder_features': tensor([[ 0.8910,  1.9902,  0.0095,  ...,  1.1676,  5.9335,  0.1280],\n",
      "        [ 2.5225,  0.8325,  0.3014,  ...,  1.2802,  5.2854, -0.2099],\n",
      "        [ 1.9918,  1.4558,  0.7890,  ...,  1.4387,  4.8233,  0.6186],\n",
      "        ...,\n",
      "        [ 2.1812,  1.0188,  0.6352,  ...,  1.4023,  5.2032,  0.4299],\n",
      "        [ 1.3845,  1.0713,  0.4135,  ...,  2.0127,  5.1946,  0.1978],\n",
      "        [ 1.1576,  0.4487, -0.5617,  ...,  1.3371,  5.4190, -0.4237]]), 'last_per_encoder_features': tensor([[ 0.1147,  3.0628, -1.2588,  ..., -0.4051, -0.6193,  0.7605],\n",
      "        [ 0.0674,  3.0821, -1.4463,  ..., -0.3831, -0.5386,  0.8875],\n",
      "        [-0.1577,  3.1393, -1.3236,  ..., -0.2405, -0.6521,  0.5088],\n",
      "        ...,\n",
      "        [ 0.2161,  3.1424, -1.0550,  ..., -0.1422, -0.4336,  0.7335],\n",
      "        [-0.0495,  2.9963, -1.1185,  ..., -0.1556, -0.4951,  0.5982],\n",
      "        [-0.2266,  2.8633, -1.8597,  ..., -0.2111, -0.7979,  0.7207]])}, 'face': {'emotion_logits': tensor([ 0.6361,  1.7097, -0.3265, -0.9575, -1.2868,  0.9896, -0.9342]), 'personality_scores': tensor([0.4642, 0.6192, 0.3096, 0.5205, 0.4690]), 'last_emo_encoder_features': tensor([[-9.9860e+00, -5.5704e+00, -6.9748e+00,  ..., -2.9028e+00,\n",
      "         -2.1139e+00,  1.2775e+00],\n",
      "        [-6.9461e+00, -5.6657e+00, -6.9557e+00,  ..., -8.7009e-01,\n",
      "          5.3950e-02,  1.7703e-01],\n",
      "        [-5.9452e+00, -7.4198e+00, -4.9803e+00,  ..., -8.5484e-03,\n",
      "          7.9632e-01, -2.4705e-02],\n",
      "        ...,\n",
      "        [-6.1289e+00, -8.3457e+00, -5.0904e+00,  ..., -8.5610e-02,\n",
      "          4.8874e-01,  6.4534e-02],\n",
      "        [-8.1219e+00, -1.0622e+01, -3.3079e+00,  ..., -7.9865e-01,\n",
      "         -1.0351e+00,  7.0248e-01],\n",
      "        [-1.1086e+01, -8.6675e+00, -4.0842e+00,  ..., -2.6896e+00,\n",
      "          9.9180e-01,  1.6466e+00]]), 'last_per_encoder_features': tensor([[-1.0854,  0.8254, -1.4879,  ..., -5.0949,  2.4173,  1.8911],\n",
      "        [ 0.0330,  0.4961, -0.3963,  ..., -5.4614,  1.9742,  1.7687],\n",
      "        [-1.0600, -0.2235, -1.3814,  ..., -5.1717,  3.1158,  1.3058],\n",
      "        ...,\n",
      "        [ 0.0084, -0.0367, -0.4505,  ..., -5.0352,  2.4372,  1.9426],\n",
      "        [-0.6924, -0.8137, -0.2567,  ..., -5.1866,  2.7663,  2.0023],\n",
      "        [-1.5158, -0.5393, -1.5786,  ..., -4.8632,  2.6353,  1.7605]])}, 'scene': {'emotion_logits': tensor([ 0.4340,  0.8846, -1.4251,  0.3228, -0.6891,  1.5473, -1.7124]), 'personality_scores': tensor([0.3037, 0.6032, 0.2477, 0.4853, 0.3627]), 'last_emo_encoder_features': tensor([[  0.0979,  -5.2525,  -3.1047,  ...,  -7.2780,   2.9799,  -5.3333],\n",
      "        [  2.7721,  -7.9352,   1.1998,  ...,  -8.8459,   3.2454,  -5.2233],\n",
      "        [  3.6671, -12.0169,   1.5976,  ...,  -9.2026,   3.3516,  -5.6934],\n",
      "        ...,\n",
      "        [  6.5030, -10.6219,   2.1366,  ...,  -8.8559,   4.3416,  -4.9611],\n",
      "        [  3.1656, -12.4014,   1.6377,  ...,  -8.8757,   5.3078,  -4.2006],\n",
      "        [ -1.5611, -10.4939,  -2.8233,  ...,  -7.5795,   4.8727,  -4.1236]]), 'last_per_encoder_features': tensor([[ 4.3583,  6.5275, -4.8523,  ...,  0.0173,  6.7446,  1.2936],\n",
      "        [ 5.9961,  5.8024, -3.4643,  ..., -0.1448,  6.7305,  1.0612],\n",
      "        [ 5.8416,  3.9707, -3.1716,  ..., -0.1311,  6.8122,  1.0319],\n",
      "        ...,\n",
      "        [ 6.0105,  4.8758, -2.8414,  ...,  0.3611,  7.0566,  1.0048],\n",
      "        [ 4.6360,  3.7961, -2.7986,  ...,  0.4646,  7.1301,  1.0763],\n",
      "        [ 3.3510,  4.0755, -4.6343,  ...,  0.5065,  6.8570,  1.0980]])}, 'audio': {'emotion_logits': tensor([ 1.2522, -0.9251, -1.6752,  1.2129,  0.9181,  1.9537, -2.1170]), 'personality_scores': tensor([0.3824, 0.5868, 0.2976, 0.4869, 0.4345]), 'last_emo_encoder_features': tensor([[-1.2242,  0.7371,  0.2738,  ..., -0.3894,  0.0867,  1.0350],\n",
      "        [-1.8368,  0.4067, -0.4202,  ..., -0.1683,  1.5026, -0.3206],\n",
      "        [-1.9104, -0.6285, -0.2820,  ...,  0.0754,  1.4302, -0.9230],\n",
      "        ...,\n",
      "        [-0.5416, -0.6038, -0.9203,  ...,  1.0997,  1.2080, -0.8801],\n",
      "        [-0.7529, -0.5074, -0.0176,  ...,  0.9060,  0.7809, -0.9069],\n",
      "        [-1.2200,  1.2929, -0.6882,  ...,  0.7242,  0.7511, -1.2545]]), 'last_per_encoder_features': tensor([[-0.9537, -0.1484, -0.0814,  ..., -1.0280,  0.6873,  1.1913],\n",
      "        [-0.2112, -0.4629,  0.2371,  ..., -2.4776,  0.5322,  0.9463],\n",
      "        [ 0.0145, -0.7231,  0.2895,  ..., -2.2374, -0.2334,  0.4262],\n",
      "        ...,\n",
      "        [-0.6510, -0.6846, -1.0284,  ..., -1.4400, -0.8334,  1.0978],\n",
      "        [-0.5005, -0.8906, -0.9889,  ..., -1.2227, -1.6883,  0.9453],\n",
      "        [-0.1298, -0.9050, -0.5891,  ..., -0.5147, -2.3600,  0.8644]])}, 'text': {'emotion_logits': tensor([ 0.1648, -0.1253, -2.0227,  0.9871,  0.1848,  0.5950, -0.6119]), 'personality_scores': tensor([0.5357, 0.6516, 0.4252, 0.5848, 0.5745]), 'last_emo_encoder_features': tensor([[-1.6320, -0.9790, -0.1631,  ..., -1.4697, -1.8069, -0.3427],\n",
      "        [-0.9508, -0.4465, -0.4635,  ..., -1.0224, -1.7144, -0.2783],\n",
      "        [-0.5040, -0.0673, -0.3727,  ..., -0.8523, -2.1922, -0.1043],\n",
      "        ...,\n",
      "        [-1.8813, -1.8721,  0.3241,  ..., -0.6458, -0.8459, -0.5190],\n",
      "        [-2.1665, -1.8932,  0.1265,  ..., -0.7716, -2.0768, -0.7467],\n",
      "        [-1.6294, -0.9779, -0.1678,  ..., -1.4689, -1.8066, -0.3429]]), 'last_per_encoder_features': tensor([[-0.9899, -0.3765, -1.2002,  ..., -2.4626,  2.4331,  1.6271],\n",
      "        [-1.1459, -0.4092, -0.6138,  ..., -2.6232,  3.4057,  1.2165],\n",
      "        [-1.6027, -0.3328,  0.1319,  ..., -2.1694,  3.1142,  1.7280],\n",
      "        ...,\n",
      "        [-1.0687,  0.3455, -0.3255,  ..., -1.4857,  4.1092,  2.9153],\n",
      "        [-1.1180, -0.2203, -0.7494,  ..., -2.1444,  3.7891,  1.7575],\n",
      "        [-0.9262, -0.9760, -1.3553,  ..., -2.3933,  2.1425,  1.1844]])}}, 'labels': {'emotion': tensor([0.0000, 0.5000, 0.0000, 0.0000, 0.0000, 0.5000, 0.0000])}}\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# –ø—É—Ç—å –∫ —Ç–≤–æ–µ–º—É .pickle —Ñ–∞–π–ª—É\n",
    "pickle_path = \"../features/cmu_mosei_test_seed_42_subset_size_2_average_features_False_feature_norm_False.pickle\"\n",
    "\n",
    "with open(pickle_path, \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# –ü–æ—Å–º–æ—Ç—Ä–µ—Ç—å –ø–µ—Ä–≤—ã–π —ç–ª–µ–º–µ–Ω—Ç\n",
    "print(\"üîç –ü–µ—Ä–≤—ã–π —ç–ª–µ–º–µ–Ω—Ç:\")\n",
    "item = data[0]\n",
    "print(item)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "808de167-db23-499e-acc4-2cf13f191a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç –ü–µ—Ä–≤—ã–π —ç–ª–µ–º–µ–Ω—Ç:\n",
      "{'sample_name': 'htH89DBizno.004', 'video_path': 'E:/FirstImpressionsV2//video/test/htH89DBizno.004.mp4', 'audio_path': 'E:/FirstImpressionsV2//audio/test/htH89DBizno.004.wav', 'features': {'body': {'emotion_logits': tensor([-1.3183, -0.8238, -0.6552, -0.6306,  1.2407, -0.7158,  1.3891]), 'personality_scores': tensor([0.5326, 0.5365, 0.4849, 0.6321, 0.5423]), 'last_emo_encoder_features': tensor([[-6.0984, -1.2685, -2.9987,  ..., -4.1940, -0.9488, -2.8461],\n",
      "        [-4.2881, -1.9723, -2.6279,  ..., -4.4866, -0.6987, -2.1308],\n",
      "        [-4.4905, -2.9841, -2.6212,  ..., -4.8182, -0.5226, -2.0801],\n",
      "        ...,\n",
      "        [-3.2482, -2.0572, -2.8292,  ..., -3.3157, -1.0039, -1.2865],\n",
      "        [-3.4654, -3.4890, -2.1004,  ..., -3.6636, -0.5618, -0.3304],\n",
      "        [-3.5330, -3.4800, -2.3680,  ..., -3.8267, -0.4109,  0.1079]]), 'last_per_encoder_features': tensor([[ 4.5091,  1.5342,  0.8601,  ...,  0.1812, -1.9097, -0.6636],\n",
      "        [ 4.4139,  1.5332,  0.4595,  ...,  0.0348, -2.1830, -0.8356],\n",
      "        [ 4.6776,  1.4330,  0.4705,  ...,  0.0304, -2.1044, -0.8673],\n",
      "        ...,\n",
      "        [ 4.7170,  2.0024,  1.1686,  ...,  0.4623, -1.0888, -0.5134],\n",
      "        [ 4.4056,  1.6596,  1.5654,  ...,  0.1556, -1.3079, -0.4568],\n",
      "        [ 4.3678,  1.6311,  1.2025,  ...,  0.0751, -1.6647, -0.7255]])}, 'face': {'emotion_logits': tensor([ 0.3950, -1.3291, -0.8733,  0.8395, -0.5555, -0.5297, -0.3164]), 'personality_scores': tensor([0.5448, 0.5832, 0.4877, 0.6371, 0.5482]), 'last_emo_encoder_features': tensor([[-7.1254,  6.9763,  0.0996,  ...,  0.1947, -0.2427,  0.4157],\n",
      "        [-7.3499,  5.1696, -2.0016,  ..., -3.5410, -0.6252, -1.5081],\n",
      "        [-8.0040,  2.1621, -0.5708,  ..., -1.6620, -2.3714, -2.6078],\n",
      "        ...,\n",
      "        [-5.0937, -2.1653,  3.0004,  ..., -0.9598, -2.7117, -1.7931],\n",
      "        [-5.4252, -3.9499,  1.7339,  ..., -0.6478, -2.9329, -1.7859],\n",
      "        [-7.0157, -2.8471,  0.5105,  ..., -0.4020, -2.1515, -1.1315]]), 'last_per_encoder_features': tensor([[ 2.1015e+00,  1.5308e+00,  1.1959e+00,  ...,  1.2455e+00,\n",
      "          3.2535e+00, -1.8923e+00],\n",
      "        [ 4.1566e+00,  7.3953e-01,  2.1500e+00,  ...,  1.3285e+00,\n",
      "          2.8972e+00, -5.2107e-01],\n",
      "        [ 4.5499e+00, -1.0893e-01,  2.7566e+00,  ...,  1.3754e+00,\n",
      "          2.9973e+00, -6.7792e-02],\n",
      "        ...,\n",
      "        [ 2.1316e+00,  1.2126e-01,  2.2196e+00,  ..., -2.7378e-01,\n",
      "          1.9760e+00,  2.5806e-01],\n",
      "        [ 3.7078e+00, -3.2591e-01,  2.8538e+00,  ...,  2.9921e-01,\n",
      "          2.3356e+00, -8.3598e-02],\n",
      "        [ 2.1466e+00,  4.4164e-01,  2.3121e+00,  ...,  9.4861e-01,\n",
      "          1.9651e+00,  3.4791e-03]])}, 'scene': {'emotion_logits': tensor([-1.4333,  0.7245,  0.2676, -0.8584,  0.4059, -0.1515,  1.7094]), 'personality_scores': tensor([0.6100, 0.5503, 0.4974, 0.6232, 0.5360]), 'last_emo_encoder_features': tensor([[ -1.5820,  -0.1379,  -6.1000,  ...,  -5.6277,   0.9956, -11.4797],\n",
      "        [  0.3218,  -3.2245,  -4.0771,  ...,  -4.3450,   0.1954, -13.7199],\n",
      "        [  0.8340,  -8.8290,  -1.6438,  ...,  -4.2214,   0.8009, -14.7166],\n",
      "        ...,\n",
      "        [  3.6729,  -0.7600,  -1.0852,  ...,  -4.2953,   3.1178, -15.5726],\n",
      "        [  1.7159,  -6.8385,  -5.3422,  ...,  -6.9742,   1.6608, -15.5757],\n",
      "        [ -1.9777,  -7.5987,  -9.6486,  ...,  -6.5622,   0.4008, -16.0360]]), 'last_per_encoder_features': tensor([[-10.4207,   4.4906,  -4.3028,  ...,  -5.4041,   6.2734,   3.6023],\n",
      "        [ -8.7449,   3.7026,  -2.9235,  ...,  -5.3889,   6.0306,   3.1579],\n",
      "        [ -8.7266,   1.4346,  -2.7959,  ...,  -5.3490,   6.0262,   3.1923],\n",
      "        ...,\n",
      "        [ -8.9200,   2.3360,  -2.2833,  ...,  -5.0840,   6.7676,   3.7170],\n",
      "        [-10.1209,   1.1287,  -2.3554,  ...,  -5.3809,   6.5599,   3.6489],\n",
      "        [-11.5820,   1.5412,  -4.0757,  ...,  -5.1289,   6.4812,   3.7994]])}, 'audio': {'emotion_logits': tensor([-0.1076,  0.1115,  0.2050, -0.5880,  1.1384, -0.1085,  0.2426]), 'personality_scores': tensor([0.5481, 0.4844, 0.4369, 0.5322, 0.5242]), 'last_emo_encoder_features': tensor([[-2.2607, -0.0481, -0.2945,  ...,  0.2134, -0.3237, -0.1998],\n",
      "        [-1.3714, -0.0234, -0.4167,  ..., -0.4762, -0.6489,  0.5436],\n",
      "        [-2.9946, -0.5918,  1.3662,  ..., -1.2152, -1.2985, -0.2805],\n",
      "        ...,\n",
      "        [-2.3955, -1.1217,  0.1828,  ...,  0.3913,  0.2954, -0.1354],\n",
      "        [-2.0401,  0.3487,  0.1720,  ..., -0.3803, -1.7429,  0.1738],\n",
      "        [-3.9404, -1.6141, -1.4832,  ..., -0.0927, -0.2740, -1.0235]]), 'last_per_encoder_features': tensor([[-0.6255, -0.4836, -2.0819,  ...,  0.1048, -0.0955, -1.3250],\n",
      "        [-0.6004, -0.3702, -1.2481,  ..., -1.3851, -0.9427, -0.8352],\n",
      "        [-0.3357, -1.1481, -1.1270,  ..., -1.9530, -0.7597, -0.7394],\n",
      "        ...,\n",
      "        [-0.5076, -0.4364, -2.8296,  ...,  0.7577,  0.3192, -2.0148],\n",
      "        [ 0.2480, -0.4035, -1.5805,  ...,  0.4558, -1.1683, -1.2801],\n",
      "        [-0.9342, -0.1586, -2.5298,  ...,  0.4028, -0.6795, -2.5535]])}, 'text': {'emotion_logits': tensor([-1.7925,  0.0654,  0.3438, -0.3643, -1.1636, -0.1055,  2.4729]), 'personality_scores': tensor([0.4693, 0.4101, 0.3828, 0.5089, 0.4350]), 'last_emo_encoder_features': tensor([[ 1.3661, -0.4455,  0.1856,  ...,  0.4275,  0.4248, -0.7693],\n",
      "        [ 0.7265,  0.6047, -0.2163,  ...,  1.4826,  1.8750, -0.8699],\n",
      "        [ 0.6034,  0.4946, -0.0882,  ...,  1.3350,  1.5275, -0.8105],\n",
      "        ...,\n",
      "        [ 1.1702,  0.8942,  0.2512,  ...,  1.7365,  0.9980, -0.3480],\n",
      "        [ 0.1939, -0.3550,  0.1246,  ...,  1.3208,  0.8546, -0.1398],\n",
      "        [ 1.3682, -0.4421,  0.1878,  ...,  0.4285,  0.4221, -0.7685]]), 'last_per_encoder_features': tensor([[ 0.4456,  0.7985, -2.4482,  ..., -2.0087, -3.4925,  2.4304],\n",
      "        [ 1.4161,  2.6822, -1.1361,  ..., -1.8590, -1.9409,  3.0707],\n",
      "        [ 1.4938,  2.6739, -1.1269,  ..., -1.9386, -2.3545,  3.0515],\n",
      "        ...,\n",
      "        [ 0.5378,  1.6804, -2.3527,  ..., -1.5242, -2.7210,  2.7399],\n",
      "        [ 0.5840,  1.9639, -2.1514,  ..., -1.5364, -2.6613,  2.9677],\n",
      "        [ 0.2244,  0.9481, -2.5943,  ..., -2.1074, -3.7933,  2.5620]])}}, 'label': tensor([0.8222, 0.6699, 0.4860, 0.6813, 0.6458])}\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# –ø—É—Ç—å –∫ —Ç–≤–æ–µ–º—É .pickle —Ñ–∞–π–ª—É\n",
    "pickle_path = \"../features/fiv2_test_seed_42_subset_size_2_average_features_False_feature_norm_False.pickle\"\n",
    "\n",
    "with open(pickle_path, \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# –ü–æ—Å–º–æ—Ç—Ä–µ—Ç—å –ø–µ—Ä–≤—ã–π —ç–ª–µ–º–µ–Ω—Ç\n",
    "print(\"üîç –ü–µ—Ä–≤—ã–π —ç–ª–µ–º–µ–Ω—Ç:\")\n",
    "item = data[0]\n",
    "print(item)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "94645014-a1a1-4c20-8342-d9bf1711a310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîé –®–µ–π–ø—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:\n",
      "\n",
      "[BODY]\n",
      "  emotion_logits: torch.Size([7])\n",
      "  personality_scores: torch.Size([5])\n",
      "  last_emo_encoder_features: torch.Size([30, 1024])\n",
      "  last_per_encoder_features: torch.Size([30, 1024])\n",
      "\n",
      "[FACE]\n",
      "  emotion_logits: torch.Size([7])\n",
      "  personality_scores: torch.Size([5])\n",
      "  last_emo_encoder_features: torch.Size([30, 512])\n",
      "  last_per_encoder_features: torch.Size([30, 512])\n",
      "\n",
      "[SCENE]\n",
      "  emotion_logits: torch.Size([7])\n",
      "  personality_scores: torch.Size([5])\n",
      "  last_emo_encoder_features: torch.Size([30, 768])\n",
      "  last_per_encoder_features: torch.Size([30, 768])\n",
      "\n",
      "[AUDIO]\n",
      "  emotion_logits: torch.Size([7])\n",
      "  personality_scores: torch.Size([5])\n",
      "  last_emo_encoder_features: torch.Size([421, 256])\n",
      "  last_per_encoder_features: torch.Size([421, 256])\n",
      "\n",
      "[TEXT]\n",
      "  emotion_logits: torch.Size([7])\n",
      "  personality_scores: torch.Size([5])\n",
      "  last_emo_encoder_features: torch.Size([33, 256])\n",
      "  last_per_encoder_features: torch.Size([33, 256])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ: —Ä–∞—Å–ø–µ—á–∞—Ç–∞—Ç—å —à–µ–π–ø—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—è–º\n",
    "print(\"\\nüîé –®–µ–π–ø—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:\")\n",
    "modalities = item.get(\"features\", {})\n",
    "for mod_name, features in modalities.items():\n",
    "    print(f\"\\n[{mod_name.upper()}]\")\n",
    "    for feat_name, feat_val in features.items():\n",
    "        if isinstance(feat_val, torch.Tensor):\n",
    "            print(f\"  {feat_name}: {feat_val.shape}\")\n",
    "        elif isinstance(feat_val, np.ndarray):\n",
    "            print(f\"  {feat_name}: {feat_val.shape}\")\n",
    "        else:\n",
    "            print(f\"  {feat_name}: not a tensor ({type(feat_val)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3ce5167-c20c-45ae-a2b6-83fe00944210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "–≠–ª–µ–º–µ–Ω—Ç–æ–≤ –≤ —Å–ø–∏—Å–∫–µ: 10\n"
     ]
    }
   ],
   "source": [
    "print(type(data))\n",
    "print(f\"–≠–ª–µ–º–µ–Ω—Ç–æ–≤ –≤ —Å–ø–∏—Å–∫–µ: {len(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b54d3cb-f4d8-4c14-b792-c8d6b4b72fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–£–¥–∞–ª—è–µ–º –∫–æ–ª–æ–Ω–∫—É/–∏: ['Unnamed: 0']\n",
      "–ì–æ—Ç–æ–≤–æ. –ß–∏—Å—Ç—ã–π CSV —Å–æ—Ö—Ä–∞–Ω—ë–Ω —Å—é–¥–∞: E:/FirstImpressionsV2/dev_full.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "def clean_csv_to_copy(csv_path: str, output_path: str):\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # –£–¥–∞–ª—è–µ–º –Ω–µ–Ω–∞–∑–≤–∞–Ω–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏\n",
    "    unnamed_cols = [col for col in df.columns if col.startswith(\"Unnamed\") or col.strip() == \"\"]\n",
    "    if unnamed_cols:\n",
    "        print(f\"–£–¥–∞–ª—è–µ–º –∫–æ–ª–æ–Ω–∫—É/–∏: {unnamed_cols}\")\n",
    "        df = df.drop(columns=unnamed_cols)\n",
    "\n",
    "    # –ß–∏—Å—Ç–∏–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –≤ video_name\n",
    "    if 'video_name' not in df.columns:\n",
    "        raise ValueError(\"–§–∞–π–ª –±–µ–∑ 'video_name'. –¢—ã —á—Ç–æ, —Ä–µ—à–∏–ª –ø–æ—à—É—Ç–∏—Ç—å?\")\n",
    "    \n",
    "    df['video_name'] = df['video_name'].apply(lambda x: os.path.splitext(str(x))[0])\n",
    "\n",
    "    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –Ω–æ–≤—ã–π —Ñ–∞–π–ª\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"–ì–æ—Ç–æ–≤–æ. –ß–∏—Å—Ç—ã–π CSV —Å–æ—Ö—Ä–∞–Ω—ë–Ω —Å—é–¥–∞: {output_path}\")\n",
    "    \n",
    "# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è\n",
    "csv_path = \"E:/FirstImpressionsV2/dev_FIv2.csv\"\n",
    "output_path = \"E:/FirstImpressionsV2/dev_full.csv\"\n",
    "clean_csv_to_copy(csv_path, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a29e415-3053-4ba2-ac3e-88ee39d105a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–£–¥–∞–ª—è–µ–º –∫–æ–ª–æ–Ω–∫—É/–∏: ['Unnamed: 0']\n",
      "–ì–æ—Ç–æ–≤–æ. –ß–∏—Å—Ç—ã–π CSV —Å–æ—Ö—Ä–∞–Ω—ë–Ω —Å—é–¥–∞: E:/FirstImpressionsV2/test_full.csv\n"
     ]
    }
   ],
   "source": [
    "csv_path = \"E:/FirstImpressionsV2/test_FIv2.csv\"\n",
    "output_path = \"E:/FirstImpressionsV2/test_full.csv\"\n",
    "clean_csv_to_copy(csv_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1aae35de-5703-48e0-ad13-8b306d8a2403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–£–¥–∞–ª—è–µ–º –∫–æ–ª–æ–Ω–∫—É/–∏: ['Unnamed: 0']\n",
      "–ì–æ—Ç–æ–≤–æ. –ß–∏—Å—Ç—ã–π CSV —Å–æ—Ö—Ä–∞–Ω—ë–Ω —Å—é–¥–∞: E:/FirstImpressionsV2/train_full.csv\n"
     ]
    }
   ],
   "source": [
    "csv_path = \"E:/FirstImpressionsV2/train_FIv2.csv\"\n",
    "output_path = \"E:/FirstImpressionsV2/train_full.csv\"\n",
    "clean_csv_to_copy(csv_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51906294-cbe0-4c0a-83dd-5bb069cbb0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['emo_model.emo_proj.0.weight', 'emo_model.emo_proj.0.bias', 'emo_model.emo_proj.1.weight', 'emo_model.emo_proj.1.bias', 'emo_model.emotion_encoder.0.in_proj.weight', 'emo_model.emotion_encoder.0.in_proj.bias', 'emo_model.emotion_encoder.0.s_B.weight', 'emo_model.emotion_encoder.0.s_B.bias', 'emo_model.emotion_encoder.0.s_C.weight', 'emo_model.emotion_encoder.0.s_C.bias', 'emo_model.emotion_encoder.0.out_proj.weight', 'emo_model.emotion_encoder.0.out_proj.bias', 'emo_model.emotion_encoder.0.norm.weight', 'emo_model.emotion_encoder.0.norm.bias', 'emo_model.emotion_encoder.1.in_proj.weight', 'emo_model.emotion_encoder.1.in_proj.bias', 'emo_model.emotion_encoder.1.s_B.weight', 'emo_model.emotion_encoder.1.s_B.bias', 'emo_model.emotion_encoder.1.s_C.weight', 'emo_model.emotion_encoder.1.s_C.bias', 'emo_model.emotion_encoder.1.out_proj.weight', 'emo_model.emotion_encoder.1.out_proj.bias', 'emo_model.emotion_encoder.1.norm.weight', 'emo_model.emotion_encoder.1.norm.bias', 'emo_model.emotion_encoder.2.in_proj.weight', 'emo_model.emotion_encoder.2.in_proj.bias', 'emo_model.emotion_encoder.2.s_B.weight', 'emo_model.emotion_encoder.2.s_B.bias', 'emo_model.emotion_encoder.2.s_C.weight', 'emo_model.emotion_encoder.2.s_C.bias', 'emo_model.emotion_encoder.2.out_proj.weight', 'emo_model.emotion_encoder.2.out_proj.bias', 'emo_model.emotion_encoder.2.norm.weight', 'emo_model.emotion_encoder.2.norm.bias', 'emo_model.emotion_encoder.3.in_proj.weight', 'emo_model.emotion_encoder.3.in_proj.bias', 'emo_model.emotion_encoder.3.s_B.weight', 'emo_model.emotion_encoder.3.s_B.bias', 'emo_model.emotion_encoder.3.s_C.weight', 'emo_model.emotion_encoder.3.s_C.bias', 'emo_model.emotion_encoder.3.out_proj.weight', 'emo_model.emotion_encoder.3.out_proj.bias', 'emo_model.emotion_encoder.3.norm.weight', 'emo_model.emotion_encoder.3.norm.bias', 'emo_model.emotion_fc_out.0.weight', 'emo_model.emotion_fc_out.0.bias', 'emo_model.emotion_fc_out.1.weight', 'emo_model.emotion_fc_out.1.bias', 'emo_model.emotion_fc_out.4.weight', 'emo_model.emotion_fc_out.4.bias', 'per_model.per_proj.0.weight', 'per_model.per_proj.0.bias', 'per_model.per_proj.1.weight', 'per_model.per_proj.1.bias', 'per_model.personality_encoder.0.self_attention.in_proj_weight', 'per_model.personality_encoder.0.self_attention.in_proj_bias', 'per_model.personality_encoder.0.self_attention.out_proj.weight', 'per_model.personality_encoder.0.self_attention.out_proj.bias', 'per_model.personality_encoder.0.feed_forward.layer_1.weight', 'per_model.personality_encoder.0.feed_forward.layer_1.bias', 'per_model.personality_encoder.0.feed_forward.layer_2.weight', 'per_model.personality_encoder.0.feed_forward.layer_2.bias', 'per_model.personality_encoder.0.add_norm_after_attention.norm.weight', 'per_model.personality_encoder.0.add_norm_after_attention.norm.bias', 'per_model.personality_encoder.0.add_norm_after_ff.norm.weight', 'per_model.personality_encoder.0.add_norm_after_ff.norm.bias', 'per_model.personality_encoder.0.positional_encoding.pe', 'per_model.personality_fc_out.0.weight', 'per_model.personality_fc_out.0.bias', 'per_model.personality_fc_out.1.weight', 'per_model.personality_fc_out.1.bias', 'per_model.personality_fc_out.4.weight', 'per_model.personality_fc_out.4.bias', 'emo_proj.0.weight', 'emo_proj.0.bias', 'emo_proj.1.weight', 'emo_proj.1.bias', 'per_proj.0.weight', 'per_proj.0.bias', 'per_proj.1.weight', 'per_proj.1.bias', 'emotion_to_personality_attn.0.self_attention.in_proj_weight', 'emotion_to_personality_attn.0.self_attention.in_proj_bias', 'emotion_to_personality_attn.0.self_attention.out_proj.weight', 'emotion_to_personality_attn.0.self_attention.out_proj.bias', 'emotion_to_personality_attn.0.feed_forward.layer_1.weight', 'emotion_to_personality_attn.0.feed_forward.layer_1.bias', 'emotion_to_personality_attn.0.feed_forward.layer_2.weight', 'emotion_to_personality_attn.0.feed_forward.layer_2.bias', 'emotion_to_personality_attn.0.add_norm_after_attention.norm.weight', 'emotion_to_personality_attn.0.add_norm_after_attention.norm.bias', 'emotion_to_personality_attn.0.add_norm_after_ff.norm.weight', 'emotion_to_personality_attn.0.add_norm_after_ff.norm.bias', 'emotion_to_personality_attn.0.positional_encoding.pe', 'emotion_to_personality_attn.1.self_attention.in_proj_weight', 'emotion_to_personality_attn.1.self_attention.in_proj_bias', 'emotion_to_personality_attn.1.self_attention.out_proj.weight', 'emotion_to_personality_attn.1.self_attention.out_proj.bias', 'emotion_to_personality_attn.1.feed_forward.layer_1.weight', 'emotion_to_personality_attn.1.feed_forward.layer_1.bias', 'emotion_to_personality_attn.1.feed_forward.layer_2.weight', 'emotion_to_personality_attn.1.feed_forward.layer_2.bias', 'emotion_to_personality_attn.1.add_norm_after_attention.norm.weight', 'emotion_to_personality_attn.1.add_norm_after_attention.norm.bias', 'emotion_to_personality_attn.1.add_norm_after_ff.norm.weight', 'emotion_to_personality_attn.1.add_norm_after_ff.norm.bias', 'emotion_to_personality_attn.1.positional_encoding.pe', 'personality_to_emotion_attn.0.self_attention.in_proj_weight', 'personality_to_emotion_attn.0.self_attention.in_proj_bias', 'personality_to_emotion_attn.0.self_attention.out_proj.weight', 'personality_to_emotion_attn.0.self_attention.out_proj.bias', 'personality_to_emotion_attn.0.feed_forward.layer_1.weight', 'personality_to_emotion_attn.0.feed_forward.layer_1.bias', 'personality_to_emotion_attn.0.feed_forward.layer_2.weight', 'personality_to_emotion_attn.0.feed_forward.layer_2.bias', 'personality_to_emotion_attn.0.add_norm_after_attention.norm.weight', 'personality_to_emotion_attn.0.add_norm_after_attention.norm.bias', 'personality_to_emotion_attn.0.add_norm_after_ff.norm.weight', 'personality_to_emotion_attn.0.add_norm_after_ff.norm.bias', 'personality_to_emotion_attn.0.positional_encoding.pe', 'personality_to_emotion_attn.1.self_attention.in_proj_weight', 'personality_to_emotion_attn.1.self_attention.in_proj_bias', 'personality_to_emotion_attn.1.self_attention.out_proj.weight', 'personality_to_emotion_attn.1.self_attention.out_proj.bias', 'personality_to_emotion_attn.1.feed_forward.layer_1.weight', 'personality_to_emotion_attn.1.feed_forward.layer_1.bias', 'personality_to_emotion_attn.1.feed_forward.layer_2.weight', 'personality_to_emotion_attn.1.feed_forward.layer_2.bias', 'personality_to_emotion_attn.1.add_norm_after_attention.norm.weight', 'personality_to_emotion_attn.1.add_norm_after_attention.norm.bias', 'personality_to_emotion_attn.1.add_norm_after_ff.norm.weight', 'personality_to_emotion_attn.1.add_norm_after_ff.norm.bias', 'personality_to_emotion_attn.1.positional_encoding.pe', 'emotion_personality_fc_out.0.weight', 'emotion_personality_fc_out.0.bias', 'emotion_personality_fc_out.1.weight', 'emotion_personality_fc_out.1.bias', 'emotion_personality_fc_out.4.weight', 'emotion_personality_fc_out.4.bias', 'personality_emotion_fc_out.0.weight', 'personality_emotion_fc_out.0.bias', 'personality_emotion_fc_out.1.weight', 'personality_emotion_fc_out.1.bias', 'personality_emotion_fc_out.4.weight', 'personality_emotion_fc_out.4.bias'])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "checkpoint = torch.load(\"../modalities/text/checkpoints/Mamba_Transformer_bge-small_fusion.pt\", map_location=\"cpu\")\n",
    "\n",
    "print(checkpoint.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fb29a3-acbc-4709-a56a-057115c001eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
