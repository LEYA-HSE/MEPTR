{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-25T15:41:52.006655Z",
     "start_time": "2025-06-25T15:41:49.731204Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-25T15:42:11.377448Z",
     "start_time": "2025-06-25T15:42:11.290680Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ============ Load CSV Labels ============\n",
    "base_path = Path('emo_video')\n",
    "train_df = pd.read_csv(base_path / 'train_full.csv').drop(columns=[\"text\", \"Other\"])\n",
    "test_df = pd.read_csv(base_path / 'test_full' / 'test_full.csv').drop(columns=[\"text\", \"Other\"])"
   ],
   "id": "58329c7c484641af",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-25T15:48:16.678452Z",
     "start_time": "2025-06-25T15:48:16.660878Z"
    }
   },
   "cell_type": "code",
   "source": "train_df.head()",
   "id": "767ade8bf68dc3d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                      video_name  Neutral  Anger  Disgust      Fear  \\\n",
       "0   -3g5yACwYnA_82.7645_100.5550      0.0    0.0      0.0  0.333333   \n",
       "1  -3g5yACwYnA_119.9190_125.2990      1.0    0.0      0.0  0.000000   \n",
       "2     -3g5yACwYnA_4.8400_13.6315      0.0    0.0      0.0  0.200000   \n",
       "3    -3g5yACwYnA_13.6315_27.0310      0.0    0.0      0.0  0.000000   \n",
       "4    -3g5yACwYnA_27.0310_41.3000      0.0    0.0      0.0  0.000000   \n",
       "\n",
       "   Happiness   Sadness  Surprise  \n",
       "0   0.333333  0.333333       0.0  \n",
       "1   0.000000  0.000000       0.0  \n",
       "2   0.400000  0.400000       0.0  \n",
       "3   0.500000  0.500000       0.0  \n",
       "4   1.000000  0.000000       0.0  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_name</th>\n",
       "      <th>Neutral</th>\n",
       "      <th>Anger</th>\n",
       "      <th>Disgust</th>\n",
       "      <th>Fear</th>\n",
       "      <th>Happiness</th>\n",
       "      <th>Sadness</th>\n",
       "      <th>Surprise</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-3g5yACwYnA_82.7645_100.5550</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-3g5yACwYnA_119.9190_125.2990</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-3g5yACwYnA_4.8400_13.6315</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-3g5yACwYnA_13.6315_27.0310</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-3g5yACwYnA_27.0310_41.3000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-25T15:52:38.712369Z",
     "start_time": "2025-06-25T15:49:01.609882Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ============ Load Embeddings ============\n",
    "def load_emo_embeddings(df, emb_dir):\n",
    "    X, y, lengths = [], [], []\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        video_id = row['video_name']\n",
    "        pt_file = Path(emb_dir) / f\"{video_id}.pt\"\n",
    "        \n",
    "        if not pt_file.exists():\n",
    "            continue\n",
    "        try:\n",
    "            emb = torch.load(pt_file, weights_only=True)\n",
    "            if isinstance(emb, dict):\n",
    "                emb = emb['emb']\n",
    "            if torch.isnan(emb).any() or emb.shape != (30, 1024):\n",
    "                continue\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "            \n",
    "        X.append(emb)\n",
    "        y.append(torch.tensor(row.drop('video_name').values.astype('float32')))\n",
    "        lengths.append(torch.tensor(emb.shape[0]))\n",
    "    return torch.stack(X), torch.stack(y), torch.stack(lengths)\n",
    "\n",
    "X_train, y_train, lengths_train = load_emo_embeddings(train_df, 'Embeddings_emonext/cmu_mosei_embeddings/train')\n",
    "X_test, y_test, lengths_test = load_emo_embeddings(test_df, 'Embeddings_emonext/cmu_mosei_embeddings/test')"
   ],
   "id": "7142128079f02212",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16274/16274 [02:55<00:00, 92.68it/s] \n",
      "100%|██████████| 4653/4653 [00:40<00:00, 114.64it/s]\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-25T16:18:00.145072Z",
     "start_time": "2025-06-25T16:18:00.134448Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ============ Data Loaders ============\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train, lengths_train), batch_size=2048, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(X_test, y_test, lengths_test), batch_size=2048)"
   ],
   "id": "67bb3b3968079395",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-25T16:18:01.626440Z",
     "start_time": "2025-06-25T16:18:01.619267Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ============ Model ============\n",
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=1024, d_model=512, num_layers=3, n_heads=8, dropout=0.2, num_classes=7, max_len=30):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(input_dim, d_model)\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, max_len, d_model))\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=n_heads, dim_feedforward=d_model * 4, dropout=dropout, batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        x = self.input_proj(x) + self.pos_embed[:, :x.size(1)]\n",
    "        x = self.encoder(x)\n",
    "        mask = torch.arange(x.size(1), device=lengths.device)[None, :] < lengths[:, None]\n",
    "        mask = mask.float().unsqueeze(2)\n",
    "        summed = (x * mask).sum(dim=1)\n",
    "        count = mask.sum(dim=1).clamp(min=1)\n",
    "        pooled = summed / count\n",
    "        return self.fc(self.dropout(pooled))"
   ],
   "id": "7cdf17f2f640f462",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-25T16:18:02.331213Z",
     "start_time": "2025-06-25T16:18:02.322162Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "def mf1(targets: list[np.ndarray] | np.ndarray, \n",
    "                         predicts: list[np.ndarray] | np.ndarray,\n",
    "                         return_scores: bool = False) -> float | tuple[float, list[float]]:\n",
    "    \"\"\"Calculates mean Macro F1 score (emotional multilabel mMacroF1)\n",
    "    \n",
    "    Args:\n",
    "        targets: Targets array (ground truth)\n",
    "        predicts: Predicts array (model predictions)\n",
    "        return_scores: If True, returns both mean and per-class scores\n",
    "        \n",
    "    Returns:\n",
    "        float: Mean Macro F1 score across all classes\n",
    "        or\n",
    "        tuple[float, list[float]]: If return_scores=True, returns (mean, per_class_scores)\n",
    "    \"\"\"\n",
    "    targets = np.array(targets)\n",
    "    predicts = np.array(predicts)\n",
    "\n",
    "    f1_macro_scores = []\n",
    "    for i in range(predicts.shape[1]):\n",
    "        cr = classification_report(targets[:, i], predicts[:, i], \n",
    "                                         output_dict=True, zero_division=0)\n",
    "        f1_macro_scores.append(cr['macro avg']['f1-score'])\n",
    "\n",
    "    if return_scores:\n",
    "        return np.mean(f1_macro_scores), f1_macro_scores\n",
    "    return np.mean(f1_macro_scores)\n",
    "\n",
    "\n",
    "def uar(targets: list[np.ndarray] | np.ndarray,\n",
    "                    predicts: list[np.ndarray] | np.ndarray,\n",
    "                    return_scores: bool = False) -> float | tuple[float, list[float]]:\n",
    "    \"\"\"Calculates mean Unweighted Average Recall (emotional multilabel mUAR)\n",
    "    \n",
    "    Args:\n",
    "        targets: Targets array (ground truth)\n",
    "        predicts: Predicts array (model predictions)\n",
    "        return_scores: If True, returns both mean and per-class scores\n",
    "        \n",
    "    Returns:\n",
    "        float: Mean UAR across all classes\n",
    "        or\n",
    "        tuple[float, list[float]]: If return_scores=True, returns (mean, per_class_scores)\n",
    "    \"\"\"\n",
    "    targets = np.array(targets)\n",
    "    predicts = np.array(predicts)\n",
    "\n",
    "    uar_scores = []\n",
    "    for i in range(predicts.shape[1]):\n",
    "        cr = classification_report(targets[:, i], predicts[:, i],\n",
    "                                         output_dict=True, zero_division=0)\n",
    "        uar_scores.append(cr['macro avg']['recall'])\n",
    "\n",
    "    if return_scores:\n",
    "        return np.mean(uar_scores), uar_scores\n",
    "    return np.mean(uar_scores)\n",
    "\n",
    "\n",
    "def transform_matrix(matrix):\n",
    "    threshold1 = 1 - 1/7 \n",
    "    threshold2 = 1/7\n",
    "    mask1 = matrix[:, 0] >= threshold1\n",
    "    result = np.zeros_like(matrix[:, 1:])\n",
    "    transformed = (matrix[:, 1:] >= threshold2).astype(int)\n",
    "    result[~mask1] = transformed[~mask1]\n",
    "    return result\n",
    "\n",
    "def process_predictions(pred_emo, true_emo):\n",
    "    pred_emo = torch.nn.functional.softmax(pred_emo, dim=1).cpu().detach().numpy()\n",
    "    pred_emo = transform_matrix(pred_emo).tolist()\n",
    "    true_emo = true_emo.cpu().detach().numpy()\n",
    "    true_emo = np.where(true_emo > 0, 1, 0)[:, 1:].tolist()\n",
    "    return pred_emo, true_emo"
   ],
   "id": "cf732be17a339266",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-25T19:46:29.421033Z",
     "start_time": "2025-06-25T19:10:36.707219Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ============ Training ============\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = TransformerClassifier(input_dim=1024, num_classes=7).to(DEVICE)\n",
    "\n",
    "def weighted_kl_div(log_probs, target_probs, class_weights):\n",
    "    \"\"\"\n",
    "    Manually computes class-weighted KL divergence:\n",
    "    L = sum_j w_j * q_j * (log q_j - log p_j)\n",
    "    where q_j is target prob and p_j is predicted prob.\n",
    "    \"\"\"\n",
    "    kl_per_class = target_probs * (torch.log(target_probs + 1e-8) - log_probs)  # (B, C)\n",
    "    weighted_kl = kl_per_class * class_weights  # apply weights per class\n",
    "    return weighted_kl.sum(dim=1).mean()  # average over batch\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0003)\n",
    "\n",
    "class_weights = torch.tensor([3.0, 10.0, 13.0, 20.0, 2.0, 15.0, 18.0], device=DEVICE)\n",
    "\n",
    "for epoch in range(1, 121):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for xb, yb, lb in train_loader:\n",
    "        xb, yb, lb = xb.to(DEVICE), yb.to(DEVICE), lb.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(xb, lb)\n",
    "        \n",
    "        log_probs = torch.log_softmax(logits, dim=1)\n",
    "        loss = weighted_kl_div(log_probs, yb, class_weights)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    val_loss, raw_preds, raw_targets = 0, [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb, lb in val_loader:\n",
    "            xb, yb, lb = xb.to(DEVICE), yb.to(DEVICE), lb.to(DEVICE)\n",
    "            logits = model(xb, lb)\n",
    "            \n",
    "            log_probs = torch.log_softmax(logits, dim=1)\n",
    "            loss = weighted_kl_div(log_probs, yb, class_weights)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            raw_preds.append(logits)\n",
    "            raw_targets.append(yb)\n",
    "\n",
    "    # === Post-processing ===\n",
    "    raw_preds = torch.cat(raw_preds)\n",
    "    raw_targets = torch.cat(raw_targets)\n",
    "    bin_preds, bin_targets = process_predictions(raw_preds, raw_targets) # Сюда лучше пихать предсказанные логиты \n",
    "\n",
    "    f1 = mf1(bin_targets, bin_preds)\n",
    "    recall = uar(bin_targets, bin_preds)\n",
    "\n",
    "    print(f\"[Epoch {epoch}] Train Loss: {total_loss:.4f} | Val Loss: {val_loss:.4f} | mF1: {f1:.4f} | mUAR: {recall:.4f}\")\n"
   ],
   "id": "ef4b91eeb271ce18",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Train Loss: 107.1628 | Val Loss: 32.3877 | mF1: 0.3661 | mUAR: 0.5141\n",
      "[Epoch 2] Train Loss: 82.0465 | Val Loss: 30.0806 | mF1: 0.3896 | mUAR: 0.5260\n",
      "[Epoch 3] Train Loss: 78.1312 | Val Loss: 28.7276 | mF1: 0.4519 | mUAR: 0.5333\n",
      "[Epoch 4] Train Loss: 77.0553 | Val Loss: 29.0115 | mF1: 0.4569 | mUAR: 0.5333\n",
      "[Epoch 5] Train Loss: 75.9762 | Val Loss: 28.3438 | mF1: 0.4664 | mUAR: 0.5398\n",
      "[Epoch 6] Train Loss: 75.1291 | Val Loss: 28.5183 | mF1: 0.4760 | mUAR: 0.5421\n",
      "[Epoch 7] Train Loss: 74.2318 | Val Loss: 28.1862 | mF1: 0.4768 | mUAR: 0.5522\n",
      "[Epoch 8] Train Loss: 73.6746 | Val Loss: 28.6274 | mF1: 0.4846 | mUAR: 0.5475\n",
      "[Epoch 9] Train Loss: 73.1868 | Val Loss: 28.9858 | mF1: 0.4629 | mUAR: 0.5446\n",
      "[Epoch 10] Train Loss: 72.5164 | Val Loss: 28.4995 | mF1: 0.4775 | mUAR: 0.5535\n",
      "[Epoch 11] Train Loss: 72.1916 | Val Loss: 28.5801 | mF1: 0.4911 | mUAR: 0.5540\n",
      "[Epoch 12] Train Loss: 71.3207 | Val Loss: 29.3425 | mF1: 0.4958 | mUAR: 0.5525\n",
      "[Epoch 13] Train Loss: 71.2465 | Val Loss: 29.0834 | mF1: 0.4851 | mUAR: 0.5524\n",
      "[Epoch 14] Train Loss: 71.5540 | Val Loss: 28.8872 | mF1: 0.4878 | mUAR: 0.5477\n",
      "[Epoch 15] Train Loss: 70.4390 | Val Loss: 29.5470 | mF1: 0.4894 | mUAR: 0.5554\n",
      "[Epoch 16] Train Loss: 70.1207 | Val Loss: 29.6828 | mF1: 0.5096 | mUAR: 0.5551\n",
      "[Epoch 17] Train Loss: 69.6675 | Val Loss: 28.9813 | mF1: 0.5032 | mUAR: 0.5567\n",
      "[Epoch 18] Train Loss: 69.3823 | Val Loss: 29.3168 | mF1: 0.4934 | mUAR: 0.5616\n",
      "[Epoch 19] Train Loss: 68.7968 | Val Loss: 29.6190 | mF1: 0.5009 | mUAR: 0.5579\n",
      "[Epoch 20] Train Loss: 68.3321 | Val Loss: 29.6424 | mF1: 0.4953 | mUAR: 0.5573\n",
      "[Epoch 21] Train Loss: 68.1028 | Val Loss: 29.6650 | mF1: 0.4952 | mUAR: 0.5602\n",
      "[Epoch 22] Train Loss: 68.0787 | Val Loss: 29.3268 | mF1: 0.5240 | mUAR: 0.5624\n",
      "[Epoch 23] Train Loss: 68.7354 | Val Loss: 30.8503 | mF1: 0.5031 | mUAR: 0.5598\n",
      "[Epoch 24] Train Loss: 68.9305 | Val Loss: 29.6464 | mF1: 0.4918 | mUAR: 0.5575\n",
      "[Epoch 25] Train Loss: 68.0839 | Val Loss: 29.5643 | mF1: 0.5102 | mUAR: 0.5628\n",
      "[Epoch 26] Train Loss: 68.8453 | Val Loss: 29.1136 | mF1: 0.5163 | mUAR: 0.5614\n",
      "[Epoch 27] Train Loss: 67.8954 | Val Loss: 29.2823 | mF1: 0.5013 | mUAR: 0.5590\n",
      "[Epoch 28] Train Loss: 66.8059 | Val Loss: 29.9214 | mF1: 0.5217 | mUAR: 0.5710\n",
      "[Epoch 29] Train Loss: 66.2116 | Val Loss: 29.5740 | mF1: 0.5166 | mUAR: 0.5656\n",
      "[Epoch 30] Train Loss: 65.8881 | Val Loss: 29.8576 | mF1: 0.5116 | mUAR: 0.5622\n",
      "[Epoch 31] Train Loss: 66.3611 | Val Loss: 30.2300 | mF1: 0.4852 | mUAR: 0.5548\n",
      "[Epoch 32] Train Loss: 66.0397 | Val Loss: 30.1819 | mF1: 0.4982 | mUAR: 0.5527\n",
      "[Epoch 33] Train Loss: 66.0699 | Val Loss: 30.0725 | mF1: 0.5194 | mUAR: 0.5653\n",
      "[Epoch 34] Train Loss: 65.0128 | Val Loss: 31.0136 | mF1: 0.5293 | mUAR: 0.5747\n",
      "[Epoch 35] Train Loss: 65.5672 | Val Loss: 30.8445 | mF1: 0.4973 | mUAR: 0.5638\n",
      "[Epoch 36] Train Loss: 65.9050 | Val Loss: 29.8645 | mF1: 0.5112 | mUAR: 0.5678\n",
      "[Epoch 37] Train Loss: 65.0163 | Val Loss: 30.3540 | mF1: 0.5268 | mUAR: 0.5635\n",
      "[Epoch 38] Train Loss: 64.6683 | Val Loss: 31.4537 | mF1: 0.4955 | mUAR: 0.5591\n",
      "[Epoch 39] Train Loss: 66.5387 | Val Loss: 29.7385 | mF1: 0.5005 | mUAR: 0.5609\n",
      "[Epoch 40] Train Loss: 66.3302 | Val Loss: 30.6308 | mF1: 0.4769 | mUAR: 0.5563\n",
      "[Epoch 41] Train Loss: 65.7187 | Val Loss: 30.3895 | mF1: 0.5213 | mUAR: 0.5669\n",
      "[Epoch 42] Train Loss: 64.4454 | Val Loss: 30.1625 | mF1: 0.5284 | mUAR: 0.5718\n",
      "[Epoch 43] Train Loss: 65.1988 | Val Loss: 30.7656 | mF1: 0.4942 | mUAR: 0.5598\n",
      "[Epoch 44] Train Loss: 65.0132 | Val Loss: 30.0274 | mF1: 0.5020 | mUAR: 0.5607\n",
      "[Epoch 45] Train Loss: 64.8221 | Val Loss: 30.4558 | mF1: 0.5095 | mUAR: 0.5563\n",
      "[Epoch 46] Train Loss: 64.3370 | Val Loss: 30.8159 | mF1: 0.4992 | mUAR: 0.5613\n",
      "[Epoch 47] Train Loss: 63.9372 | Val Loss: 30.2032 | mF1: 0.5124 | mUAR: 0.5624\n",
      "[Epoch 48] Train Loss: 63.1456 | Val Loss: 30.5354 | mF1: 0.5180 | mUAR: 0.5684\n",
      "[Epoch 49] Train Loss: 63.4150 | Val Loss: 30.5630 | mF1: 0.5281 | mUAR: 0.5653\n",
      "[Epoch 50] Train Loss: 63.0534 | Val Loss: 30.7431 | mF1: 0.5220 | mUAR: 0.5687\n",
      "[Epoch 51] Train Loss: 62.6245 | Val Loss: 30.5550 | mF1: 0.5252 | mUAR: 0.5724\n",
      "[Epoch 52] Train Loss: 62.5937 | Val Loss: 31.0476 | mF1: 0.5308 | mUAR: 0.5664\n",
      "[Epoch 53] Train Loss: 62.3093 | Val Loss: 31.1609 | mF1: 0.5328 | mUAR: 0.5774\n",
      "[Epoch 54] Train Loss: 62.2886 | Val Loss: 30.8123 | mF1: 0.5190 | mUAR: 0.5664\n",
      "[Epoch 55] Train Loss: 61.9767 | Val Loss: 31.1257 | mF1: 0.5354 | mUAR: 0.5731\n",
      "[Epoch 56] Train Loss: 62.8505 | Val Loss: 31.0550 | mF1: 0.5150 | mUAR: 0.5679\n",
      "[Epoch 57] Train Loss: 62.7215 | Val Loss: 31.0465 | mF1: 0.5097 | mUAR: 0.5620\n",
      "[Epoch 58] Train Loss: 62.3254 | Val Loss: 31.5165 | mF1: 0.5216 | mUAR: 0.5705\n",
      "[Epoch 59] Train Loss: 62.1961 | Val Loss: 30.6094 | mF1: 0.5140 | mUAR: 0.5634\n",
      "[Epoch 60] Train Loss: 61.6189 | Val Loss: 31.6046 | mF1: 0.5172 | mUAR: 0.5633\n",
      "[Epoch 61] Train Loss: 61.7069 | Val Loss: 31.2725 | mF1: 0.5141 | mUAR: 0.5614\n",
      "[Epoch 62] Train Loss: 61.2124 | Val Loss: 30.9589 | mF1: 0.5232 | mUAR: 0.5657\n",
      "[Epoch 63] Train Loss: 61.0918 | Val Loss: 31.2992 | mF1: 0.5213 | mUAR: 0.5693\n",
      "[Epoch 64] Train Loss: 61.7043 | Val Loss: 31.4321 | mF1: 0.5121 | mUAR: 0.5620\n",
      "[Epoch 65] Train Loss: 61.4812 | Val Loss: 31.4386 | mF1: 0.5323 | mUAR: 0.5680\n",
      "[Epoch 66] Train Loss: 61.4515 | Val Loss: 32.8250 | mF1: 0.5136 | mUAR: 0.5653\n",
      "[Epoch 67] Train Loss: 62.0543 | Val Loss: 31.5840 | mF1: 0.5158 | mUAR: 0.5636\n",
      "[Epoch 68] Train Loss: 61.6350 | Val Loss: 32.4264 | mF1: 0.5160 | mUAR: 0.5663\n",
      "[Epoch 69] Train Loss: 60.9454 | Val Loss: 31.5176 | mF1: 0.5082 | mUAR: 0.5602\n",
      "[Epoch 70] Train Loss: 60.4549 | Val Loss: 32.7147 | mF1: 0.5066 | mUAR: 0.5582\n",
      "[Epoch 71] Train Loss: 61.1080 | Val Loss: 31.6809 | mF1: 0.5198 | mUAR: 0.5665\n",
      "[Epoch 72] Train Loss: 60.1447 | Val Loss: 32.4644 | mF1: 0.5268 | mUAR: 0.5650\n",
      "[Epoch 73] Train Loss: 59.4976 | Val Loss: 32.1701 | mF1: 0.5331 | mUAR: 0.5705\n",
      "[Epoch 74] Train Loss: 59.7406 | Val Loss: 32.2398 | mF1: 0.5236 | mUAR: 0.5655\n",
      "[Epoch 75] Train Loss: 60.1334 | Val Loss: 31.9928 | mF1: 0.5172 | mUAR: 0.5580\n",
      "[Epoch 76] Train Loss: 60.2098 | Val Loss: 32.5029 | mF1: 0.5263 | mUAR: 0.5662\n",
      "[Epoch 77] Train Loss: 59.7436 | Val Loss: 31.7239 | mF1: 0.5214 | mUAR: 0.5624\n",
      "[Epoch 78] Train Loss: 58.8334 | Val Loss: 32.8012 | mF1: 0.5080 | mUAR: 0.5564\n",
      "[Epoch 79] Train Loss: 59.2393 | Val Loss: 32.2824 | mF1: 0.5283 | mUAR: 0.5682\n",
      "[Epoch 80] Train Loss: 58.9738 | Val Loss: 33.1810 | mF1: 0.5309 | mUAR: 0.5666\n",
      "[Epoch 81] Train Loss: 58.9614 | Val Loss: 32.4294 | mF1: 0.5095 | mUAR: 0.5627\n",
      "[Epoch 82] Train Loss: 58.8227 | Val Loss: 34.3615 | mF1: 0.5236 | mUAR: 0.5638\n",
      "[Epoch 83] Train Loss: 58.4730 | Val Loss: 32.4962 | mF1: 0.5326 | mUAR: 0.5686\n",
      "[Epoch 84] Train Loss: 59.4717 | Val Loss: 33.2759 | mF1: 0.5282 | mUAR: 0.5627\n",
      "[Epoch 85] Train Loss: 59.3966 | Val Loss: 32.6354 | mF1: 0.5248 | mUAR: 0.5610\n",
      "[Epoch 86] Train Loss: 58.4129 | Val Loss: 32.3438 | mF1: 0.5248 | mUAR: 0.5691\n",
      "[Epoch 87] Train Loss: 57.7416 | Val Loss: 33.4366 | mF1: 0.5401 | mUAR: 0.5681\n",
      "[Epoch 88] Train Loss: 57.7874 | Val Loss: 31.8948 | mF1: 0.5214 | mUAR: 0.5654\n",
      "[Epoch 89] Train Loss: 57.6968 | Val Loss: 33.8842 | mF1: 0.5336 | mUAR: 0.5605\n",
      "[Epoch 90] Train Loss: 58.2715 | Val Loss: 33.3955 | mF1: 0.5434 | mUAR: 0.5715\n",
      "[Epoch 91] Train Loss: 57.0288 | Val Loss: 32.1603 | mF1: 0.5230 | mUAR: 0.5622\n",
      "[Epoch 92] Train Loss: 56.5463 | Val Loss: 34.7369 | mF1: 0.5390 | mUAR: 0.5651\n",
      "[Epoch 93] Train Loss: 57.2942 | Val Loss: 33.3923 | mF1: 0.5382 | mUAR: 0.5680\n",
      "[Epoch 94] Train Loss: 56.7955 | Val Loss: 32.5745 | mF1: 0.5290 | mUAR: 0.5677\n",
      "[Epoch 95] Train Loss: 56.0721 | Val Loss: 33.5173 | mF1: 0.5398 | mUAR: 0.5684\n",
      "[Epoch 96] Train Loss: 55.2289 | Val Loss: 33.3025 | mF1: 0.5416 | mUAR: 0.5717\n",
      "[Epoch 97] Train Loss: 55.1750 | Val Loss: 32.9035 | mF1: 0.5309 | mUAR: 0.5712\n",
      "[Epoch 98] Train Loss: 55.2842 | Val Loss: 33.8883 | mF1: 0.5355 | mUAR: 0.5686\n",
      "[Epoch 99] Train Loss: 54.7525 | Val Loss: 33.9197 | mF1: 0.5284 | mUAR: 0.5656\n",
      "[Epoch 100] Train Loss: 54.8190 | Val Loss: 34.0083 | mF1: 0.5353 | mUAR: 0.5693\n",
      "[Epoch 101] Train Loss: 54.6011 | Val Loss: 35.4652 | mF1: 0.5395 | mUAR: 0.5619\n",
      "[Epoch 102] Train Loss: 54.4577 | Val Loss: 34.5829 | mF1: 0.5401 | mUAR: 0.5695\n",
      "[Epoch 103] Train Loss: 53.5480 | Val Loss: 35.3967 | mF1: 0.5454 | mUAR: 0.5687\n",
      "[Epoch 104] Train Loss: 54.9869 | Val Loss: 36.0530 | mF1: 0.5405 | mUAR: 0.5661\n",
      "[Epoch 105] Train Loss: 57.0604 | Val Loss: 33.6345 | mF1: 0.5320 | mUAR: 0.5654\n",
      "[Epoch 106] Train Loss: 55.5411 | Val Loss: 33.7175 | mF1: 0.5418 | mUAR: 0.5661\n",
      "[Epoch 107] Train Loss: 54.9282 | Val Loss: 34.6106 | mF1: 0.5377 | mUAR: 0.5692\n",
      "[Epoch 108] Train Loss: 54.1820 | Val Loss: 34.9005 | mF1: 0.5316 | mUAR: 0.5649\n",
      "[Epoch 109] Train Loss: 52.9768 | Val Loss: 34.5432 | mF1: 0.5457 | mUAR: 0.5737\n",
      "[Epoch 110] Train Loss: 53.0859 | Val Loss: 35.0172 | mF1: 0.5341 | mUAR: 0.5681\n",
      "[Epoch 111] Train Loss: 52.7892 | Val Loss: 34.8762 | mF1: 0.5460 | mUAR: 0.5774\n",
      "[Epoch 112] Train Loss: 52.9617 | Val Loss: 35.4336 | mF1: 0.5422 | mUAR: 0.5675\n",
      "[Epoch 113] Train Loss: 52.4080 | Val Loss: 35.4908 | mF1: 0.5386 | mUAR: 0.5715\n",
      "[Epoch 114] Train Loss: 51.3241 | Val Loss: 35.5278 | mF1: 0.5359 | mUAR: 0.5647\n",
      "[Epoch 115] Train Loss: 51.5491 | Val Loss: 35.4254 | mF1: 0.5367 | mUAR: 0.5722\n",
      "[Epoch 116] Train Loss: 52.9155 | Val Loss: 36.7495 | mF1: 0.5414 | mUAR: 0.5642\n",
      "[Epoch 117] Train Loss: 53.2373 | Val Loss: 35.8276 | mF1: 0.5282 | mUAR: 0.5642\n",
      "[Epoch 118] Train Loss: 53.2808 | Val Loss: 36.0789 | mF1: 0.5204 | mUAR: 0.5634\n",
      "[Epoch 119] Train Loss: 52.4394 | Val Loss: 35.0130 | mF1: 0.5371 | mUAR: 0.5737\n",
      "[Epoch 120] Train Loss: 51.8193 | Val Loss: 34.9875 | mF1: 0.5399 | mUAR: 0.5700\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-25T20:22:51.548407Z",
     "start_time": "2025-06-25T20:22:41.514463Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save weights and optimizer\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "}, \"cmu_mosei_best_checkpoint.pth\")"
   ],
   "id": "8f136c553a779565",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f422553b2c959484"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
