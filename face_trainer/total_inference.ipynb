{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-26T10:48:41.803090Z",
     "start_time": "2025-06-26T10:48:39.755806Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class TransformerBackbone(nn.Module):\n",
    "    def __init__(self, input_dim=1024, d_model=512, num_layers=3, n_heads=8, dropout=0.2, max_len=30):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(input_dim, d_model)\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, max_len, d_model))\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=n_heads, dim_feedforward=d_model * 4, dropout=dropout, batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        x = self.input_proj(x) + self.pos_embed[:, :x.size(1)]\n",
    "        x = self.encoder(x)\n",
    "        mask = torch.arange(x.size(1), device=lengths.device)[None, :] < lengths[:, None]\n",
    "        mask = mask.float().unsqueeze(2)\n",
    "        summed = (x * mask).sum(dim=1)\n",
    "        count = mask.sum(dim=1).clamp(min=1)\n",
    "        pooled = summed / count\n",
    "        return self.dropout(pooled)\n",
    "\n",
    "\n",
    "class FrozenTransformerWrapper(nn.Module):\n",
    "    def __init__(self, scripted_model_path, backbone_type='traits'):\n",
    "        super().__init__()\n",
    "        self.model = TransformerBackbone()\n",
    "        #state_dict = torch.jit.load(scripted_model_path).state_dict()\n",
    "        state_dict = torch.load(scripted_model_path, weights_only=True)['model_state_dict']\n",
    "        filtered = {k: v for k, v in state_dict.items() if not k.startswith('fc')}\n",
    "        self.model.load_state_dict(filtered, strict=False)\n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        return self.model(x, lengths)\n",
    "\n",
    "\n",
    "class CrossAttentionBlock(nn.Module):\n",
    "    def __init__(self, d_model=512, n_heads=8, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout, batch_first=True)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, q, kv):\n",
    "        out, _ = self.attn(q, kv, kv)\n",
    "        return self.norm(q + self.dropout(out))\n",
    "\n",
    "\n",
    "\n",
    "class MultiTaskFusionModel(nn.Module):\n",
    "    def __init__(self, trait_model_path=\"models_checkpoints/fiv2_best_checkpoint.pth\", emo_model_path=\"models_checkpoints/cmu_mosei_best_checkpoint.pth\", d_model=512, dropout=0.2):\n",
    "        super().__init__()\n",
    "        \n",
    "        # self.trait_model = FrozenTransformerWrapper(\"models_checkpoints/fiv2_best_checkpoint.pth\")\n",
    "        # self.emo_model = FrozenTransformerWrapper(\"models_checkpoints/cmu_mosei_best_checkpoint.pth\")\n",
    "\n",
    "        self.trait_model = FrozenTransformerWrapper(trait_model_path)\n",
    "        self.emo_model = FrozenTransformerWrapper(emo_model_path)\n",
    "\n",
    "        self.trait_encoder = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model, 8, 4*d_model, dropout=dropout, batch_first=True), num_layers=2)\n",
    "        self.emo_encoder = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model, 8, 4*d_model, dropout=dropout, batch_first=True), num_layers=2)\n",
    "\n",
    "        self.cross1 = CrossAttentionBlock(d_model, 8, dropout)\n",
    "        self.cross2 = CrossAttentionBlock(d_model, 8, dropout)\n",
    "\n",
    "        self.shared_mlp = nn.Sequential(\n",
    "            nn.Linear(2 * d_model, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.trait_head = nn.Linear(d_model, 5)\n",
    "        self.emo_head = nn.Linear(d_model, 7)\n",
    "\n",
    "    def forward(self, x, lengths, task='traits'):\n",
    "        trait_feat = self.trait_model(x, lengths).unsqueeze(1)\n",
    "        emo_feat = self.emo_model(x, lengths).unsqueeze(1)\n",
    "\n",
    "        trait_encoded = self.trait_encoder(trait_feat)\n",
    "        emo_encoded = self.emo_encoder(emo_feat)\n",
    "\n",
    "        trait_cross = self.cross1(trait_encoded, emo_encoded)\n",
    "        emo_cross = self.cross2(emo_encoded, trait_encoded)\n",
    "\n",
    "        fused = torch.cat([trait_cross, emo_cross], dim=-1).squeeze(1)\n",
    "        hidden = self.shared_mlp(fused)\n",
    "\n",
    "        if task == 'traits':\n",
    "            return self.trait_head(hidden)\n",
    "        elif task == 'emotions':\n",
    "            return self.emo_head(hidden)\n",
    "        else:\n",
    "            raise ValueError(\"task must be either 'traits' or 'emotions'\")"
   ],
   "id": "6e3e8e0c982d767a",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-26T10:48:45.037035Z",
     "start_time": "2025-06-26T10:48:41.825536Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "from emonext_model import get_model\n",
    "\n",
    "# =================== CONFIG ===================\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_FRAMES = 30\n",
    "FRAME_SIZE = 224\n",
    "EMO_CHECKPOINT = 'models_checkpoints/cmu_mosei_best_checkpoint.pth'\n",
    "TRAIT_CHECKPOINT = 'models_checkpoints/fiv2_best_checkpoint.pth'\n",
    "FUSION_MODEL_PATH = 'models_checkpoints/multitask_fusion_model.pth'\n",
    "\n",
    "# ================== Load EmoNeXt ==================\n",
    "emonext = get_model(num_classes=7, model_size=\"base\", in_22k=False).to(DEVICE)\n",
    "emonext.eval()\n",
    "\n",
    "# ================ Face Detection ==================\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "face_detector = mp_face_detection.FaceDetection(model_selection=1, min_detection_confidence=0.6)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((FRAME_SIZE, FRAME_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def get_evenly_spaced_frames(video_path, num_frames):\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    idxs = np.linspace(0, total - 1, num=num_frames, dtype=int)\n",
    "    frames = []\n",
    "    for i in idxs:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            frames.append(frame)\n",
    "    cap.release()\n",
    "    return frames\n",
    "\n",
    "def get_face(frame):\n",
    "    results = face_detector.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    if results.detections:\n",
    "        detection = results.detections[0]\n",
    "        bbox = detection.location_data.relative_bounding_box\n",
    "        h, w, _ = frame.shape\n",
    "        x1 = max(int(bbox.xmin * w), 0)\n",
    "        y1 = max(int(bbox.ymin * h), 0)\n",
    "        x2 = min(int((bbox.xmin + bbox.width) * w), w)\n",
    "        y2 = min(int((bbox.ymin + bbox.height) * h), h)\n",
    "        return frame[y1:y2, x1:x2]\n",
    "    return None\n",
    "\n",
    "def get_embeddings_from_video(video_path, num_frames):\n",
    "    frames = get_evenly_spaced_frames(video_path, num_frames)\n",
    "    face_tensors = []\n",
    "    last_face = None\n",
    "    for frame in frames:\n",
    "        face = get_face(frame)\n",
    "        if face is not None:\n",
    "            last_face = face\n",
    "        if last_face is not None:\n",
    "            tensor = transform(last_face)\n",
    "            face_tensors.append(tensor)\n",
    "\n",
    "    if not face_tensors:\n",
    "        return None\n",
    "\n",
    "    batch = torch.stack(face_tensors).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        aligned = emonext.stn(batch)\n",
    "        emb = emonext.forward_features(aligned)\n",
    "    return emb.cpu()  # shape: (30, 1024)\n",
    "\n",
    "# ================== Load Model ==================\n",
    "class FrozenTransformerWrapper(nn.Module):\n",
    "    def __init__(self, checkpoint_path):\n",
    "        super().__init__()\n",
    "        self.model = TransformerBackbone()\n",
    "        state_dict = torch.load(checkpoint_path, weights_only=True)['model_state_dict']\n",
    "        filtered = {k: v for k, v in state_dict.items() if not k.startswith('fc')}\n",
    "        self.model.load_state_dict(filtered, strict=False)\n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        return self.model(x, lengths)\n",
    "\n",
    "\n",
    "# ================== Inference Function ==================\n",
    "@torch.no_grad()\n",
    "def infer_from_video(video_path, model):\n",
    "    emb = get_embeddings_from_video(video_path, num_frames=30)\n",
    "    if emb is None or torch.isnan(emb).any():\n",
    "        raise ValueError(\"Embedding failed or NaNs present\")\n",
    "\n",
    "    emb = emb.unsqueeze(0).to(DEVICE)  # [1, 30, 1024]\n",
    "    lengths = torch.tensor([emb.shape[1]], device=DEVICE)\n",
    "\n",
    "    model.eval()\n",
    "    emo_logits = model(emb, lengths, task='emotions')  # (1, 7)\n",
    "    trait_out = model(emb, lengths, task='traits')     # (1, 5)\n",
    "\n",
    "    emo_probs = F.softmax(emo_logits, dim=1).squeeze(0).cpu()\n",
    "    trait_scores = trait_out.squeeze(0).cpu()\n",
    "    return emo_probs, trait_scores\n"
   ],
   "id": "b1c8bcb6ddc4c434",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-26T10:48:46.418319Z",
     "start_time": "2025-06-26T10:48:45.197899Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ================== Main ==================\n",
    "\n",
    "fusion_model = MultiTaskFusionModel().to(DEVICE)\n",
    "fusion_model.load_state_dict(torch.load(FUSION_MODEL_PATH, weights_only=True))\n",
    "fusion_model.eval()\n",
    "\n",
    "VIDEO_PATH = 'sample_data/meow2.mp4'\n",
    "emotions, traits = infer_from_video(VIDEO_PATH, fusion_model)\n"
   ],
   "id": "27cee44eba15a401",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-26T10:48:46.446845Z",
     "start_time": "2025-06-26T10:48:46.440845Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"\\nEmotion distribution (softmax, sum=1):\")\n",
    "emotion_names = ['Neutral', 'Anger', 'Disgust', 'Fear', 'Happiness', 'Sadness', 'Surprise']\n",
    "for emotion_name, emotion in zip(emotion_names, emotions.tolist()):\n",
    "    print(f\"  {emotion_name}: {emotion:.4f}\")\n",
    "\n",
    "print(\"\\nPersonality traits (OCEAN):\")\n",
    "trait_names = ['Openness', 'Conscientiousness', 'Extraversion', 'Agreeableness', 'Neuroticism']\n",
    "for name, v in zip(trait_names, traits.tolist()):\n",
    "    print(f\"  {name}: {v:.4f}\")"
   ],
   "id": "9d183d781450bfc0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Emotion distribution (softmax, sum=1):\n",
      "  Neutral: 0.0096\n",
      "  Anger: 0.0062\n",
      "  Disgust: 0.0616\n",
      "  Fear: 0.0133\n",
      "  Happiness: 0.6115\n",
      "  Sadness: 0.2362\n",
      "  Surprise: 0.0617\n",
      "\n",
      "Personality traits (OCEAN):\n",
      "  Openness: 0.5722\n",
      "  Conscientiousness: 0.5299\n",
      "  Extraversion: 0.4823\n",
      "  Agreeableness: 0.5799\n",
      "  Neuroticism: 0.5416\n"
     ]
    }
   ],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
