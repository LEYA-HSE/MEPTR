{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-05T02:32:54.700924Z",
     "start_time": "2025-07-05T02:32:49.451049Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x4/w5lsbv5n3csbc11x8v5nb8sm0000gn/T/ipykernel_77278/3936055183.py:243: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ck = torch.load(path, map_location=device)\n",
      "/var/folders/x4/w5lsbv5n3csbc11x8v5nb8sm0000gn/T/ipykernel_77278/3936055183.py:273: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ck = torch.load(path, map_location=device)\n",
      "/var/folders/x4/w5lsbv5n3csbc11x8v5nb8sm0000gn/T/ipykernel_77278/3936055183.py:300: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt       = torch.load(fusion_ckpt_path, map_location=device)\n",
      "/var/folders/x4/w5lsbv5n3csbc11x8v5nb8sm0000gn/T/ipykernel_77278/3936055183.py:329: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  signal, sr = librosa.load(audio_path, sr=16000)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'your_audio.wav'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mLibsndfileError\u001B[0m                           Traceback (most recent call last)",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/site-packages/librosa/core/audio.py:176\u001B[0m, in \u001B[0;36mload\u001B[0;34m(path, sr, mono, offset, duration, dtype, res_type)\u001B[0m\n\u001B[1;32m    175\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 176\u001B[0m     y, sr_native \u001B[38;5;241m=\u001B[39m \u001B[43m__soundfile_load\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moffset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mduration\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    178\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m sf\u001B[38;5;241m.\u001B[39mSoundFileRuntimeError \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[1;32m    179\u001B[0m     \u001B[38;5;66;03m# If soundfile failed, try audioread instead\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/site-packages/librosa/core/audio.py:209\u001B[0m, in \u001B[0;36m__soundfile_load\u001B[0;34m(path, offset, duration, dtype)\u001B[0m\n\u001B[1;32m    207\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    208\u001B[0m     \u001B[38;5;66;03m# Otherwise, create the soundfile object\u001B[39;00m\n\u001B[0;32m--> 209\u001B[0m     context \u001B[38;5;241m=\u001B[39m \u001B[43msf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mSoundFile\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    211\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m context \u001B[38;5;28;01mas\u001B[39;00m sf_desc:\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/site-packages/soundfile.py:658\u001B[0m, in \u001B[0;36mSoundFile.__init__\u001B[0;34m(self, file, mode, samplerate, channels, subtype, endian, format, closefd)\u001B[0m\n\u001B[1;32m    656\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_info \u001B[38;5;241m=\u001B[39m _create_info_struct(file, mode, samplerate, channels,\n\u001B[1;32m    657\u001B[0m                                  \u001B[38;5;28mformat\u001B[39m, subtype, endian)\n\u001B[0;32m--> 658\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_file \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode_int\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mclosefd\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    659\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mset\u001B[39m(mode)\u001B[38;5;241m.\u001B[39missuperset(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mr+\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mseekable():\n\u001B[1;32m    660\u001B[0m     \u001B[38;5;66;03m# Move write position to 0 (like in Python file objects)\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/site-packages/soundfile.py:1216\u001B[0m, in \u001B[0;36mSoundFile._open\u001B[0;34m(self, file, mode_int, closefd)\u001B[0m\n\u001B[1;32m   1215\u001B[0m     err \u001B[38;5;241m=\u001B[39m _snd\u001B[38;5;241m.\u001B[39msf_error(file_ptr)\n\u001B[0;32m-> 1216\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m LibsndfileError(err, prefix\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mError opening \u001B[39m\u001B[38;5;132;01m{0!r}\u001B[39;00m\u001B[38;5;124m: \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname))\n\u001B[1;32m   1217\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m mode_int \u001B[38;5;241m==\u001B[39m _snd\u001B[38;5;241m.\u001B[39mSFM_WRITE:\n\u001B[1;32m   1218\u001B[0m     \u001B[38;5;66;03m# Due to a bug in libsndfile version <= 1.0.25, frames != 0\u001B[39;00m\n\u001B[1;32m   1219\u001B[0m     \u001B[38;5;66;03m# when opening a named pipe in SFM_WRITE mode.\u001B[39;00m\n\u001B[1;32m   1220\u001B[0m     \u001B[38;5;66;03m# See http://github.com/erikd/libsndfile/issues/77.\u001B[39;00m\n",
      "\u001B[0;31mLibsndfileError\u001B[0m: Error opening 'your_audio.wav': System error.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[17], line 345\u001B[0m\n\u001B[1;32m    342\u001B[0m emo_names \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mNeutral\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mAnger\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mDisgust\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mFear\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mHappiness\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mSadness\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mSurprise\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m    343\u001B[0m pers_names \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mOpenness\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mConscientiousness\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mExtraversion\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mAgreeableness\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mNeuroticism\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m--> 345\u001B[0m emb \u001B[38;5;241m=\u001B[39m \u001B[43mextract_embeddings\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43myour_audio.wav\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    346\u001B[0m emo_probs, per_scores \u001B[38;5;241m=\u001B[39m run_inference(model, device, emb)\n\u001B[1;32m    347\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEmotion\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[0;32mIn[17], line 329\u001B[0m, in \u001B[0;36mextract_embeddings\u001B[0;34m(audio_path)\u001B[0m\n\u001B[1;32m    328\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mextract_embeddings\u001B[39m(audio_path: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m np\u001B[38;5;241m.\u001B[39mndarray:\n\u001B[0;32m--> 329\u001B[0m         signal, sr \u001B[38;5;241m=\u001B[39m \u001B[43mlibrosa\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43maudio_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m16000\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    330\u001B[0m         emb \u001B[38;5;241m=\u001B[39m process_audio(signal, sr)\n\u001B[1;32m    331\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m emb\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/site-packages/librosa/core/audio.py:184\u001B[0m, in \u001B[0;36mload\u001B[0;34m(path, sr, mono, offset, duration, dtype, res_type)\u001B[0m\n\u001B[1;32m    180\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(path, (\u001B[38;5;28mstr\u001B[39m, pathlib\u001B[38;5;241m.\u001B[39mPurePath)):\n\u001B[1;32m    181\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[1;32m    182\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPySoundFile failed. Trying audioread instead.\u001B[39m\u001B[38;5;124m\"\u001B[39m, stacklevel\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m\n\u001B[1;32m    183\u001B[0m     )\n\u001B[0;32m--> 184\u001B[0m     y, sr_native \u001B[38;5;241m=\u001B[39m \u001B[43m__audioread_load\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moffset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mduration\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    185\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    186\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m exc\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/site-packages/decorator.py:235\u001B[0m, in \u001B[0;36mdecorate.<locals>.fun\u001B[0;34m(*args, **kw)\u001B[0m\n\u001B[1;32m    233\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m kwsyntax:\n\u001B[1;32m    234\u001B[0m     args, kw \u001B[38;5;241m=\u001B[39m fix(args, kw, sig)\n\u001B[0;32m--> 235\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcaller\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mextras\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/site-packages/librosa/util/decorators.py:59\u001B[0m, in \u001B[0;36mdeprecated.<locals>.__wrapper\u001B[0;34m(func, *args, **kwargs)\u001B[0m\n\u001B[1;32m     50\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Warn the user, and then proceed.\"\"\"\u001B[39;00m\n\u001B[1;32m     51\u001B[0m warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{:s}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{:s}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124mDeprecated as of librosa version \u001B[39m\u001B[38;5;132;01m{:s}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     53\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124mIt will be removed in librosa version \u001B[39m\u001B[38;5;132;01m{:s}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     57\u001B[0m     stacklevel\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m,  \u001B[38;5;66;03m# Would be 2, but the decorator adds a level\u001B[39;00m\n\u001B[1;32m     58\u001B[0m )\n\u001B[0;32m---> 59\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/site-packages/librosa/core/audio.py:240\u001B[0m, in \u001B[0;36m__audioread_load\u001B[0;34m(path, offset, duration, dtype)\u001B[0m\n\u001B[1;32m    237\u001B[0m     reader \u001B[38;5;241m=\u001B[39m path\n\u001B[1;32m    238\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    239\u001B[0m     \u001B[38;5;66;03m# If the input was not an audioread object, try to open it\u001B[39;00m\n\u001B[0;32m--> 240\u001B[0m     reader \u001B[38;5;241m=\u001B[39m \u001B[43maudioread\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maudio_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    242\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m reader \u001B[38;5;28;01mas\u001B[39;00m input_file:\n\u001B[1;32m    243\u001B[0m     sr_native \u001B[38;5;241m=\u001B[39m input_file\u001B[38;5;241m.\u001B[39msamplerate\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/site-packages/audioread/__init__.py:127\u001B[0m, in \u001B[0;36maudio_open\u001B[0;34m(path, backends)\u001B[0m\n\u001B[1;32m    125\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m BackendClass \u001B[38;5;129;01min\u001B[39;00m backends:\n\u001B[1;32m    126\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 127\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mBackendClass\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    128\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m DecodeError:\n\u001B[1;32m    129\u001B[0m         \u001B[38;5;28;01mpass\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/site-packages/audioread/rawread.py:59\u001B[0m, in \u001B[0;36mRawAudioFile.__init__\u001B[0;34m(self, filename)\u001B[0m\n\u001B[1;32m     58\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, filename):\n\u001B[0;32m---> 59\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fh \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     61\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     62\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_file \u001B[38;5;241m=\u001B[39m aifc\u001B[38;5;241m.\u001B[39mopen(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fh)\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'your_audio.wav'"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import librosa\n",
    "import torch.nn as nn\n",
    "from transformers import Wav2Vec2Processor\n",
    "from transformers.models.wav2vec2.modeling_wav2vec2 import (\n",
    "    Wav2Vec2Model,\n",
    "    Wav2Vec2PreTrainedModel,\n",
    ")\n",
    "\n",
    "class EmotionModel(Wav2Vec2PreTrainedModel):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "        self.wav2vec2 = Wav2Vec2Model(config)\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_values):\n",
    "        outputs = self.wav2vec2(input_values)\n",
    "        hidden_states = outputs[0]  # (batch_size, sequence_length, hidden_size)\n",
    "        return hidden_states\n",
    "    \n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_name = 'audeering/wav2vec2-large-robust-12-ft-emotion-msp-dim'\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
    "audio_embedder = EmotionModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "def process_audio(signal: np.ndarray, sampling_rate: int) -> np.ndarray:\n",
    "    inputs = processor(signal, sampling_rate=sampling_rate, return_tensors=\"pt\", padding=True)\n",
    "    input_values = inputs[\"input_values\"].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = audio_embedder(input_values)\n",
    "        embeddings = outputs\n",
    "        \n",
    "    return embeddings.detach().cpu().numpy()\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=10000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pos = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n",
    "        div = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(pos * div)\n",
    "        pe[:, 1::2] = torch.cos(pos * div)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return x\n",
    "    \n",
    "class PositionalEncoding_per(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pos = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n",
    "        div = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(pos * div)\n",
    "        pe[:, 1::2] = torch.cos(pos * div)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return x\n",
    "\n",
    "class TransformerRegressor(nn.Module):\n",
    "    def __init__(self, input_size, d_model, num_layers, num_heads, dim_feedforward, dropout, num_targets):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(input_size, d_model)\n",
    "        self.pos_enc = PositionalEncoding_per(d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            activation='relu'\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.head = nn.Linear(d_model, num_targets)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        x = self.input_proj(x)\n",
    "        x = self.pos_enc(x)\n",
    "        x = x.transpose(0, 1) \n",
    "        lengths_cpu = lengths.cpu()\n",
    "        mask = self._generate_padding_mask(lengths_cpu, x.size(0)).to(x.device)\n",
    "        x = self.encoder(x, src_key_padding_mask=mask)\n",
    "        x = x.transpose(0, 1)\n",
    "        pooled = []\n",
    "        for i, L in enumerate(lengths_cpu):\n",
    "            if L > 0:\n",
    "                pooled.append(x[i, :L].mean(dim=0))\n",
    "            else:\n",
    "                pooled.append(torch.zeros(x.size(-1), device=x.device))\n",
    "        pooled = torch.stack(pooled, dim=0)\n",
    "        return self.head(pooled)\n",
    "\n",
    "    @staticmethod\n",
    "    def _generate_padding_mask(lengths, max_len):\n",
    "        mask = torch.arange(max_len).expand(len(lengths), max_len) >= lengths.unsqueeze(1)\n",
    "        return mask\n",
    "\n",
    "\n",
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim: int,\n",
    "                 d_model: int,\n",
    "                 num_heads: int,\n",
    "                 num_layers: int,\n",
    "                 dim_feedforward: int,\n",
    "                 num_classes: int,\n",
    "                 dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(input_dim, d_model)\n",
    "        self.pos_enc = PositionalEncoding(d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            activation='relu'\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        x = self.input_proj(x)                      \n",
    "        x = self.pos_enc(x)\n",
    "\n",
    "        x = x.transpose(0, 1)\n",
    "        mask = self._generate_padding_mask(lengths, x.size(0))\n",
    "        x = self.transformer(x, src_key_padding_mask=mask)\n",
    "        x = x.transpose(0, 1)         \n",
    "        pooled = torch.stack([\n",
    "            x[i, :lengths[i], :].mean(dim=0) if lengths[i] > 0\n",
    "            else torch.zeros(x.size(2), device=x.device)\n",
    "            for i in range(x.size(0))\n",
    "        ], dim=0)\n",
    "        return self.classifier(self.dropout(pooled))\n",
    "\n",
    "    @staticmethod\n",
    "    def _generate_padding_mask(lengths, max_len):\n",
    "        bs = lengths.size(0)\n",
    "        mask = torch.arange(max_len, device=lengths.device).expand(bs, max_len)\n",
    "        return mask >= lengths.unsqueeze(1)\n",
    "\n",
    "class FusionTransformer(nn.Module):\n",
    "    def __init__(self, emo_enc, per_enc, config):\n",
    "        super().__init__()\n",
    "        for p in emo_enc.parameters(): p.requires_grad = False\n",
    "        for p in per_enc.parameters(): p.requires_grad = False\n",
    "\n",
    "        self.emo_enc = emo_enc\n",
    "        self.per_enc = per_enc\n",
    "\n",
    "        h = config['hidden_dim']\n",
    "        d = config['dropout']\n",
    "        heads = config['tr_heads']\n",
    "\n",
    "        self.emo_proj = nn.Sequential(\n",
    "            nn.Linear(emo_enc.output_dim, h),\n",
    "            nn.LayerNorm(h),\n",
    "            nn.Dropout(d)\n",
    "        )\n",
    "        self.per_proj = nn.Sequential(\n",
    "            nn.Linear(per_enc.output_dim, h),\n",
    "            nn.LayerNorm(h),\n",
    "            nn.Dropout(d)\n",
    "        )\n",
    "\n",
    "        self.mha_e2p = nn.MultiheadAttention(embed_dim=h, num_heads=heads, dropout=d, batch_first=True)\n",
    "        self.mha_p2e = nn.MultiheadAttention(embed_dim=h, num_heads=heads, dropout=d, batch_first=True)\n",
    "\n",
    "        self.emo_head = nn.Sequential(\n",
    "            nn.Linear(h*2,   h),\n",
    "            nn.LayerNorm(h),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(d),\n",
    "            nn.Linear(h, config['num_emotions'])\n",
    "        )\n",
    "        self.per_head = nn.Sequential(\n",
    "            nn.Linear(h*2,   h),\n",
    "            nn.LayerNorm(h),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(d),\n",
    "            nn.Linear(h, config['num_traits'])\n",
    "        )\n",
    "\n",
    "    def forward(self, emo_input=None, per_input=None):\n",
    "        base_emo_logits = base_per_scores = None\n",
    "        if emo_input is not None:\n",
    "            x_e, len_e = emo_input\n",
    "            feat_e = self.emo_enc.extract_features(x_e, len_e)  \n",
    "            base_emo_logits = self.emo_enc(x_e, len_e)        \n",
    "            emo_emd = self.emo_proj(feat_e)                     \n",
    "        if per_input is not None:\n",
    "            x_p, len_p = per_input\n",
    "            feat_p = self.per_enc.extract_features(x_p, len_p)  \n",
    "            base_per_scores = self.per_enc(x_p, len_p)      \n",
    "            per_emd = self.per_proj(feat_p)                   \n",
    "\n",
    "        if emo_input is not None and per_input is not None:\n",
    "            attn_e2p, _ = self.mha_e2p(query=emo_emd, key=per_emd, value=per_emd)\n",
    "            emo_emd = emo_emd + attn_e2p\n",
    "\n",
    "            attn_p2e, _ = self.mha_p2e(query=per_emd, key=emo_emd, value=emo_emd)\n",
    "            per_emd = per_emd + attn_p2e\n",
    "\n",
    "            fe = emo_emd.mean(dim=1)  \n",
    "            fp = per_emd.mean(dim=1)  \n",
    "            cat = torch.cat([fe, fp], dim=-1)  \n",
    "\n",
    "            emo_new = self.emo_head(cat) \n",
    "            per_new = self.per_head(cat) \n",
    "\n",
    "            final_emo = (emo_new + base_emo_logits) / 2\n",
    "            final_per = (per_new + base_per_scores) / 2\n",
    "\n",
    "            return {'emotion': final_emo, 'personality': final_per}\n",
    "\n",
    "        elif emo_input is not None:\n",
    "            return {'emotion': base_emo_logits}\n",
    "        else:\n",
    "            return {'personality': base_per_scores}\n",
    "\n",
    "\n",
    "def load_pretrained_emotion_encoder(path, device):\n",
    "    enc = TransformerClassifier(\n",
    "        input_dim=1024,\n",
    "        d_model=128,\n",
    "        num_heads=4,\n",
    "        num_layers=3,\n",
    "        dim_feedforward=512,\n",
    "        num_classes=7,\n",
    "        dropout=0.2\n",
    "    ).to(device)\n",
    "\n",
    "    ck = torch.load(path, map_location=device)\n",
    "    enc.load_state_dict(ck)\n",
    "    enc.eval()\n",
    "\n",
    "    enc.output_dim = enc.input_proj.out_features  \n",
    "\n",
    "    def extract_features(x, lengths):\n",
    "        h = enc.input_proj(x)           \n",
    "        h = enc.pos_enc(h)          \n",
    "        h = h.transpose(0, 1)          \n",
    "        mask = TransformerClassifier._generate_padding_mask(lengths, h.size(0))\n",
    "        h = enc.transformer(h, src_key_padding_mask=mask) \n",
    "        h = h.transpose(0, 1)          \n",
    "        return h\n",
    "\n",
    "    enc.extract_features = extract_features\n",
    "    return enc\n",
    "\n",
    "\n",
    "def load_pretrained_personality_encoder(path, device):\n",
    "    enc = TransformerRegressor(\n",
    "        input_size=1024,\n",
    "        d_model=256,\n",
    "        num_heads=4,\n",
    "        num_layers=3,\n",
    "        dim_feedforward=512,\n",
    "        num_targets=5,\n",
    "        dropout=0.3\n",
    "    ).to(device)\n",
    "\n",
    "    ck = torch.load(path, map_location=device)\n",
    "    enc.load_state_dict(ck)\n",
    "    enc.eval()\n",
    "\n",
    "    enc.output_dim = enc.input_proj.out_features  \n",
    "\n",
    "    def extract_features(x, lengths):\n",
    "        h = enc.input_proj(x)          \n",
    "        h = enc.pos_enc(h)\n",
    "        h = h.transpose(0, 1)          \n",
    "        mask = TransformerRegressor._generate_padding_mask(lengths.cpu(), h.size(0)).to(h.device)\n",
    "        h = enc.encoder(h, src_key_padding_mask=mask) \n",
    "        h = h.transpose(0, 1)          \n",
    "        return h\n",
    "\n",
    "    enc.extract_features = extract_features\n",
    "    return enc\n",
    "\n",
    "def load_fusion_model(\n",
    "    fusion_ckpt_path: str,\n",
    "    emo_encoder_ckpt: str,\n",
    "    per_encoder_ckpt: str,\n",
    "    device: str = 'cpu'\n",
    "):\n",
    "    device = torch.device(device)\n",
    "    emo_enc = load_pretrained_emotion_encoder(emo_encoder_ckpt, device)\n",
    "    per_enc = load_pretrained_personality_encoder(per_encoder_ckpt, device)\n",
    "    ckpt       = torch.load(fusion_ckpt_path, map_location=device)\n",
    "    best_cfg   = ckpt['config']\n",
    "    state_dict = ckpt['state_dict']\n",
    "    model = FusionTransformer(emo_enc, per_enc, best_cfg).to(device)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()\n",
    "    return model, device\n",
    "\n",
    "def run_inference(\n",
    "    model: FusionTransformer,\n",
    "    device: torch.device,\n",
    "    embedding: np.ndarray\n",
    "):\n",
    "    if embedding.ndim == 3 and embedding.shape[0] == 1:\n",
    "        emb = embedding[0]  \n",
    "    elif embedding.ndim == 2:\n",
    "        emb = embedding    \n",
    "\n",
    "    x = torch.tensor(emb, dtype=torch.float32).unsqueeze(0).to(device) \n",
    "    lengths = torch.tensor([emb.shape[0]], dtype=torch.long).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model(emo_input=(x, lengths), per_input=(x, lengths))\n",
    "        emo_logits = out['emotion'].cpu().numpy().squeeze(0)\n",
    "        per_scores = out['personality'].cpu().numpy().squeeze(0)\n",
    "        emo_probs  = torch.softmax(torch.tensor(emo_logits), dim=-1).numpy()\n",
    "    return emo_probs, per_scores\n",
    "\n",
    "def extract_embeddings(audio_path: str) -> np.ndarray:\n",
    "        signal, sr = librosa.load(audio_path, sr=16000)\n",
    "        emb = process_audio(signal, sr)\n",
    "        return emb\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    FUSION_CKPT = \"best_fusion_overall_trans.pt\"\n",
    "    EMO_ENC_CKPT = \"final_best_model_uni_trans.pt\"\n",
    "    PER_ENC_CKPT = \"best_trans_fiv2.pt\"\n",
    "\n",
    "    model, device = load_fusion_model(\n",
    "        FUSION_CKPT, EMO_ENC_CKPT, PER_ENC_CKPT, device='cpu'\n",
    "    )\n",
    "    \n",
    "    emo_names = ['Neutral', 'Anger', 'Disgust', 'Fear', 'Happiness', 'Sadness', 'Surprise']\n",
    "    pers_names = ['Openness', 'Conscientiousness', 'Extraversion', 'Agreeableness', 'Neuroticism']\n",
    "    \n",
    "    emb = extract_embeddings(\"your_audio.wav\")\n",
    "    emo_probs, per_scores = run_inference(model, device, emb)\n",
    "    print(\"Emotion\")\n",
    "    for name, v in zip(emo_names, emo_probs):\n",
    "        print(f\"  {name}: {v:.4f}\")\n",
    "        \n",
    "    print(\"Personality\")   \n",
    "    for name, v in zip(pers_names, per_scores):\n",
    "        print(f\"  {name}: {v:.4f}\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x4/w5lsbv5n3csbc11x8v5nb8sm0000gn/T/ipykernel_77278/966127404.py:176: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ck = torch.load(path, map_location=device)\n",
      "/var/folders/x4/w5lsbv5n3csbc11x8v5nb8sm0000gn/T/ipykernel_77278/966127404.py:192: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ck = torch.load(path, map_location=device)\n",
      "/var/folders/x4/w5lsbv5n3csbc11x8v5nb8sm0000gn/T/ipykernel_77278/966127404.py:216: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt       = torch.load(fusion_ckpt_path, map_location=device)\n",
      "/var/folders/x4/w5lsbv5n3csbc11x8v5nb8sm0000gn/T/ipykernel_77278/966127404.py:245: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  signal, sr = librosa.load(audio_path, sr=16000)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'your_audio.wav'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mLibsndfileError\u001B[0m                           Traceback (most recent call last)",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/site-packages/librosa/core/audio.py:176\u001B[0m, in \u001B[0;36mload\u001B[0;34m(path, sr, mono, offset, duration, dtype, res_type)\u001B[0m\n\u001B[1;32m    175\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 176\u001B[0m     y, sr_native \u001B[38;5;241m=\u001B[39m \u001B[43m__soundfile_load\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moffset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mduration\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    178\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m sf\u001B[38;5;241m.\u001B[39mSoundFileRuntimeError \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[1;32m    179\u001B[0m     \u001B[38;5;66;03m# If soundfile failed, try audioread instead\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/site-packages/librosa/core/audio.py:209\u001B[0m, in \u001B[0;36m__soundfile_load\u001B[0;34m(path, offset, duration, dtype)\u001B[0m\n\u001B[1;32m    207\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    208\u001B[0m     \u001B[38;5;66;03m# Otherwise, create the soundfile object\u001B[39;00m\n\u001B[0;32m--> 209\u001B[0m     context \u001B[38;5;241m=\u001B[39m \u001B[43msf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mSoundFile\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    211\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m context \u001B[38;5;28;01mas\u001B[39;00m sf_desc:\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/site-packages/soundfile.py:658\u001B[0m, in \u001B[0;36mSoundFile.__init__\u001B[0;34m(self, file, mode, samplerate, channels, subtype, endian, format, closefd)\u001B[0m\n\u001B[1;32m    656\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_info \u001B[38;5;241m=\u001B[39m _create_info_struct(file, mode, samplerate, channels,\n\u001B[1;32m    657\u001B[0m                                  \u001B[38;5;28mformat\u001B[39m, subtype, endian)\n\u001B[0;32m--> 658\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_file \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode_int\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mclosefd\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    659\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mset\u001B[39m(mode)\u001B[38;5;241m.\u001B[39missuperset(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mr+\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mseekable():\n\u001B[1;32m    660\u001B[0m     \u001B[38;5;66;03m# Move write position to 0 (like in Python file objects)\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/site-packages/soundfile.py:1216\u001B[0m, in \u001B[0;36mSoundFile._open\u001B[0;34m(self, file, mode_int, closefd)\u001B[0m\n\u001B[1;32m   1215\u001B[0m     err \u001B[38;5;241m=\u001B[39m _snd\u001B[38;5;241m.\u001B[39msf_error(file_ptr)\n\u001B[0;32m-> 1216\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m LibsndfileError(err, prefix\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mError opening \u001B[39m\u001B[38;5;132;01m{0!r}\u001B[39;00m\u001B[38;5;124m: \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname))\n\u001B[1;32m   1217\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m mode_int \u001B[38;5;241m==\u001B[39m _snd\u001B[38;5;241m.\u001B[39mSFM_WRITE:\n\u001B[1;32m   1218\u001B[0m     \u001B[38;5;66;03m# Due to a bug in libsndfile version <= 1.0.25, frames != 0\u001B[39;00m\n\u001B[1;32m   1219\u001B[0m     \u001B[38;5;66;03m# when opening a named pipe in SFM_WRITE mode.\u001B[39;00m\n\u001B[1;32m   1220\u001B[0m     \u001B[38;5;66;03m# See http://github.com/erikd/libsndfile/issues/77.\u001B[39;00m\n",
      "\u001B[0;31mLibsndfileError\u001B[0m: Error opening 'your_audio.wav': System error.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[18], line 261\u001B[0m\n\u001B[1;32m    258\u001B[0m emo_names \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mNeutral\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mAnger\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mDisgust\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mFear\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mHappiness\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mSadness\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mSurprise\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m    259\u001B[0m pers_names \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mOpenness\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mConscientiousness\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mExtraversion\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mAgreeableness\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mNeuroticism\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m--> 261\u001B[0m emb \u001B[38;5;241m=\u001B[39m \u001B[43mextract_embeddings\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43myour_audio.wav\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    262\u001B[0m emo_probs, per_scores \u001B[38;5;241m=\u001B[39m run_inference(model, device, emb)\n\u001B[1;32m    263\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEmotion\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[0;32mIn[18], line 245\u001B[0m, in \u001B[0;36mextract_embeddings\u001B[0;34m(audio_path)\u001B[0m\n\u001B[1;32m    244\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mextract_embeddings\u001B[39m(audio_path: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m np\u001B[38;5;241m.\u001B[39mndarray:\n\u001B[0;32m--> 245\u001B[0m         signal, sr \u001B[38;5;241m=\u001B[39m \u001B[43mlibrosa\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43maudio_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m16000\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    246\u001B[0m         emb \u001B[38;5;241m=\u001B[39m process_audio(signal, sr)\n\u001B[1;32m    247\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m emb\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/site-packages/librosa/core/audio.py:184\u001B[0m, in \u001B[0;36mload\u001B[0;34m(path, sr, mono, offset, duration, dtype, res_type)\u001B[0m\n\u001B[1;32m    180\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(path, (\u001B[38;5;28mstr\u001B[39m, pathlib\u001B[38;5;241m.\u001B[39mPurePath)):\n\u001B[1;32m    181\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[1;32m    182\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPySoundFile failed. Trying audioread instead.\u001B[39m\u001B[38;5;124m\"\u001B[39m, stacklevel\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m\n\u001B[1;32m    183\u001B[0m     )\n\u001B[0;32m--> 184\u001B[0m     y, sr_native \u001B[38;5;241m=\u001B[39m \u001B[43m__audioread_load\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moffset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mduration\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    185\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    186\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m exc\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/site-packages/decorator.py:235\u001B[0m, in \u001B[0;36mdecorate.<locals>.fun\u001B[0;34m(*args, **kw)\u001B[0m\n\u001B[1;32m    233\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m kwsyntax:\n\u001B[1;32m    234\u001B[0m     args, kw \u001B[38;5;241m=\u001B[39m fix(args, kw, sig)\n\u001B[0;32m--> 235\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcaller\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mextras\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/site-packages/librosa/util/decorators.py:59\u001B[0m, in \u001B[0;36mdeprecated.<locals>.__wrapper\u001B[0;34m(func, *args, **kwargs)\u001B[0m\n\u001B[1;32m     50\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Warn the user, and then proceed.\"\"\"\u001B[39;00m\n\u001B[1;32m     51\u001B[0m warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{:s}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{:s}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124mDeprecated as of librosa version \u001B[39m\u001B[38;5;132;01m{:s}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     53\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124mIt will be removed in librosa version \u001B[39m\u001B[38;5;132;01m{:s}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     57\u001B[0m     stacklevel\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m,  \u001B[38;5;66;03m# Would be 2, but the decorator adds a level\u001B[39;00m\n\u001B[1;32m     58\u001B[0m )\n\u001B[0;32m---> 59\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/site-packages/librosa/core/audio.py:240\u001B[0m, in \u001B[0;36m__audioread_load\u001B[0;34m(path, offset, duration, dtype)\u001B[0m\n\u001B[1;32m    237\u001B[0m     reader \u001B[38;5;241m=\u001B[39m path\n\u001B[1;32m    238\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    239\u001B[0m     \u001B[38;5;66;03m# If the input was not an audioread object, try to open it\u001B[39;00m\n\u001B[0;32m--> 240\u001B[0m     reader \u001B[38;5;241m=\u001B[39m \u001B[43maudioread\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maudio_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    242\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m reader \u001B[38;5;28;01mas\u001B[39;00m input_file:\n\u001B[1;32m    243\u001B[0m     sr_native \u001B[38;5;241m=\u001B[39m input_file\u001B[38;5;241m.\u001B[39msamplerate\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/site-packages/audioread/__init__.py:127\u001B[0m, in \u001B[0;36maudio_open\u001B[0;34m(path, backends)\u001B[0m\n\u001B[1;32m    125\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m BackendClass \u001B[38;5;129;01min\u001B[39;00m backends:\n\u001B[1;32m    126\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 127\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mBackendClass\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    128\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m DecodeError:\n\u001B[1;32m    129\u001B[0m         \u001B[38;5;28;01mpass\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/site-packages/audioread/rawread.py:59\u001B[0m, in \u001B[0;36mRawAudioFile.__init__\u001B[0;34m(self, filename)\u001B[0m\n\u001B[1;32m     58\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, filename):\n\u001B[0;32m---> 59\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fh \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     61\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     62\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_file \u001B[38;5;241m=\u001B[39m aifc\u001B[38;5;241m.\u001B[39mopen(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fh)\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'your_audio.wav'"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import librosa\n",
    "import torch.nn as nn\n",
    "from transformers import Wav2Vec2Processor\n",
    "from transformers.models.wav2vec2.modeling_wav2vec2 import (\n",
    "    Wav2Vec2Model,\n",
    "    Wav2Vec2PreTrainedModel,\n",
    ")\n",
    "\n",
    "class EmotionModel(Wav2Vec2PreTrainedModel):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "        self.wav2vec2 = Wav2Vec2Model(config)\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_values):\n",
    "        outputs = self.wav2vec2(input_values)\n",
    "        hidden_states = outputs[0]  # (batch_size, sequence_length, hidden_size)\n",
    "        return hidden_states\n",
    "    \n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_name = 'audeering/wav2vec2-large-robust-12-ft-emotion-msp-dim'\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
    "audio_embedder = EmotionModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "def process_audio(signal: np.ndarray, sampling_rate: int) -> np.ndarray:\n",
    "    inputs = processor(signal, sampling_rate=sampling_rate, return_tensors=\"pt\", padding=True)\n",
    "    input_values = inputs[\"input_values\"].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = audio_embedder(input_values)\n",
    "        embeddings = outputs\n",
    "        \n",
    "    return embeddings.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "class CustomMambaBlock(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.in_proj = nn.Linear(d_model, d_model)\n",
    "        self.s_B = nn.Linear(d_model, d_model)\n",
    "        self.s_C = nn.Linear(d_model, d_model)\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.act = nn.ReLU()\n",
    "    def forward(self, x):\n",
    "        x_in = x\n",
    "        x = self.in_proj(x)\n",
    "        x = x + self.s_B(x) + self.s_C(x)\n",
    "        x = self.act(x)\n",
    "        x = self.out_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return self.norm(x + x_in)\n",
    "\n",
    "class CustomMambaClassifier(nn.Module):\n",
    "    def __init__(self, input_size=1024, d_model=256, num_layers=2, num_classes=7, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(input_size, d_model)\n",
    "        self.blocks = nn.ModuleList([CustomMambaBlock(d_model, dropout) for _ in range(num_layers)])\n",
    "        self.fc = nn.Linear(d_model, num_classes)\n",
    "    def forward(self, x, lengths):\n",
    "        x = self.input_proj(x)\n",
    "        for blk in self.blocks: x = blk(x)\n",
    "        pooled = [x[i, :L].mean(dim=0) if L>0 else torch.zeros(x.size(-1), device=x.device)\n",
    "                  for i, L in enumerate(lengths)]\n",
    "        return self.fc(torch.stack(pooled, dim=0))\n",
    "    \n",
    "\n",
    "class CustomMambaRegressor(nn.Module):\n",
    "    def __init__(self, input_size, d_model, num_layers, num_targets, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(input_size, d_model)\n",
    "        self.blocks = nn.ModuleList([CustomMambaBlock(d_model, dropout) for _ in range(num_layers)])\n",
    "        self.head = nn.Linear(d_model, num_targets)\n",
    "    def forward(self, x, lengths):\n",
    "        x = self.input_proj(x)\n",
    "        for blk in self.blocks: x = blk(x)\n",
    "        pooled = [x[i, :L].mean(dim=0) if L>0 else torch.zeros(x.size(-1), device=x.device)\n",
    "                  for i, L in enumerate(lengths)]\n",
    "        return self.head(torch.stack(pooled, dim=0))\n",
    "\n",
    "class FusionTransformer(nn.Module):\n",
    "    def __init__(self, emo_enc, per_enc, config):\n",
    "        super().__init__()\n",
    "       \n",
    "        for p in emo_enc.parameters(): p.requires_grad = False\n",
    "        for p in per_enc.parameters(): p.requires_grad = False\n",
    "\n",
    "        self.emo_enc = emo_enc\n",
    "        self.per_enc = per_enc\n",
    "\n",
    "        h = config['hidden_dim']\n",
    "        d = config['dropout']\n",
    "        heads = config['tr_heads']\n",
    "\n",
    "        \n",
    "        self.emo_proj = nn.Sequential(\n",
    "            nn.Linear(emo_enc.output_dim, h),\n",
    "            nn.LayerNorm(h),\n",
    "            nn.Dropout(d)\n",
    "        )\n",
    "        self.per_proj = nn.Sequential(\n",
    "            nn.Linear(per_enc.output_dim, h),\n",
    "            nn.LayerNorm(h),\n",
    "            nn.Dropout(d)\n",
    "        )\n",
    "\n",
    "        \n",
    "        self.mha_e2p = nn.MultiheadAttention(embed_dim=h, num_heads=heads, dropout=d, batch_first=True)\n",
    "        self.mha_p2e = nn.MultiheadAttention(embed_dim=h, num_heads=heads, dropout=d, batch_first=True)\n",
    "\n",
    "  \n",
    "        self.emo_head = nn.Sequential(\n",
    "            nn.Linear(h*2,   h),\n",
    "            nn.LayerNorm(h),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(d),\n",
    "            nn.Linear(h, config['num_emotions'])\n",
    "        )\n",
    "        self.per_head = nn.Sequential(\n",
    "            nn.Linear(h*2,   h),\n",
    "            nn.LayerNorm(h),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(d),\n",
    "            nn.Linear(h, config['num_traits'])\n",
    "        )\n",
    "\n",
    "    def forward(self, emo_input=None, per_input=None):\n",
    "        \n",
    "        base_emo_logits = base_per_scores = None\n",
    "        if emo_input is not None:\n",
    "            x_e, len_e = emo_input\n",
    "            feat_e = self.emo_enc.extract_features(x_e, len_e)   \n",
    "            base_emo_logits = self.emo_enc(x_e, len_e)         \n",
    "            emo_emd = self.emo_proj(feat_e)                    \n",
    "        if per_input is not None:\n",
    "            x_p, len_p = per_input\n",
    "            feat_p = self.per_enc.extract_features(x_p, len_p) \n",
    "            base_per_scores = self.per_enc(x_p, len_p)          \n",
    "            per_emd = self.per_proj(feat_p)                    \n",
    "            \n",
    "        if emo_input is not None and per_input is not None:\n",
    "           \n",
    "            attn_e2p, _ = self.mha_e2p(query=emo_emd, key=per_emd, value=per_emd)\n",
    "            emo_emd = emo_emd + attn_e2p\n",
    "          \n",
    "            attn_p2e, _ = self.mha_p2e(query=per_emd, key=emo_emd, value=emo_emd)\n",
    "            per_emd = per_emd + attn_p2e\n",
    "\n",
    "            fe = emo_emd.mean(dim=1) \n",
    "            fp = per_emd.mean(dim=1) \n",
    "            cat = torch.cat([fe, fp], dim=-1) \n",
    "            \n",
    "            emo_new = self.emo_head(cat)   \n",
    "            per_new = self.per_head(cat)\n",
    "            \n",
    "            final_emo = (emo_new + base_emo_logits) / 2\n",
    "            final_per = (per_new + base_per_scores) / 2\n",
    "\n",
    "            return {'emotion': final_emo, 'personality': final_per}\n",
    "\n",
    "        elif emo_input is not None:\n",
    "            return {'emotion': base_emo_logits}\n",
    "        else:\n",
    "            return {'personality': base_per_scores}\n",
    "\n",
    "def load_pretrained_emotion_encoder(path, device):\n",
    "    enc = CustomMambaClassifier(input_size=1024, d_model=256, num_layers=3, num_classes=7, dropout=0.2).to(device)\n",
    "    ck = torch.load(path, map_location=device)\n",
    "    enc.load_state_dict(ck['model_state_dict'])\n",
    "    enc.output_dim = 256\n",
    "\n",
    "    def extract_features(x, lengths):\n",
    "        h = enc.input_proj(x)               \n",
    "        for blk in enc.blocks:\n",
    "            h = blk(h)                       \n",
    "        return h\n",
    "\n",
    "    enc.extract_features = extract_features\n",
    "    enc.eval()\n",
    "    return enc\n",
    "\n",
    "def load_pretrained_personality_encoder(path, device):\n",
    "    enc = CustomMambaRegressor(input_size=1024, d_model=256, num_layers=3, num_targets=5, dropout=0.2).to(device)\n",
    "    ck = torch.load(path, map_location=device)\n",
    "    enc.load_state_dict(ck)\n",
    "    enc.output_dim = 256\n",
    "\n",
    "    def extract_features(x, lengths):\n",
    "        h = enc.input_proj(x)               \n",
    "        for blk in enc.blocks:\n",
    "            h = blk(h)                       \n",
    "        return h\n",
    "\n",
    "    enc.extract_features = extract_features\n",
    "    enc.eval()\n",
    "    return enc\n",
    "\n",
    "\n",
    "def load_fusion_model(\n",
    "    fusion_ckpt_path: str,\n",
    "    emo_encoder_ckpt: str,\n",
    "    per_encoder_ckpt: str,\n",
    "    device: str = 'cpu'\n",
    "):\n",
    "    device = torch.device(device)\n",
    "    emo_enc = load_pretrained_emotion_encoder(emo_encoder_ckpt, device)\n",
    "    per_enc = load_pretrained_personality_encoder(per_encoder_ckpt, device)\n",
    "    ckpt       = torch.load(fusion_ckpt_path, map_location=device)\n",
    "    best_cfg   = ckpt['config']\n",
    "    state_dict = ckpt['state_dict']\n",
    "    model = FusionTransformer(emo_enc, per_enc, best_cfg).to(device)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()\n",
    "    return model, device\n",
    "\n",
    "def run_inference(\n",
    "    model: FusionTransformer,\n",
    "    device: torch.device,\n",
    "    embedding: np.ndarray\n",
    "):\n",
    "    if embedding.ndim == 3 and embedding.shape[0] == 1:\n",
    "        emb = embedding[0]  \n",
    "    elif embedding.ndim == 2:\n",
    "        emb = embedding    \n",
    "\n",
    "    x = torch.tensor(emb, dtype=torch.float32).unsqueeze(0).to(device) \n",
    "    lengths = torch.tensor([emb.shape[0]], dtype=torch.long).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model(emo_input=(x, lengths), per_input=(x, lengths))\n",
    "        emo_logits = out['emotion'].cpu().numpy().squeeze(0)\n",
    "        per_scores = out['personality'].cpu().numpy().squeeze(0)\n",
    "        emo_probs  = torch.softmax(torch.tensor(emo_logits), dim=-1).numpy()\n",
    "    return emo_probs, per_scores\n",
    "\n",
    "def extract_embeddings(audio_path: str) -> np.ndarray:\n",
    "        signal, sr = librosa.load(audio_path, sr=16000)\n",
    "        emb = process_audio(signal, sr)\n",
    "        return emb\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    FUSION_CKPT = \"best_fusion_overall_mamba.pt\"\n",
    "    EMO_ENC_CKPT = \"final_best_model_uni_mamba.pt\"\n",
    "    PER_ENC_CKPT = \"best_mamba_regressor.pth\"\n",
    "\n",
    "    model, device = load_fusion_model(\n",
    "        FUSION_CKPT, EMO_ENC_CKPT, PER_ENC_CKPT, device='cpu'\n",
    "    )\n",
    "    \n",
    "    emo_names = ['Neutral', 'Anger', 'Disgust', 'Fear', 'Happiness', 'Sadness', 'Surprise']\n",
    "    pers_names = ['Openness', 'Conscientiousness', 'Extraversion', 'Agreeableness', 'Neuroticism']\n",
    "    \n",
    "    emb = extract_embeddings(\"your_audio.wav\")\n",
    "    emo_probs, per_scores = run_inference(model, device, emb)\n",
    "    print(\"Emotion\")\n",
    "    for name, v in zip(emo_names, emo_probs):\n",
    "        print(f\"  {name}: {v:.4f}\")\n",
    "        \n",
    "    print(\"Personality\")   \n",
    "    for name, v in zip(pers_names, per_scores):\n",
    "        print(f\"  {name}: {v:.4f}\")\n",
    "        \n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-05T02:33:16.859601Z",
     "start_time": "2025-07-05T02:33:14.667493Z"
    }
   },
   "id": "f09c2663ddccab51",
   "execution_count": 18
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
